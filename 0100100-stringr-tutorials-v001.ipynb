{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "── \u001b[1mAttaching packages\u001b[22m ─────────────────────────────────────── tidyverse 1.2.1 ──\n",
      "\n",
      "\u001b[32m✔\u001b[39m \u001b[34mggplot2\u001b[39m 3.2.1.\u001b[31m9000\u001b[39m     \u001b[32m✔\u001b[39m \u001b[34mpurrr  \u001b[39m 0.3.3     \n",
      "\u001b[32m✔\u001b[39m \u001b[34mtibble \u001b[39m 2.1.3          \u001b[32m✔\u001b[39m \u001b[34mdplyr  \u001b[39m 0.8.3     \n",
      "\u001b[32m✔\u001b[39m \u001b[34mtidyr  \u001b[39m 1.0.0          \u001b[32m✔\u001b[39m \u001b[34mstringr\u001b[39m 1.4.0     \n",
      "\u001b[32m✔\u001b[39m \u001b[34mreadr  \u001b[39m 1.3.1          \u001b[32m✔\u001b[39m \u001b[34mforcats\u001b[39m 0.4.0     \n",
      "\n",
      "── \u001b[1mConflicts\u001b[22m ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mfilter()\u001b[39m masks \u001b[34mstats\u001b[39m::filter()\n",
      "\u001b[31m✖\u001b[39m \u001b[34mdplyr\u001b[39m::\u001b[32mlag()\u001b[39m    masks \u001b[34mstats\u001b[39m::lag()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load in some data for use in some of the lessons\n",
    "library(RSQLite)\n",
    "library(tidyverse)\n",
    "\n",
    "sub <- \"MachineLearning\" \n",
    "db <- src_sqlite('../input/reddit-comments-may-2015/reddit-comments-may-2015/database.sqlite', create = F)\n",
    "\n",
    "# Load the desired subset of data from the database\n",
    "db_subset <- db %>% \n",
    "             tbl('May2015') %>% \n",
    "             filter(subreddit == sub)\n",
    "          \n",
    "data <- data.frame(db_subset)[, c(\"author\",\"score\",\"body\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stringr: Basic String Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "\n",
    "chr_data <- c(\"Data\", \"Daft\", \"YouTube\", \"channel\",\n",
    "             \"learn\", \"and\", \"have\", \"FUN!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "20"
      ],
      "text/latex": [
       "20"
      ],
      "text/markdown": [
       "20"
      ],
      "text/plain": [
       "[1] 20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>4</li>\n",
       "\t<li>4</li>\n",
       "\t<li>7</li>\n",
       "\t<li>7</li>\n",
       "\t<li>5</li>\n",
       "\t<li>3</li>\n",
       "\t<li>4</li>\n",
       "\t<li>4</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\item 7\n",
       "\\item 7\n",
       "\\item 5\n",
       "\\item 3\n",
       "\\item 4\n",
       "\\item 4\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 4\n",
       "2. 4\n",
       "3. 7\n",
       "4. 7\n",
       "5. 5\n",
       "6. 3\n",
       "7. 4\n",
       "8. 4\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 4 4 7 7 5 3 4 4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the length of a string\n",
    "str_length(\"awefon8g-gn951nksjdg\")\n",
    "str_length(chr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'DATA'</li>\n",
       "\t<li>'DAFT'</li>\n",
       "\t<li>'YOUTUBE'</li>\n",
       "\t<li>'CHANNEL'</li>\n",
       "\t<li>'LEARN'</li>\n",
       "\t<li>'AND'</li>\n",
       "\t<li>'HAVE'</li>\n",
       "\t<li>'FUN!'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'DATA'\n",
       "\\item 'DAFT'\n",
       "\\item 'YOUTUBE'\n",
       "\\item 'CHANNEL'\n",
       "\\item 'LEARN'\n",
       "\\item 'AND'\n",
       "\\item 'HAVE'\n",
       "\\item 'FUN!'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'DATA'\n",
       "2. 'DAFT'\n",
       "3. 'YOUTUBE'\n",
       "4. 'CHANNEL'\n",
       "5. 'LEARN'\n",
       "6. 'AND'\n",
       "7. 'HAVE'\n",
       "8. 'FUN!'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"DATA\"    \"DAFT\"    \"YOUTUBE\" \"CHANNEL\" \"LEARN\"   \"AND\"     \"HAVE\"   \n",
       "[8] \"FUN!\"   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert string letters to uppercase\n",
    "str_to_upper(chr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'data'</li>\n",
       "\t<li>'daft'</li>\n",
       "\t<li>'youtube'</li>\n",
       "\t<li>'channel'</li>\n",
       "\t<li>'learn'</li>\n",
       "\t<li>'and'</li>\n",
       "\t<li>'have'</li>\n",
       "\t<li>'fun!'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'data'\n",
       "\\item 'daft'\n",
       "\\item 'youtube'\n",
       "\\item 'channel'\n",
       "\\item 'learn'\n",
       "\\item 'and'\n",
       "\\item 'have'\n",
       "\\item 'fun!'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'data'\n",
       "2. 'daft'\n",
       "3. 'youtube'\n",
       "4. 'channel'\n",
       "5. 'learn'\n",
       "6. 'and'\n",
       "7. 'have'\n",
       "8. 'fun!'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"data\"    \"daft\"    \"youtube\" \"channel\" \"learn\"   \"and\"     \"have\"   \n",
       "[8] \"fun!\"   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert string letters to lowercase\n",
    "str_to_lower(chr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'Data'</li>\n",
       "\t<li>'Daft'</li>\n",
       "\t<li>'Youtube'</li>\n",
       "\t<li>'Channel'</li>\n",
       "\t<li>'Learn'</li>\n",
       "\t<li>'And'</li>\n",
       "\t<li>'Have'</li>\n",
       "\t<li>'Fun!'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Data'\n",
       "\\item 'Daft'\n",
       "\\item 'Youtube'\n",
       "\\item 'Channel'\n",
       "\\item 'Learn'\n",
       "\\item 'And'\n",
       "\\item 'Have'\n",
       "\\item 'Fun!'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Data'\n",
       "2. 'Daft'\n",
       "3. 'Youtube'\n",
       "4. 'Channel'\n",
       "5. 'Learn'\n",
       "6. 'And'\n",
       "7. 'Have'\n",
       "8. 'Fun!'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"Data\"    \"Daft\"    \"Youtube\" \"Channel\" \"Learn\"   \"And\"     \"Have\"   \n",
       "[8] \"Fun!\"   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert string to title (first letter uppercase)\n",
    "str_to_title(chr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Make me into a sentence!'"
      ],
      "text/latex": [
       "'Make me into a sentence!'"
      ],
      "text/markdown": [
       "'Make me into a sentence!'"
      ],
      "text/plain": [
       "[1] \"Make me into a sentence!\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert string to sentence (only first letter of first word uppercase)\n",
    "str_to_sentence(\"make me into a SENTENCE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Trim Me!'"
      ],
      "text/latex": [
       "'Trim Me!'"
      ],
      "text/markdown": [
       "'Trim Me!'"
      ],
      "text/plain": [
       "[1] \"Trim Me!\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trim whitespace\n",
    "str_trim(\"  Trim Me!   \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'    Pad Me!    '</span>"
      ],
      "text/latex": [
       "'    Pad Me!    '"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'    Pad Me!    '</span>"
      ],
      "text/plain": [
       "[1] \"    Pad Me!    \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pad strings with whitespace\n",
    "str_pad(\"Pad Me!\", width = 15, side=\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'If you have a long string, you might want to tr...'"
      ],
      "text/latex": [
       "'If you have a long string, you might want to tr...'"
      ],
      "text/markdown": [
       "'If you have a long string, you might want to tr...'"
      ],
      "text/plain": [
       "[1] \"If you have a long string, you might want to tr...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Truncate strings to a given length\n",
    "str_trunc(\"If you have a long string, you might want to truncate it!\", \n",
    "          width = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stringr: Split and Join Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li><ol class=list-inline>\n",
       "\t<li>'Split'</li>\n",
       "\t<li>'Me!'</li>\n",
       "</ol>\n",
       "</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item \\begin{enumerate*}\n",
       "\\item 'Split'\n",
       "\\item 'Me!'\n",
       "\\end{enumerate*}\n",
       "\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. 1. 'Split'\n",
       "2. 'Me!'\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "[1] \"Split\" \"Me!\"  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(tidyverse)\n",
    "\n",
    "# Split strings\n",
    "str_split(\"Split Me!\", pattern = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Join_Me!'"
      ],
      "text/latex": [
       "'Join\\_Me!'"
      ],
      "text/markdown": [
       "'Join_Me!'"
      ],
      "text/plain": [
       "[1] \"Join_Me!\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'Join_Me!'</li>\n",
       "\t<li>'vectors_too!'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Join\\_Me!'\n",
       "\\item 'vectors\\_too!'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Join_Me!'\n",
       "2. 'vectors_too!'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"Join_Me!\"     \"vectors_too!\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Join strings (equivalent to base R paste())\n",
    "str_c(\"Join\", \"Me!\", sep=\"_\")\n",
    "\n",
    "# Join strings (equivalent to base R paste())\n",
    "str_c(c(\"Join\", \"vectors\"), c(\"Me!\", \"too!\"), sep=\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Turn me into one string!'"
      ],
      "text/latex": [
       "'Turn me into one string!'"
      ],
      "text/markdown": [
       "'Turn me into one string!'"
      ],
      "text/plain": [
       "[1] \"Turn me into one string!\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Collapse a vector of strings into a single string\n",
    "str_c(c(\"Turn\", \"me\", \"into\", \"one\", \"string!\"), collapse= \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'Make'</li>\n",
       "\t<li>'NA'</li>\n",
       "\t<li>'strings!'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Make'\n",
       "\\item 'NA'\n",
       "\\item 'strings!'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Make'\n",
       "2. 'NA'\n",
       "3. 'strings!'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"Make\"     \"NA\"       \"strings!\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert NA values in character vector to string \"NA\"\n",
    "str_replace_na(c(\"Make\", NA, \"strings!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stringr: Sorting Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>2</li>\n",
       "\t<li>3</li>\n",
       "\t<li>1</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 2\n",
       "\\item 3\n",
       "\\item 1\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 2\n",
       "2. 3\n",
       "3. 1\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 2 3 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(tidyverse)\n",
    "\n",
    "sort_data <- c(\"sort\", \"me\", \"please!\")\n",
    "\n",
    "# Get vector of indicies that would sort a string alphabetically\n",
    "str_order(sort_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'me'</li>\n",
       "\t<li>'please!'</li>\n",
       "\t<li>'sort'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'me'\n",
       "\\item 'please!'\n",
       "\\item 'sort'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'me'\n",
       "2. 'please!'\n",
       "3. 'sort'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"me\"      \"please!\" \"sort\"   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use discovered ordering to extract data in sorted order\n",
    "sort_data[str_order(sort_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'me'</li>\n",
       "\t<li>'please!'</li>\n",
       "\t<li>'sort'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'me'\n",
       "\\item 'please!'\n",
       "\\item 'sort'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'me'\n",
       "2. 'please!'\n",
       "3. 'sort'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"me\"      \"please!\" \"sort\"   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Directly extract sorted strings\n",
    "str_sort(sort_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'sort'</li>\n",
       "\t<li>'please!'</li>\n",
       "\t<li>'me'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'sort'\n",
       "\\item 'please!'\n",
       "\\item 'me'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'sort'\n",
       "2. 'please!'\n",
       "3. 'me'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"sort\"    \"please!\" \"me\"     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract in reverse sorted order\n",
    "str_sort(sort_data, decreasing = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stringr: String Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'My name is Luke. Luke Skywalker.'</li>\n",
       "\t<li>'My name is Han. Han Solo.'</li>\n",
       "\t<li>'My name is Jean-Luc. Jean-Luc Picard.'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'My name is Luke. Luke Skywalker.'\n",
       "\\item 'My name is Han. Han Solo.'\n",
       "\\item 'My name is Jean-Luc. Jean-Luc Picard.'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'My name is Luke. Luke Skywalker.'\n",
       "2. 'My name is Han. Han Solo.'\n",
       "3. 'My name is Jean-Luc. Jean-Luc Picard.'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "My name is Luke. Luke Skywalker.\n",
       "My name is Han. Han Solo.\n",
       "My name is Jean-Luc. Jean-Luc Picard."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(tidyverse)\n",
    "\n",
    "first <- c(\"Luke\", \"Han\", \"Jean-Luc\")\n",
    "last <- c(\"Skywalker\", \"Solo\", \"Picard\")\n",
    "\n",
    "# Interpolate (insert variable values) into strings with str_glue()\n",
    "str_glue(\"My name is {first}. {first} {last}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'Luke Skywalker is 23 years old.'</li>\n",
       "\t<li>'Han Solo is 35 years old.'</li>\n",
       "\t<li>'Jean-Luc Picard is 51 years old.'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Luke Skywalker is 23 years old.'\n",
       "\\item 'Han Solo is 35 years old.'\n",
       "\\item 'Jean-Luc Picard is 51 years old.'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Luke Skywalker is 23 years old.'\n",
       "2. 'Han Solo is 35 years old.'\n",
       "3. 'Jean-Luc Picard is 51 years old.'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "Luke Skywalker is 23 years old.\n",
       "Han Solo is 35 years old.\n",
       "Jean-Luc Picard is 51 years old."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "minimum_age <- 18\n",
    "over_minimum <- c(5, 17, 33)\n",
    "\n",
    "# Interpolate the result of an execution into a string\n",
    "str_glue(\"{first} {last} is {minimum_age + over_minimum} years old.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'The square root of 1 is 1.'</li>\n",
       "\t<li>'The square root of 2 is 1.414.'</li>\n",
       "\t<li>'The square root of 3 is 1.732.'</li>\n",
       "\t<li>'The square root of 4 is 2.'</li>\n",
       "\t<li>'The square root of 5 is 2.236.'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'The square root of 1 is 1.'\n",
       "\\item 'The square root of 2 is 1.414.'\n",
       "\\item 'The square root of 3 is 1.732.'\n",
       "\\item 'The square root of 4 is 2.'\n",
       "\\item 'The square root of 5 is 2.236.'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'The square root of 1 is 1.'\n",
       "2. 'The square root of 2 is 1.414.'\n",
       "3. 'The square root of 3 is 1.732.'\n",
       "4. 'The square root of 4 is 2.'\n",
       "5. 'The square root of 5 is 2.236.'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "The square root of 1 is 1.\n",
       "The square root of 2 is 1.414.\n",
       "The square root of 3 is 1.732.\n",
       "The square root of 4 is 2.\n",
       "The square root of 5 is 2.236."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num <- c(1:5)\n",
    "\n",
    "# Interpolate the result of function calls\n",
    "str_glue(\"The square root of {num} is {round(sqrt(num), 3)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'The Fiat 128 gets 32.4 mpg.'</li>\n",
       "\t<li>'The Honda Civic gets 30.4 mpg.'</li>\n",
       "\t<li>'The Toyota Corolla gets 33.9 mpg.'</li>\n",
       "\t<li>'The Lotus Europa gets 30.4 mpg.'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'The Fiat 128 gets 32.4 mpg.'\n",
       "\\item 'The Honda Civic gets 30.4 mpg.'\n",
       "\\item 'The Toyota Corolla gets 33.9 mpg.'\n",
       "\\item 'The Lotus Europa gets 30.4 mpg.'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'The Fiat 128 gets 32.4 mpg.'\n",
       "2. 'The Honda Civic gets 30.4 mpg.'\n",
       "3. 'The Toyota Corolla gets 33.9 mpg.'\n",
       "4. 'The Lotus Europa gets 30.4 mpg.'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "The Fiat 128 gets 32.4 mpg.\n",
       "The Honda Civic gets 30.4 mpg.\n",
       "The Toyota Corolla gets 33.9 mpg.\n",
       "The Lotus Europa gets 30.4 mpg."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fuel_efficiency <- 30\n",
    "\n",
    "# Interpolate strings using data from a data frame\n",
    "mtcars %>% rownames_to_column(\"Model\") %>%\n",
    "         filter(mpg > fuel_efficiency) %>%\n",
    "         str_glue_data(\"The {Model} gets {mpg} mpg.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stringr: String Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 8 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>author</th><th scope=col>score</th><th scope=col>body</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>benanne    </td><td>3</td><td>I would advise against using RBMs nowadays. If you want to pre-train a deep autoencoder, you can just train shallow autoencoders to do that. It will work just as well as training an RBM and it's much simpler conceptually and in terms of implementation.\n",
       "\n",
       "But depending on the problem, pre-training may be unnecessary anyway. Just build a deep autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>butt_ghost </td><td>3</td><td>Hdf5. It's structured, it's easy to get data in and out, and it's fast. Plus it will scale if you ever get up there in dataset size.                                                                                                                                                                                                                                                                                                                              </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>buntaro_pup</td><td>1</td><td>yep, good point.                                                                                                                                                                                                                                                                                                                                                                                                                                                  </td></tr>\n",
       "\t<tr><th scope=row>4</th><td>iidealized </td><td>2</td><td>Google must have done (and is doing) serious internal research in ranking. I've heard they're pretty good at that and they've even made some money doing it :P                                                                                                                                                                                                                                                                                                    </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>[deleted]  </td><td>1</td><td>[deleted]                                                                                                                                                                                                                                                                                                                                                                                                                                                         </td></tr>\n",
       "\t<tr><th scope=row>6</th><td>stathibus  </td><td>6</td><td>Sebastian Thrun's book, Probabilistic Robotics, goes through this in great detail. Get it, read it, make it your bible.                                                                                                                                                                                                                                                                                                                                           </td></tr>\n",
       "\t<tr><th scope=row>7</th><td>soulslicer0</td><td>2</td><td>This. Such a fucking legendary book. Kalman filters, particle filters, recursive Bayesian filters and a whole bunch of other stuff. I learnt so much. \n",
       "\n",
       "Read these 3 for starts from the book, then come back and ask the questions                                                                                                                                                                                                                             </td></tr>\n",
       "\t<tr><th scope=row>8</th><td>swiftsecond</td><td>1</td><td>Do you still need help?                                                                                                                                                                                                                                                                                                                                                                                                                                           </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 8 × 3\n",
       "\\begin{tabular}{r|lll}\n",
       "  & author & score & body\\\\\n",
       "  & <chr> & <int> & <chr>\\\\\n",
       "\\hline\n",
       "\t1 & benanne     & 3 & I would advise against using RBMs nowadays. If you want to pre-train a deep autoencoder, you can just train shallow autoencoders to do that. It will work just as well as training an RBM and it's much simpler conceptually and in terms of implementation.\n",
       "\n",
       "But depending on the problem, pre-training may be unnecessary anyway. Just build a deep autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.\\\\\n",
       "\t2 & butt\\_ghost  & 3 & Hdf5. It's structured, it's easy to get data in and out, and it's fast. Plus it will scale if you ever get up there in dataset size.                                                                                                                                                                                                                                                                                                                              \\\\\n",
       "\t3 & buntaro\\_pup & 1 & yep, good point.                                                                                                                                                                                                                                                                                                                                                                                                                                                  \\\\\n",
       "\t4 & iidealized  & 2 & Google must have done (and is doing) serious internal research in ranking. I've heard they're pretty good at that and they've even made some money doing it :P                                                                                                                                                                                                                                                                                                    \\\\\n",
       "\t5 & {[}deleted{]}   & 1 & {[}deleted{]}                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\\\\n",
       "\t6 & stathibus   & 6 & Sebastian Thrun's book, Probabilistic Robotics, goes through this in great detail. Get it, read it, make it your bible.                                                                                                                                                                                                                                                                                                                                           \\\\\n",
       "\t7 & soulslicer0 & 2 & This. Such a fucking legendary book. Kalman filters, particle filters, recursive Bayesian filters and a whole bunch of other stuff. I learnt so much. \n",
       "\n",
       "Read these 3 for starts from the book, then come back and ask the questions                                                                                                                                                                                                                             \\\\\n",
       "\t8 & swiftsecond & 1 & Do you still need help?                                                                                                                                                                                                                                                                                                                                                                                                                                           \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 8 × 3\n",
       "\n",
       "| <!--/--> | author &lt;chr&gt; | score &lt;int&gt; | body &lt;chr&gt; |\n",
       "|---|---|---|---|\n",
       "| 1 | benanne     | 3 | I would advise against using RBMs nowadays. If you want to pre-train a deep autoencoder, you can just train shallow autoencoders to do that. It will work just as well as training an RBM and it's much simpler conceptually and in terms of implementation.\n",
       "\n",
       "But depending on the problem, pre-training may be unnecessary anyway. Just build a deep autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases. |\n",
       "| 2 | butt_ghost  | 3 | Hdf5. It's structured, it's easy to get data in and out, and it's fast. Plus it will scale if you ever get up there in dataset size.                                                                                                                                                                                                                                                                                                                               |\n",
       "| 3 | buntaro_pup | 1 | yep, good point.                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
       "| 4 | iidealized  | 2 | Google must have done (and is doing) serious internal research in ranking. I've heard they're pretty good at that and they've even made some money doing it :P                                                                                                                                                                                                                                                                                                     |\n",
       "| 5 | [deleted]   | 1 | [deleted]                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
       "| 6 | stathibus   | 6 | Sebastian Thrun's book, Probabilistic Robotics, goes through this in great detail. Get it, read it, make it your bible.                                                                                                                                                                                                                                                                                                                                            |\n",
       "| 7 | soulslicer0 | 2 | This. Such a fucking legendary book. Kalman filters, particle filters, recursive Bayesian filters and a whole bunch of other stuff. I learnt so much. \n",
       "\n",
       "Read these 3 for starts from the book, then come back and ask the questions                                                                                                                                                                                                                              |\n",
       "| 8 | swiftsecond | 1 | Do you still need help?                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
       "\n"
      ],
      "text/plain": [
       "  author      score\n",
       "1 benanne     3    \n",
       "2 butt_ghost  3    \n",
       "3 buntaro_pup 1    \n",
       "4 iidealized  2    \n",
       "5 [deleted]   1    \n",
       "6 stathibus   6    \n",
       "7 soulslicer0 2    \n",
       "8 swiftsecond 1    \n",
       "  body                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "1 I would advise against using RBMs nowadays. If you want to pre-train a deep autoencoder, you can just train shallow autoencoders to do that. It will work just as well as training an RBM and it's much simpler conceptually and in terms of implementation.\\n\\nBut depending on the problem, pre-training may be unnecessary anyway. Just build a deep autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.\n",
       "2 Hdf5. It's structured, it's easy to get data in and out, and it's fast. Plus it will scale if you ever get up there in dataset size.                                                                                                                                                                                                                                                                                                                              \n",
       "3 yep, good point.                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "4 Google must have done (and is doing) serious internal research in ranking. I've heard they're pretty good at that and they've even made some money doing it :P                                                                                                                                                                                                                                                                                                    \n",
       "5 [deleted]                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "6 Sebastian Thrun's book, Probabilistic Robotics, goes through this in great detail. Get it, read it, make it your bible.                                                                                                                                                                                                                                                                                                                                           \n",
       "7 This. Such a fucking legendary book. Kalman filters, particle filters, recursive Bayesian filters and a whole bunch of other stuff. I learnt so much. \\n\\nRead these 3 for starts from the book, then come back and ask the questions                                                                                                                                                                                                                             \n",
       "8 Do you still need help?                                                                                                                                                                                                                                                                                                                                                                                                                                           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(tidyverse)\n",
    "\n",
    "head(data,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>TRUE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "\t<li>FALSE</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item TRUE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item TRUE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item TRUE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item TRUE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item TRUE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\item FALSE\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. TRUE\n",
       "2. FALSE\n",
       "3. FALSE\n",
       "4. FALSE\n",
       "5. FALSE\n",
       "6. FALSE\n",
       "7. FALSE\n",
       "8. FALSE\n",
       "9. FALSE\n",
       "10. FALSE\n",
       "11. FALSE\n",
       "12. FALSE\n",
       "13. FALSE\n",
       "14. FALSE\n",
       "15. TRUE\n",
       "16. FALSE\n",
       "17. FALSE\n",
       "18. FALSE\n",
       "19. TRUE\n",
       "20. FALSE\n",
       "21. FALSE\n",
       "22. FALSE\n",
       "23. FALSE\n",
       "24. FALSE\n",
       "25. FALSE\n",
       "26. FALSE\n",
       "27. FALSE\n",
       "28. FALSE\n",
       "29. FALSE\n",
       "30. FALSE\n",
       "31. FALSE\n",
       "32. FALSE\n",
       "33. FALSE\n",
       "34. FALSE\n",
       "35. FALSE\n",
       "36. FALSE\n",
       "37. FALSE\n",
       "38. FALSE\n",
       "39. FALSE\n",
       "40. FALSE\n",
       "41. FALSE\n",
       "42. FALSE\n",
       "43. FALSE\n",
       "44. FALSE\n",
       "45. FALSE\n",
       "46. FALSE\n",
       "47. FALSE\n",
       "48. FALSE\n",
       "49. FALSE\n",
       "50. FALSE\n",
       "51. FALSE\n",
       "52. FALSE\n",
       "53. FALSE\n",
       "54. FALSE\n",
       "55. FALSE\n",
       "56. FALSE\n",
       "57. FALSE\n",
       "58. FALSE\n",
       "59. FALSE\n",
       "60. FALSE\n",
       "61. FALSE\n",
       "62. FALSE\n",
       "63. FALSE\n",
       "64. FALSE\n",
       "65. FALSE\n",
       "66. FALSE\n",
       "67. FALSE\n",
       "68. FALSE\n",
       "69. FALSE\n",
       "70. TRUE\n",
       "71. FALSE\n",
       "72. FALSE\n",
       "73. TRUE\n",
       "74. FALSE\n",
       "75. FALSE\n",
       "76. FALSE\n",
       "77. FALSE\n",
       "78. FALSE\n",
       "79. FALSE\n",
       "80. FALSE\n",
       "81. FALSE\n",
       "82. FALSE\n",
       "83. FALSE\n",
       "84. FALSE\n",
       "85. FALSE\n",
       "86. FALSE\n",
       "87. FALSE\n",
       "88. FALSE\n",
       "89. FALSE\n",
       "90. FALSE\n",
       "91. FALSE\n",
       "92. FALSE\n",
       "93. FALSE\n",
       "94. FALSE\n",
       "95. FALSE\n",
       "96. FALSE\n",
       "97. FALSE\n",
       "98. FALSE\n",
       "99. FALSE\n",
       "100. FALSE\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  [1]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n",
       " [13] FALSE FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n",
       " [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n",
       " [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n",
       " [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n",
       " [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n",
       " [73]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n",
       " [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n",
       " [97] FALSE FALSE FALSE FALSE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Detecting the presence of a pattern in strings\n",
    "str_detect(data$body[1:100], pattern=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1</li>\n",
       "\t<li>15</li>\n",
       "\t<li>19</li>\n",
       "\t<li>70</li>\n",
       "\t<li>73</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1\n",
       "\\item 15\n",
       "\\item 19\n",
       "\\item 70\n",
       "\\item 73\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1\n",
       "2. 15\n",
       "3. 19\n",
       "4. 70\n",
       "5. 73\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1]  1 15 19 70 73"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the indicies of matched strings\n",
    "str_inds <- str_which(data$body[1:100], pattern=\"deep\")\n",
    "str_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'I would advise against using RBMs nowadays. If you want to pre-train a deep autoencoder, you can just train shallow autoencoders to do that. It will work just as well as training an RBM and it\\'s much simpler conceptually and in terms of implementation.\\n\\nBut depending on the problem, pre-training may be unnecessary anyway. Just build a deep autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.'</li>\n",
       "\t<li>'You know basic [lenet](http://deeplearning.net/tutorial/lenet.html) models work really well for even fine grained classification with enough test data.\\n\\nThey have been shown to outperform humans on fine grained imagenet data.\\n\\nI suspect this \"leaves are always on a white background and flat\" view will be even easier.'</li>\n",
       "\t<li>'pre-training was a bit of a stopgap solution for the vanishing gradient problem in deeper neural nets. When ReLUs were introduced it was discovered that networks with these units suffer from this problem much less, so you can make them a lot deeper without needing tricks like pre-training.'</li>\n",
       "\t<li><span style=white-space:pre-wrap>'The first, which is an elementary introduction, appears to give a good reason to use cross entropy, the gradients are larger for a bad mistake.    In MSE scoring after a sigmoid final layer, if your model predicts nearly 0 and the correct answer is 1, or vice versa, then gradients of loss function w.r.t. parameters may be very small, and hence learning from that example will be slow.  The first reference assumes that\\'s a vice.\\n\\nIt may be a virtue in some problems.  If you have, for example, a large data set with many examples of many varied types, and you expect that even an excellent model will get many of them wrong, then using MSE, and essentially \"giving up on your dead\" can be the right thing to do.  It\\'s \"example triage\".  Say a statistical learning task. \\n\\nOn the other hand, if your problem is more like \"artificial intelligence\", e.g. a human could conceivably get nearly perfect results because of a deep internal representation, and your examples are truthfully tagged with high accuracy, then penalizing big mistakes immediately is a good idea and will cause learning to be better.  So use cross-entropy/logarithmic loss. '</span></li>\n",
       "\t<li>'Word2vec isn\\'t deep learning. Its explicitly and deliberately as shallow as possible - one of Mikolov\\'s central realisations was that a very simple model trained on vast amounts of data can be superior to a complicated model trained on less data.\\n\\nThere\\'s another algorithm called Paragraph2Vec which builds on Word2Vec and *does* incorporate \\'deep learning\\' (in so far as it uses a neural network). Its purpose is to amalgamate the Word2Vec vectors for the words in a chunk of text - kinda like you do by averaging them. P2V tries to find a paragraph vector which is a good predictor of the word vectors, with the idea being that such a vector effectively represents the content of the paragraph.'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'I would advise against using RBMs nowadays. If you want to pre-train a deep autoencoder, you can just train shallow autoencoders to do that. It will work just as well as training an RBM and it\\textbackslash{}'s much simpler conceptually and in terms of implementation.\\textbackslash{}n\\textbackslash{}nBut depending on the problem, pre-training may be unnecessary anyway. Just build a deep autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.'\n",
       "\\item 'You know basic {[}lenet{]}(http://deeplearning.net/tutorial/lenet.html) models work really well for even fine grained classification with enough test data.\\textbackslash{}n\\textbackslash{}nThey have been shown to outperform humans on fine grained imagenet data.\\textbackslash{}n\\textbackslash{}nI suspect this \"leaves are always on a white background and flat\" view will be even easier.'\n",
       "\\item 'pre-training was a bit of a stopgap solution for the vanishing gradient problem in deeper neural nets. When ReLUs were introduced it was discovered that networks with these units suffer from this problem much less, so you can make them a lot deeper without needing tricks like pre-training.'\n",
       "\\item 'The first, which is an elementary introduction, appears to give a good reason to use cross entropy, the gradients are larger for a bad mistake.    In MSE scoring after a sigmoid final layer, if your model predicts nearly 0 and the correct answer is 1, or vice versa, then gradients of loss function w.r.t. parameters may be very small, and hence learning from that example will be slow.  The first reference assumes that\\textbackslash{}'s a vice.\\textbackslash{}n\\textbackslash{}nIt may be a virtue in some problems.  If you have, for example, a large data set with many examples of many varied types, and you expect that even an excellent model will get many of them wrong, then using MSE, and essentially \"giving up on your dead\" can be the right thing to do.  It\\textbackslash{}'s \"example triage\".  Say a statistical learning task. \\textbackslash{}n\\textbackslash{}nOn the other hand, if your problem is more like \"artificial intelligence\", e.g. a human could conceivably get nearly perfect results because of a deep internal representation, and your examples are truthfully tagged with high accuracy, then penalizing big mistakes immediately is a good idea and will cause learning to be better.  So use cross-entropy/logarithmic loss. '\n",
       "\\item 'Word2vec isn\\textbackslash{}'t deep learning. Its explicitly and deliberately as shallow as possible - one of Mikolov\\textbackslash{}'s central realisations was that a very simple model trained on vast amounts of data can be superior to a complicated model trained on less data.\\textbackslash{}n\\textbackslash{}nThere\\textbackslash{}'s another algorithm called Paragraph2Vec which builds on Word2Vec and *does* incorporate \\textbackslash{}'deep learning\\textbackslash{}' (in so far as it uses a neural network). Its purpose is to amalgamate the Word2Vec vectors for the words in a chunk of text - kinda like you do by averaging them. P2V tries to find a paragraph vector which is a good predictor of the word vectors, with the idea being that such a vector effectively represents the content of the paragraph.'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'I would advise against using RBMs nowadays. If you want to pre-train a deep autoencoder, you can just train shallow autoencoders to do that. It will work just as well as training an RBM and it\\'s much simpler conceptually and in terms of implementation.\\n\\nBut depending on the problem, pre-training may be unnecessary anyway. Just build a deep autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.'\n",
       "2. 'You know basic [lenet](http://deeplearning.net/tutorial/lenet.html) models work really well for even fine grained classification with enough test data.\\n\\nThey have been shown to outperform humans on fine grained imagenet data.\\n\\nI suspect this \"leaves are always on a white background and flat\" view will be even easier.'\n",
       "3. 'pre-training was a bit of a stopgap solution for the vanishing gradient problem in deeper neural nets. When ReLUs were introduced it was discovered that networks with these units suffer from this problem much less, so you can make them a lot deeper without needing tricks like pre-training.'\n",
       "4. <span style=white-space:pre-wrap>'The first, which is an elementary introduction, appears to give a good reason to use cross entropy, the gradients are larger for a bad mistake.    In MSE scoring after a sigmoid final layer, if your model predicts nearly 0 and the correct answer is 1, or vice versa, then gradients of loss function w.r.t. parameters may be very small, and hence learning from that example will be slow.  The first reference assumes that\\'s a vice.\\n\\nIt may be a virtue in some problems.  If you have, for example, a large data set with many examples of many varied types, and you expect that even an excellent model will get many of them wrong, then using MSE, and essentially \"giving up on your dead\" can be the right thing to do.  It\\'s \"example triage\".  Say a statistical learning task. \\n\\nOn the other hand, if your problem is more like \"artificial intelligence\", e.g. a human could conceivably get nearly perfect results because of a deep internal representation, and your examples are truthfully tagged with high accuracy, then penalizing big mistakes immediately is a good idea and will cause learning to be better.  So use cross-entropy/logarithmic loss. '</span>\n",
       "5. 'Word2vec isn\\'t deep learning. Its explicitly and deliberately as shallow as possible - one of Mikolov\\'s central realisations was that a very simple model trained on vast amounts of data can be superior to a complicated model trained on less data.\\n\\nThere\\'s another algorithm called Paragraph2Vec which builds on Word2Vec and *does* incorporate \\'deep learning\\' (in so far as it uses a neural network). Its purpose is to amalgamate the Word2Vec vectors for the words in a chunk of text - kinda like you do by averaging them. P2V tries to find a paragraph vector which is a good predictor of the word vectors, with the idea being that such a vector effectively represents the content of the paragraph.'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"I would advise against using RBMs nowadays. If you want to pre-train a deep autoencoder, you can just train shallow autoencoders to do that. It will work just as well as training an RBM and it's much simpler conceptually and in terms of implementation.\\n\\nBut depending on the problem, pre-training may be unnecessary anyway. Just build a deep autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "[2] \"You know basic [lenet](http://deeplearning.net/tutorial/lenet.html) models work really well for even fine grained classification with enough test data.\\n\\nThey have been shown to outperform humans on fine grained imagenet data.\\n\\nI suspect this \\\"leaves are always on a white background and flat\\\" view will be even easier.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "[3] \"pre-training was a bit of a stopgap solution for the vanishing gradient problem in deeper neural nets. When ReLUs were introduced it was discovered that networks with these units suffer from this problem much less, so you can make them a lot deeper without needing tricks like pre-training.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "[4] \"The first, which is an elementary introduction, appears to give a good reason to use cross entropy, the gradients are larger for a bad mistake.    In MSE scoring after a sigmoid final layer, if your model predicts nearly 0 and the correct answer is 1, or vice versa, then gradients of loss function w.r.t. parameters may be very small, and hence learning from that example will be slow.  The first reference assumes that's a vice.\\n\\nIt may be a virtue in some problems.  If you have, for example, a large data set with many examples of many varied types, and you expect that even an excellent model will get many of them wrong, then using MSE, and essentially \\\"giving up on your dead\\\" can be the right thing to do.  It's \\\"example triage\\\".  Say a statistical learning task. \\n\\nOn the other hand, if your problem is more like \\\"artificial intelligence\\\", e.g. a human could conceivably get nearly perfect results because of a deep internal representation, and your examples are truthfully tagged with high accuracy, then penalizing big mistakes immediately is a good idea and will cause learning to be better.  So use cross-entropy/logarithmic loss. \"\n",
       "[5] \"Word2vec isn't deep learning. Its explicitly and deliberately as shallow as possible - one of Mikolov's central realisations was that a very simple model trained on vast amounts of data can be superior to a complicated model trained on less data.\\n\\nThere's another algorithm called Paragraph2Vec which builds on Word2Vec and *does* incorporate 'deep learning' (in so far as it uses a neural network). Its purpose is to amalgamate the Word2Vec vectors for the words in a chunk of text - kinda like you do by averaging them. P2V tries to find a paragraph vector which is a good predictor of the word vectors, with the idea being that such a vector effectively represents the content of the paragraph.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract matched strings using detected indicies\n",
    "data$body[str_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>2</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>1</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>2</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>1</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>2</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "\t<li>0</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 2\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 1\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 2\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 1\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 2\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\item 0\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 2\n",
       "2. 0\n",
       "3. 0\n",
       "4. 0\n",
       "5. 0\n",
       "6. 0\n",
       "7. 0\n",
       "8. 0\n",
       "9. 0\n",
       "10. 0\n",
       "11. 0\n",
       "12. 0\n",
       "13. 0\n",
       "14. 0\n",
       "15. 1\n",
       "16. 0\n",
       "17. 0\n",
       "18. 0\n",
       "19. 2\n",
       "20. 0\n",
       "21. 0\n",
       "22. 0\n",
       "23. 0\n",
       "24. 0\n",
       "25. 0\n",
       "26. 0\n",
       "27. 0\n",
       "28. 0\n",
       "29. 0\n",
       "30. 0\n",
       "31. 0\n",
       "32. 0\n",
       "33. 0\n",
       "34. 0\n",
       "35. 0\n",
       "36. 0\n",
       "37. 0\n",
       "38. 0\n",
       "39. 0\n",
       "40. 0\n",
       "41. 0\n",
       "42. 0\n",
       "43. 0\n",
       "44. 0\n",
       "45. 0\n",
       "46. 0\n",
       "47. 0\n",
       "48. 0\n",
       "49. 0\n",
       "50. 0\n",
       "51. 0\n",
       "52. 0\n",
       "53. 0\n",
       "54. 0\n",
       "55. 0\n",
       "56. 0\n",
       "57. 0\n",
       "58. 0\n",
       "59. 0\n",
       "60. 0\n",
       "61. 0\n",
       "62. 0\n",
       "63. 0\n",
       "64. 0\n",
       "65. 0\n",
       "66. 0\n",
       "67. 0\n",
       "68. 0\n",
       "69. 0\n",
       "70. 1\n",
       "71. 0\n",
       "72. 0\n",
       "73. 2\n",
       "74. 0\n",
       "75. 0\n",
       "76. 0\n",
       "77. 0\n",
       "78. 0\n",
       "79. 0\n",
       "80. 0\n",
       "81. 0\n",
       "82. 0\n",
       "83. 0\n",
       "84. 0\n",
       "85. 0\n",
       "86. 0\n",
       "87. 0\n",
       "88. 0\n",
       "89. 0\n",
       "90. 0\n",
       "91. 0\n",
       "92. 0\n",
       "93. 0\n",
       "94. 0\n",
       "95. 0\n",
       "96. 0\n",
       "97. 0\n",
       "98. 0\n",
       "99. 0\n",
       "100. 0\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  [1] 2 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
       " [38] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 2 0\n",
       " [75] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count the number of matches\n",
    "str_count(data$body[1:100], \"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li><table>\n",
       "<caption>A matrix: 2 × 2 of type int</caption>\n",
       "<thead>\n",
       "\t<tr><th scope=col>start</th><th scope=col>end</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><td> 72</td><td> 75</td></tr>\n",
       "\t<tr><td>338</td><td>341</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item A matrix: 2 × 2 of type int\n",
       "\\begin{tabular}{ll}\n",
       " start & end\\\\\n",
       "\\hline\n",
       "\t  72 &  75\\\\\n",
       "\t 338 & 341\\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. \n",
       "A matrix: 2 × 2 of type int\n",
       "\n",
       "| start | end |\n",
       "|---|---|\n",
       "|  72 |  75 |\n",
       "| 338 | 341 |\n",
       "\n",
       "\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "     start end\n",
       "[1,]    72  75\n",
       "[2,]   338 341\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the position of matches\n",
    "str_locate_all(data$body[1], \"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'deep'</li>\n",
       "\t<li>'and'</li>\n",
       "\t<li>NA</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'deep'\n",
       "\\item 'and'\n",
       "\\item NA\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'deep'\n",
       "2. 'and'\n",
       "3. NA\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"deep\" \"and\"  NA    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a list of the first match found in each string as a vector\n",
    "str_extract(data$body[1:3], \"deep|the|and\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A matrix: 3 × 1 of type chr</caption>\n",
       "<tbody>\n",
       "\t<tr><td>deep</td></tr>\n",
       "\t<tr><td>and </td></tr>\n",
       "\t<tr><td>NA  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 3 × 1 of type chr\n",
       "\\begin{tabular}{l}\n",
       "\t deep\\\\\n",
       "\t and \\\\\n",
       "\t NA  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 3 × 1 of type chr\n",
       "\n",
       "| deep |\n",
       "| and  |\n",
       "| NA   |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]\n",
       "[1,] deep\n",
       "[2,] and \n",
       "[3,] NA  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a list of the first match found in each string as matrix\n",
    "str_match(data$body[1:3], \"deep|the|and\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol>\n",
       "\t<li><table>\n",
       "<caption>A matrix: 7 × 1 of type chr</caption>\n",
       "<tbody>\n",
       "\t<tr><td>deep</td></tr>\n",
       "\t<tr><td>and </td></tr>\n",
       "\t<tr><td>and </td></tr>\n",
       "\t<tr><td>the </td></tr>\n",
       "\t<tr><td>deep</td></tr>\n",
       "\t<tr><td>and </td></tr>\n",
       "\t<tr><td>and </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "\t<li><table>\n",
       "<caption>A matrix: 3 × 1 of type chr</caption>\n",
       "<tbody>\n",
       "\t<tr><td>and</td></tr>\n",
       "\t<tr><td>and</td></tr>\n",
       "\t<tr><td>the</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</li>\n",
       "\t<li></li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate}\n",
       "\\item A matrix: 7 × 1 of type chr\n",
       "\\begin{tabular}{l}\n",
       "\t deep\\\\\n",
       "\t and \\\\\n",
       "\t and \\\\\n",
       "\t the \\\\\n",
       "\t deep\\\\\n",
       "\t and \\\\\n",
       "\t and \\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\item A matrix: 3 × 1 of type chr\n",
       "\\begin{tabular}{l}\n",
       "\t and\\\\\n",
       "\t and\\\\\n",
       "\t the\\\\\n",
       "\\end{tabular}\n",
       "\n",
       "\\item \n",
       "\\end{enumerate}\n"
      ],
      "text/markdown": [
       "1. \n",
       "A matrix: 7 × 1 of type chr\n",
       "\n",
       "| deep |\n",
       "| and  |\n",
       "| and  |\n",
       "| the  |\n",
       "| deep |\n",
       "| and  |\n",
       "| and  |\n",
       "\n",
       "\n",
       "2. \n",
       "A matrix: 3 × 1 of type chr\n",
       "\n",
       "| and |\n",
       "| and |\n",
       "| the |\n",
       "\n",
       "\n",
       "3. \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[[1]]\n",
       "     [,1]  \n",
       "[1,] \"deep\"\n",
       "[2,] \"and\" \n",
       "[3,] \"and\" \n",
       "[4,] \"the\" \n",
       "[5,] \"deep\"\n",
       "[6,] \"and\" \n",
       "[7,] \"and\" \n",
       "\n",
       "[[2]]\n",
       "     [,1] \n",
       "[1,] \"and\"\n",
       "[2,] \"and\"\n",
       "[3,] \"the\"\n",
       "\n",
       "[[3]]\n",
       "     [,1]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a list of the all matches found in each string as list of matricies\n",
    "str_match_all(data$body[1:3], \"deep|the|and\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stringr: Subset and Replace Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 8 × 3</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>author</th><th scope=col>score</th><th scope=col>body</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>benanne    </td><td>3</td><td>I would advise against using RBMs nowadays. If you want to pre-train a deep autoencoder, you can just train shallow autoencoders to do that. It will work just as well as training an RBM and it's much simpler conceptually and in terms of implementation.\n",
       "\n",
       "But depending on the problem, pre-training may be unnecessary anyway. Just build a deep autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>butt_ghost </td><td>3</td><td>Hdf5. It's structured, it's easy to get data in and out, and it's fast. Plus it will scale if you ever get up there in dataset size.                                                                                                                                                                                                                                                                                                                              </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>buntaro_pup</td><td>1</td><td>yep, good point.                                                                                                                                                                                                                                                                                                                                                                                                                                                  </td></tr>\n",
       "\t<tr><th scope=row>4</th><td>iidealized </td><td>2</td><td>Google must have done (and is doing) serious internal research in ranking. I've heard they're pretty good at that and they've even made some money doing it :P                                                                                                                                                                                                                                                                                                    </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>[deleted]  </td><td>1</td><td>[deleted]                                                                                                                                                                                                                                                                                                                                                                                                                                                         </td></tr>\n",
       "\t<tr><th scope=row>6</th><td>stathibus  </td><td>6</td><td>Sebastian Thrun's book, Probabilistic Robotics, goes through this in great detail. Get it, read it, make it your bible.                                                                                                                                                                                                                                                                                                                                           </td></tr>\n",
       "\t<tr><th scope=row>7</th><td>soulslicer0</td><td>2</td><td>This. Such a fucking legendary book. Kalman filters, particle filters, recursive Bayesian filters and a whole bunch of other stuff. I learnt so much. \n",
       "\n",
       "Read these 3 for starts from the book, then come back and ask the questions                                                                                                                                                                                                                             </td></tr>\n",
       "\t<tr><th scope=row>8</th><td>swiftsecond</td><td>1</td><td>Do you still need help?                                                                                                                                                                                                                                                                                                                                                                                                                                           </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 8 × 3\n",
       "\\begin{tabular}{r|lll}\n",
       "  & author & score & body\\\\\n",
       "  & <chr> & <int> & <chr>\\\\\n",
       "\\hline\n",
       "\t1 & benanne     & 3 & I would advise against using RBMs nowadays. If you want to pre-train a deep autoencoder, you can just train shallow autoencoders to do that. It will work just as well as training an RBM and it's much simpler conceptually and in terms of implementation.\n",
       "\n",
       "But depending on the problem, pre-training may be unnecessary anyway. Just build a deep autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.\\\\\n",
       "\t2 & butt\\_ghost  & 3 & Hdf5. It's structured, it's easy to get data in and out, and it's fast. Plus it will scale if you ever get up there in dataset size.                                                                                                                                                                                                                                                                                                                              \\\\\n",
       "\t3 & buntaro\\_pup & 1 & yep, good point.                                                                                                                                                                                                                                                                                                                                                                                                                                                  \\\\\n",
       "\t4 & iidealized  & 2 & Google must have done (and is doing) serious internal research in ranking. I've heard they're pretty good at that and they've even made some money doing it :P                                                                                                                                                                                                                                                                                                    \\\\\n",
       "\t5 & {[}deleted{]}   & 1 & {[}deleted{]}                                                                                                                                                                                                                                                                                                                                                                                                                                                         \\\\\n",
       "\t6 & stathibus   & 6 & Sebastian Thrun's book, Probabilistic Robotics, goes through this in great detail. Get it, read it, make it your bible.                                                                                                                                                                                                                                                                                                                                           \\\\\n",
       "\t7 & soulslicer0 & 2 & This. Such a fucking legendary book. Kalman filters, particle filters, recursive Bayesian filters and a whole bunch of other stuff. I learnt so much. \n",
       "\n",
       "Read these 3 for starts from the book, then come back and ask the questions                                                                                                                                                                                                                             \\\\\n",
       "\t8 & swiftsecond & 1 & Do you still need help?                                                                                                                                                                                                                                                                                                                                                                                                                                           \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 8 × 3\n",
       "\n",
       "| <!--/--> | author &lt;chr&gt; | score &lt;int&gt; | body &lt;chr&gt; |\n",
       "|---|---|---|---|\n",
       "| 1 | benanne     | 3 | I would advise against using RBMs nowadays. If you want to pre-train a deep autoencoder, you can just train shallow autoencoders to do that. It will work just as well as training an RBM and it's much simpler conceptually and in terms of implementation.\n",
       "\n",
       "But depending on the problem, pre-training may be unnecessary anyway. Just build a deep autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases. |\n",
       "| 2 | butt_ghost  | 3 | Hdf5. It's structured, it's easy to get data in and out, and it's fast. Plus it will scale if you ever get up there in dataset size.                                                                                                                                                                                                                                                                                                                               |\n",
       "| 3 | buntaro_pup | 1 | yep, good point.                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
       "| 4 | iidealized  | 2 | Google must have done (and is doing) serious internal research in ranking. I've heard they're pretty good at that and they've even made some money doing it :P                                                                                                                                                                                                                                                                                                     |\n",
       "| 5 | [deleted]   | 1 | [deleted]                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
       "| 6 | stathibus   | 6 | Sebastian Thrun's book, Probabilistic Robotics, goes through this in great detail. Get it, read it, make it your bible.                                                                                                                                                                                                                                                                                                                                            |\n",
       "| 7 | soulslicer0 | 2 | This. Such a fucking legendary book. Kalman filters, particle filters, recursive Bayesian filters and a whole bunch of other stuff. I learnt so much. \n",
       "\n",
       "Read these 3 for starts from the book, then come back and ask the questions                                                                                                                                                                                                                              |\n",
       "| 8 | swiftsecond | 1 | Do you still need help?                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
       "\n"
      ],
      "text/plain": [
       "  author      score\n",
       "1 benanne     3    \n",
       "2 butt_ghost  3    \n",
       "3 buntaro_pup 1    \n",
       "4 iidealized  2    \n",
       "5 [deleted]   1    \n",
       "6 stathibus   6    \n",
       "7 soulslicer0 2    \n",
       "8 swiftsecond 1    \n",
       "  body                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "1 I would advise against using RBMs nowadays. If you want to pre-train a deep autoencoder, you can just train shallow autoencoders to do that. It will work just as well as training an RBM and it's much simpler conceptually and in terms of implementation.\\n\\nBut depending on the problem, pre-training may be unnecessary anyway. Just build a deep autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.\n",
       "2 Hdf5. It's structured, it's easy to get data in and out, and it's fast. Plus it will scale if you ever get up there in dataset size.                                                                                                                                                                                                                                                                                                                              \n",
       "3 yep, good point.                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "4 Google must have done (and is doing) serious internal research in ranking. I've heard they're pretty good at that and they've even made some money doing it :P                                                                                                                                                                                                                                                                                                    \n",
       "5 [deleted]                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
       "6 Sebastian Thrun's book, Probabilistic Robotics, goes through this in great detail. Get it, read it, make it your bible.                                                                                                                                                                                                                                                                                                                                           \n",
       "7 This. Such a fucking legendary book. Kalman filters, particle filters, recursive Bayesian filters and a whole bunch of other stuff. I learnt so much. \\n\\nRead these 3 for starts from the book, then come back and ask the questions                                                                                                                                                                                                                             \n",
       "8 Do you still need help?                                                                                                                                                                                                                                                                                                                                                                                                                                           "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(tidyverse)\n",
    "\n",
    "head(data,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'I would advise against using RBMs nowadays. If you want to pre-train a deep autoencoder, you can jus'"
      ],
      "text/latex": [
       "'I would advise against using RBMs nowadays. If you want to pre-train a deep autoencoder, you can jus'"
      ],
      "text/markdown": [
       "'I would advise against using RBMs nowadays. If you want to pre-train a deep autoencoder, you can jus'"
      ],
      "text/plain": [
       "[1] \"I would advise against using RBMs nowadays. If you want to pre-train a deep autoencoder, you can jus\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a string subset based on character position\n",
    "str_sub(data$body[1], start=1, end=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'I would advise against using RBMs nowadays. If you want'"
      ],
      "text/latex": [
       "'I would advise against using RBMs nowadays. If you want'"
      ],
      "text/markdown": [
       "'I would advise against using RBMs nowadays. If you want'"
      ],
      "text/plain": [
       "[1] \"I would advise against using RBMs nowadays. If you want\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a string subset based on words\n",
    "word(data$body[1], start=1, end=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'I would advise against using RBMs nowadays. If you want to pre-train a deep autoencoder, you can just train shallow autoencoders to do that. It will work just as well as training an RBM and it\\'s much simpler conceptually and in terms of implementation.\\n\\nBut depending on the problem, pre-training may be unnecessary anyway. Just build a deep autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.'</li>\n",
       "\t<li>'You know basic [lenet](http://deeplearning.net/tutorial/lenet.html) models work really well for even fine grained classification with enough test data.\\n\\nThey have been shown to outperform humans on fine grained imagenet data.\\n\\nI suspect this \"leaves are always on a white background and flat\" view will be even easier.'</li>\n",
       "\t<li>'pre-training was a bit of a stopgap solution for the vanishing gradient problem in deeper neural nets. When ReLUs were introduced it was discovered that networks with these units suffer from this problem much less, so you can make them a lot deeper without needing tricks like pre-training.'</li>\n",
       "\t<li><span style=white-space:pre-wrap>'The first, which is an elementary introduction, appears to give a good reason to use cross entropy, the gradients are larger for a bad mistake.    In MSE scoring after a sigmoid final layer, if your model predicts nearly 0 and the correct answer is 1, or vice versa, then gradients of loss function w.r.t. parameters may be very small, and hence learning from that example will be slow.  The first reference assumes that\\'s a vice.\\n\\nIt may be a virtue in some problems.  If you have, for example, a large data set with many examples of many varied types, and you expect that even an excellent model will get many of them wrong, then using MSE, and essentially \"giving up on your dead\" can be the right thing to do.  It\\'s \"example triage\".  Say a statistical learning task. \\n\\nOn the other hand, if your problem is more like \"artificial intelligence\", e.g. a human could conceivably get nearly perfect results because of a deep internal representation, and your examples are truthfully tagged with high accuracy, then penalizing big mistakes immediately is a good idea and will cause learning to be better.  So use cross-entropy/logarithmic loss. '</span></li>\n",
       "\t<li>'Word2vec isn\\'t deep learning. Its explicitly and deliberately as shallow as possible - one of Mikolov\\'s central realisations was that a very simple model trained on vast amounts of data can be superior to a complicated model trained on less data.\\n\\nThere\\'s another algorithm called Paragraph2Vec which builds on Word2Vec and *does* incorporate \\'deep learning\\' (in so far as it uses a neural network). Its purpose is to amalgamate the Word2Vec vectors for the words in a chunk of text - kinda like you do by averaging them. P2V tries to find a paragraph vector which is a good predictor of the word vectors, with the idea being that such a vector effectively represents the content of the paragraph.'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'I would advise against using RBMs nowadays. If you want to pre-train a deep autoencoder, you can just train shallow autoencoders to do that. It will work just as well as training an RBM and it\\textbackslash{}'s much simpler conceptually and in terms of implementation.\\textbackslash{}n\\textbackslash{}nBut depending on the problem, pre-training may be unnecessary anyway. Just build a deep autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.'\n",
       "\\item 'You know basic {[}lenet{]}(http://deeplearning.net/tutorial/lenet.html) models work really well for even fine grained classification with enough test data.\\textbackslash{}n\\textbackslash{}nThey have been shown to outperform humans on fine grained imagenet data.\\textbackslash{}n\\textbackslash{}nI suspect this \"leaves are always on a white background and flat\" view will be even easier.'\n",
       "\\item 'pre-training was a bit of a stopgap solution for the vanishing gradient problem in deeper neural nets. When ReLUs were introduced it was discovered that networks with these units suffer from this problem much less, so you can make them a lot deeper without needing tricks like pre-training.'\n",
       "\\item 'The first, which is an elementary introduction, appears to give a good reason to use cross entropy, the gradients are larger for a bad mistake.    In MSE scoring after a sigmoid final layer, if your model predicts nearly 0 and the correct answer is 1, or vice versa, then gradients of loss function w.r.t. parameters may be very small, and hence learning from that example will be slow.  The first reference assumes that\\textbackslash{}'s a vice.\\textbackslash{}n\\textbackslash{}nIt may be a virtue in some problems.  If you have, for example, a large data set with many examples of many varied types, and you expect that even an excellent model will get many of them wrong, then using MSE, and essentially \"giving up on your dead\" can be the right thing to do.  It\\textbackslash{}'s \"example triage\".  Say a statistical learning task. \\textbackslash{}n\\textbackslash{}nOn the other hand, if your problem is more like \"artificial intelligence\", e.g. a human could conceivably get nearly perfect results because of a deep internal representation, and your examples are truthfully tagged with high accuracy, then penalizing big mistakes immediately is a good idea and will cause learning to be better.  So use cross-entropy/logarithmic loss. '\n",
       "\\item 'Word2vec isn\\textbackslash{}'t deep learning. Its explicitly and deliberately as shallow as possible - one of Mikolov\\textbackslash{}'s central realisations was that a very simple model trained on vast amounts of data can be superior to a complicated model trained on less data.\\textbackslash{}n\\textbackslash{}nThere\\textbackslash{}'s another algorithm called Paragraph2Vec which builds on Word2Vec and *does* incorporate \\textbackslash{}'deep learning\\textbackslash{}' (in so far as it uses a neural network). Its purpose is to amalgamate the Word2Vec vectors for the words in a chunk of text - kinda like you do by averaging them. P2V tries to find a paragraph vector which is a good predictor of the word vectors, with the idea being that such a vector effectively represents the content of the paragraph.'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'I would advise against using RBMs nowadays. If you want to pre-train a deep autoencoder, you can just train shallow autoencoders to do that. It will work just as well as training an RBM and it\\'s much simpler conceptually and in terms of implementation.\\n\\nBut depending on the problem, pre-training may be unnecessary anyway. Just build a deep autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.'\n",
       "2. 'You know basic [lenet](http://deeplearning.net/tutorial/lenet.html) models work really well for even fine grained classification with enough test data.\\n\\nThey have been shown to outperform humans on fine grained imagenet data.\\n\\nI suspect this \"leaves are always on a white background and flat\" view will be even easier.'\n",
       "3. 'pre-training was a bit of a stopgap solution for the vanishing gradient problem in deeper neural nets. When ReLUs were introduced it was discovered that networks with these units suffer from this problem much less, so you can make them a lot deeper without needing tricks like pre-training.'\n",
       "4. <span style=white-space:pre-wrap>'The first, which is an elementary introduction, appears to give a good reason to use cross entropy, the gradients are larger for a bad mistake.    In MSE scoring after a sigmoid final layer, if your model predicts nearly 0 and the correct answer is 1, or vice versa, then gradients of loss function w.r.t. parameters may be very small, and hence learning from that example will be slow.  The first reference assumes that\\'s a vice.\\n\\nIt may be a virtue in some problems.  If you have, for example, a large data set with many examples of many varied types, and you expect that even an excellent model will get many of them wrong, then using MSE, and essentially \"giving up on your dead\" can be the right thing to do.  It\\'s \"example triage\".  Say a statistical learning task. \\n\\nOn the other hand, if your problem is more like \"artificial intelligence\", e.g. a human could conceivably get nearly perfect results because of a deep internal representation, and your examples are truthfully tagged with high accuracy, then penalizing big mistakes immediately is a good idea and will cause learning to be better.  So use cross-entropy/logarithmic loss. '</span>\n",
       "5. 'Word2vec isn\\'t deep learning. Its explicitly and deliberately as shallow as possible - one of Mikolov\\'s central realisations was that a very simple model trained on vast amounts of data can be superior to a complicated model trained on less data.\\n\\nThere\\'s another algorithm called Paragraph2Vec which builds on Word2Vec and *does* incorporate \\'deep learning\\' (in so far as it uses a neural network). Its purpose is to amalgamate the Word2Vec vectors for the words in a chunk of text - kinda like you do by averaging them. P2V tries to find a paragraph vector which is a good predictor of the word vectors, with the idea being that such a vector effectively represents the content of the paragraph.'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"I would advise against using RBMs nowadays. If you want to pre-train a deep autoencoder, you can just train shallow autoencoders to do that. It will work just as well as training an RBM and it's much simpler conceptually and in terms of implementation.\\n\\nBut depending on the problem, pre-training may be unnecessary anyway. Just build a deep autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "[2] \"You know basic [lenet](http://deeplearning.net/tutorial/lenet.html) models work really well for even fine grained classification with enough test data.\\n\\nThey have been shown to outperform humans on fine grained imagenet data.\\n\\nI suspect this \\\"leaves are always on a white background and flat\\\" view will be even easier.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "[3] \"pre-training was a bit of a stopgap solution for the vanishing gradient problem in deeper neural nets. When ReLUs were introduced it was discovered that networks with these units suffer from this problem much less, so you can make them a lot deeper without needing tricks like pre-training.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
       "[4] \"The first, which is an elementary introduction, appears to give a good reason to use cross entropy, the gradients are larger for a bad mistake.    In MSE scoring after a sigmoid final layer, if your model predicts nearly 0 and the correct answer is 1, or vice versa, then gradients of loss function w.r.t. parameters may be very small, and hence learning from that example will be slow.  The first reference assumes that's a vice.\\n\\nIt may be a virtue in some problems.  If you have, for example, a large data set with many examples of many varied types, and you expect that even an excellent model will get many of them wrong, then using MSE, and essentially \\\"giving up on your dead\\\" can be the right thing to do.  It's \\\"example triage\\\".  Say a statistical learning task. \\n\\nOn the other hand, if your problem is more like \\\"artificial intelligence\\\", e.g. a human could conceivably get nearly perfect results because of a deep internal representation, and your examples are truthfully tagged with high accuracy, then penalizing big mistakes immediately is a good idea and will cause learning to be better.  So use cross-entropy/logarithmic loss. \"\n",
       "[5] \"Word2vec isn't deep learning. Its explicitly and deliberately as shallow as possible - one of Mikolov's central realisations was that a very simple model trained on vast amounts of data can be superior to a complicated model trained on less data.\\n\\nThere's another algorithm called Paragraph2Vec which builds on Word2Vec and *does* incorporate 'deep learning' (in so far as it uses a neural network). Its purpose is to amalgamate the Word2Vec vectors for the words in a chunk of text - kinda like you do by averaging them. P2V tries to find a paragraph vector which is a good predictor of the word vectors, with the idea being that such a vector effectively represents the content of the paragraph.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the strings that contain a certain pattern\n",
    "str_subset(data$body[1:100], pattern=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'I WOULD ADVISE AGAINST USING RBMS NOWADAYS. IF YOU WANT TO PRE-TRAIN A DEEP AUTOENCODER, YOU CAN JUS'"
      ],
      "text/latex": [
       "'I WOULD ADVISE AGAINST USING RBMS NOWADAYS. IF YOU WANT TO PRE-TRAIN A DEEP AUTOENCODER, YOU CAN JUS'"
      ],
      "text/markdown": [
       "'I WOULD ADVISE AGAINST USING RBMS NOWADAYS. IF YOU WANT TO PRE-TRAIN A DEEP AUTOENCODER, YOU CAN JUS'"
      ],
      "text/plain": [
       "[1] \"I WOULD ADVISE AGAINST USING RBMS NOWADAYS. IF YOU WANT TO PRE-TRAIN A DEEP AUTOENCODER, YOU CAN JUS\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replace a substring with a new string by substring position\n",
    "str_sub(data$body[1], start=1, end=100) <- str_to_upper(str_sub(data$body[1], \n",
    "                                                                start=1, \n",
    "                                                                end=100))\n",
    "str_sub(data$body[1], start=1, end=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'I WOULD ADVISE AGAINST USING RBMS NOWADAYS. IF YOU WANT TO PRE-TRAIN A multi-layer AUTOENCODER, YOU CAN JUSt train shallow autoencoders to do that. It will work just as well as training an RBM and it\\'s much simpler conceptually and in terms of implementation.\\n\\nBut depending on the problem, pre-training may be unnecessary anyway. Just build a deep autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.'"
      ],
      "text/latex": [
       "'I WOULD ADVISE AGAINST USING RBMS NOWADAYS. IF YOU WANT TO PRE-TRAIN A multi-layer AUTOENCODER, YOU CAN JUSt train shallow autoencoders to do that. It will work just as well as training an RBM and it\\textbackslash{}'s much simpler conceptually and in terms of implementation.\\textbackslash{}n\\textbackslash{}nBut depending on the problem, pre-training may be unnecessary anyway. Just build a deep autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.'"
      ],
      "text/markdown": [
       "'I WOULD ADVISE AGAINST USING RBMS NOWADAYS. IF YOU WANT TO PRE-TRAIN A multi-layer AUTOENCODER, YOU CAN JUSt train shallow autoencoders to do that. It will work just as well as training an RBM and it\\'s much simpler conceptually and in terms of implementation.\\n\\nBut depending on the problem, pre-training may be unnecessary anyway. Just build a deep autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.'"
      ],
      "text/plain": [
       "[1] \"I WOULD ADVISE AGAINST USING RBMS NOWADAYS. IF YOU WANT TO PRE-TRAIN A multi-layer AUTOENCODER, YOU CAN JUSt train shallow autoencoders to do that. It will work just as well as training an RBM and it's much simpler conceptually and in terms of implementation.\\n\\nBut depending on the problem, pre-training may be unnecessary anyway. Just build a deep autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replace first occurrence of a substring with a new string by matching\n",
    "str_replace(data$body[1], pattern=\"deep|DEEP\", replacement=\"multi-layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'I WOULD ADVISE AGAINST USING RBMS NOWADAYS. IF YOU WANT TO PRE-TRAIN A multi-layer AUTOENCODER, YOU CAN JUSt train shallow autoencoders to do that. It will work just as well as training an RBM and it\\'s much simpler conceptually and in terms of implementation.\\n\\nBut depending on the problem, pre-training may be unnecessary anyway. Just build a multi-layer autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.'"
      ],
      "text/latex": [
       "'I WOULD ADVISE AGAINST USING RBMS NOWADAYS. IF YOU WANT TO PRE-TRAIN A multi-layer AUTOENCODER, YOU CAN JUSt train shallow autoencoders to do that. It will work just as well as training an RBM and it\\textbackslash{}'s much simpler conceptually and in terms of implementation.\\textbackslash{}n\\textbackslash{}nBut depending on the problem, pre-training may be unnecessary anyway. Just build a multi-layer autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.'"
      ],
      "text/markdown": [
       "'I WOULD ADVISE AGAINST USING RBMS NOWADAYS. IF YOU WANT TO PRE-TRAIN A multi-layer AUTOENCODER, YOU CAN JUSt train shallow autoencoders to do that. It will work just as well as training an RBM and it\\'s much simpler conceptually and in terms of implementation.\\n\\nBut depending on the problem, pre-training may be unnecessary anyway. Just build a multi-layer autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.'"
      ],
      "text/plain": [
       "[1] \"I WOULD ADVISE AGAINST USING RBMS NOWADAYS. IF YOU WANT TO PRE-TRAIN A multi-layer AUTOENCODER, YOU CAN JUSt train shallow autoencoders to do that. It will work just as well as training an RBM and it's much simpler conceptually and in terms of implementation.\\n\\nBut depending on the problem, pre-training may be unnecessary anyway. Just build a multi-layer autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replace all occurrences of a substring with a new string by matching\n",
    "str_replace_all(data$body[1], pattern=\"deep|DEEP\", replacement=\"multi-layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stringr: Viewing Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1] \"I WOULD ADVISE AGAINST USING RBMS NOWADAYS. IF YOU WANT TO PRE-TRAIN A DEEP AUTOENCODER, YOU CAN JUSt train shallow autoencoders to do that. It will work just as well as training an RBM and it's much simpler conceptually and in terms of implementation.\\n\\nBut depending on the problem, pre-training may be unnecessary anyway. Just build a deep autoencoder with ReLUs and possibly dropout and you should be able to train that end-to-end in many cases.\"\n",
      " [2] \"Hdf5. It's structured, it's easy to get data in and out, and it's fast. Plus it will scale if you ever get up there in dataset size. \"                                                                                                                                                                                                                                                                                                                             \n",
      " [3] \"yep, good point.\"                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
      " [4] \"Google must have done (and is doing) serious internal research in ranking. I've heard they're pretty good at that and they've even made some money doing it :P\"                                                                                                                                                                                                                                                                                                    \n",
      " [5] \"[deleted]\"                                                                                                                                                                                                                                                                                                                                                                                                                                                         \n",
      " [6] \"Sebastian Thrun's book, Probabilistic Robotics, goes through this in great detail. Get it, read it, make it your bible. \"                                                                                                                                                                                                                                                                                                                                          \n",
      " [7] \"This. Such a fucking legendary book. Kalman filters, particle filters, recursive Bayesian filters and a whole bunch of other stuff. I learnt so much. \\n\\nRead these 3 for starts from the book, then come back and ask the questions \"                                                                                                                                                                                                                            \n",
      " [8] \"Do you still need help?\"                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
      " [9] \"Thanks, I am aware of that book. I was trying to avoid reading it because I don't have that much time. I was hoping to find a paper on this topic instead of a book. Thanks anyways.\"                                                                                                                                                                                                                                                                              \n",
      "[10] \"In something like a CRF, a gazetteer feature would be a binary feature indicating whether the word was in your gazetteer. ( or phrase)\"                                                                                                                                                                                                                                                                                                                            \n"
     ]
    }
   ],
   "source": [
    "# Basic printing\n",
    "print(data$body[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!doctype html>\n",
       "<html>\n",
       "\t<head>\n",
       "\t\t<meta charset=\"utf-8\">\n",
       "\t\t<script title=\"htmlwidgets\" src=\"data:application/javascript;base64,(function() {
  // If window.HTMLWidgets is already defined, then use it; otherwise create a
  // new object. This allows preceding code to set options that affect the
  // initialization process (though none currently exist).
  window.HTMLWidgets = window.HTMLWidgets || {};

  // See if we're running in a viewer pane. If not, we're in a web browser.
  var viewerMode = window.HTMLWidgets.viewerMode =
      /\bviewer_pane=1\b/.test(window.location);

  // See if we're running in Shiny mode. If not, it's a static document.
  // Note that static widgets can appear in both Shiny and static modes, but
  // obviously, Shiny widgets can only appear in Shiny apps/documents.
  var shinyMode = window.HTMLWidgets.shinyMode =
      typeof(window.Shiny) !== "undefined" && !!window.Shiny.outputBindings;

  // We can't count on jQuery being available, so we implement our own
  // version if necessary.
  function querySelectorAll(scope, selector) {
    if (typeof(jQuery) !== "undefined" && scope instanceof jQuery) {
      return scope.find(selector);
    }
    if (scope.querySelectorAll) {
      return scope.querySelectorAll(selector);
    }
  }

  function asArray(value) {
    if (value === null)
      return [];
    if ($.isArray(value))
      return value;
    return [value];
  }

  // Implement jQuery's extend
  function extend(target /*, ... */) {
    if (arguments.length == 1) {
      return target;
    }
    for (var i = 1; i < arguments.length; i++) {
      var source = arguments[i];
      for (var prop in source) {
        if (source.hasOwnProperty(prop)) {
          target[prop] = source[prop];
        }
      }
    }
    return target;
  }

  // IE8 doesn't support Array.forEach.
  function forEach(values, callback, thisArg) {
    if (values.forEach) {
      values.forEach(callback, thisArg);
    } else {
      for (var i = 0; i < values.length; i++) {
        callback.call(thisArg, values[i], i, values);
      }
    }
  }

  // Replaces the specified method with the return value of funcSource.
  //
  // Note that funcSource should not BE the new method, it should be a function
  // that RETURNS the new method. funcSource receives a single argument that is
  // the overridden method, it can be called from the new method. The overridden
  // method can be called like a regular function, it has the target permanently
  // bound to it so "this" will work correctly.
  function overrideMethod(target, methodName, funcSource) {
    var superFunc = target[methodName] || function() {};
    var superFuncBound = function() {
      return superFunc.apply(target, arguments);
    };
    target[methodName] = funcSource(superFuncBound);
  }

  // Add a method to delegator that, when invoked, calls
  // delegatee.methodName. If there is no such method on
  // the delegatee, but there was one on delegator before
  // delegateMethod was called, then the original version
  // is invoked instead.
  // For example:
  //
  // var a = {
  //   method1: function() { console.log('a1'); }
  //   method2: function() { console.log('a2'); }
  // };
  // var b = {
  //   method1: function() { console.log('b1'); }
  // };
  // delegateMethod(a, b, "method1");
  // delegateMethod(a, b, "method2");
  // a.method1();
  // a.method2();
  //
  // The output would be "b1", "a2".
  function delegateMethod(delegator, delegatee, methodName) {
    var inherited = delegator[methodName];
    delegator[methodName] = function() {
      var target = delegatee;
      var method = delegatee[methodName];

      // The method doesn't exist on the delegatee. Instead,
      // call the method on the delegator, if it exists.
      if (!method) {
        target = delegator;
        method = inherited;
      }

      if (method) {
        return method.apply(target, arguments);
      }
    };
  }

  // Implement a vague facsimilie of jQuery's data method
  function elementData(el, name, value) {
    if (arguments.length == 2) {
      return el["htmlwidget_data_" + name];
    } else if (arguments.length == 3) {
      el["htmlwidget_data_" + name] = value;
      return el;
    } else {
      throw new Error("Wrong number of arguments for elementData: " +
        arguments.length);
    }
  }

  // http://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
  function escapeRegExp(str) {
    return str.replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
  }

  function hasClass(el, className) {
    var re = new RegExp("\\b" + escapeRegExp(className) + "\\b");
    return re.test(el.className);
  }

  // elements - array (or array-like object) of HTML elements
  // className - class name to test for
  // include - if true, only return elements with given className;
  //   if false, only return elements *without* given className
  function filterByClass(elements, className, include) {
    var results = [];
    for (var i = 0; i < elements.length; i++) {
      if (hasClass(elements[i], className) == include)
        results.push(elements[i]);
    }
    return results;
  }

  function on(obj, eventName, func) {
    if (obj.addEventListener) {
      obj.addEventListener(eventName, func, false);
    } else if (obj.attachEvent) {
      obj.attachEvent(eventName, func);
    }
  }

  function off(obj, eventName, func) {
    if (obj.removeEventListener)
      obj.removeEventListener(eventName, func, false);
    else if (obj.detachEvent) {
      obj.detachEvent(eventName, func);
    }
  }

  // Translate array of values to top/right/bottom/left, as usual with
  // the "padding" CSS property
  // https://developer.mozilla.org/en-US/docs/Web/CSS/padding
  function unpackPadding(value) {
    if (typeof(value) === "number")
      value = [value];
    if (value.length === 1) {
      return {top: value[0], right: value[0], bottom: value[0], left: value[0]};
    }
    if (value.length === 2) {
      return {top: value[0], right: value[1], bottom: value[0], left: value[1]};
    }
    if (value.length === 3) {
      return {top: value[0], right: value[1], bottom: value[2], left: value[1]};
    }
    if (value.length === 4) {
      return {top: value[0], right: value[1], bottom: value[2], left: value[3]};
    }
  }

  // Convert an unpacked padding object to a CSS value
  function paddingToCss(paddingObj) {
    return paddingObj.top + "px " + paddingObj.right + "px " + paddingObj.bottom + "px " + paddingObj.left + "px";
  }

  // Makes a number suitable for CSS
  function px(x) {
    if (typeof(x) === "number")
      return x + "px";
    else
      return x;
  }

  // Retrieves runtime widget sizing information for an element.
  // The return value is either null, or an object with fill, padding,
  // defaultWidth, defaultHeight fields.
  function sizingPolicy(el) {
    var sizingEl = document.querySelector("script[data-for='" + el.id + "'][type='application/htmlwidget-sizing']");
    if (!sizingEl)
      return null;
    var sp = JSON.parse(sizingEl.textContent || sizingEl.text || "{}");
    if (viewerMode) {
      return sp.viewer;
    } else {
      return sp.browser;
    }
  }

  // @param tasks Array of strings (or falsy value, in which case no-op).
  //   Each element must be a valid JavaScript expression that yields a
  //   function. Or, can be an array of objects with "code" and "data"
  //   properties; in this case, the "code" property should be a string
  //   of JS that's an expr that yields a function, and "data" should be
  //   an object that will be added as an additional argument when that
  //   function is called.
  // @param target The object that will be "this" for each function
  //   execution.
  // @param args Array of arguments to be passed to the functions. (The
  //   same arguments will be passed to all functions.)
  function evalAndRun(tasks, target, args) {
    if (tasks) {
      forEach(tasks, function(task) {
        var theseArgs = args;
        if (typeof(task) === "object") {
          theseArgs = theseArgs.concat([task.data]);
          task = task.code;
        }
        var taskFunc = tryEval(task);
        if (typeof(taskFunc) !== "function") {
          throw new Error("Task must be a function! Source:\n" + task);
        }
        taskFunc.apply(target, theseArgs);
      });
    }
  }

  // Attempt eval() both with and without enclosing in parentheses.
  // Note that enclosing coerces a function declaration into
  // an expression that eval() can parse
  // (otherwise, a SyntaxError is thrown)
  function tryEval(code) {
    var result = null;
    try {
      result = eval(code);
    } catch(error) {
      if (!error instanceof SyntaxError) {
        throw error;
      }
      try {
        result = eval("(" + code + ")");
      } catch(e) {
        if (e instanceof SyntaxError) {
          throw error;
        } else {
          throw e;
        }
      }
    }
    return result;
  }

  function initSizing(el) {
    var sizing = sizingPolicy(el);
    if (!sizing)
      return;

    var cel = document.getElementById("htmlwidget_container");
    if (!cel)
      return;

    if (typeof(sizing.padding) !== "undefined") {
      document.body.style.margin = "0";
      document.body.style.padding = paddingToCss(unpackPadding(sizing.padding));
    }

    if (sizing.fill) {
      document.body.style.overflow = "hidden";
      document.body.style.width = "100%";
      document.body.style.height = "100%";
      document.documentElement.style.width = "100%";
      document.documentElement.style.height = "100%";
      if (cel) {
        cel.style.position = "absolute";
        var pad = unpackPadding(sizing.padding);
        cel.style.top = pad.top + "px";
        cel.style.right = pad.right + "px";
        cel.style.bottom = pad.bottom + "px";
        cel.style.left = pad.left + "px";
        el.style.width = "100%";
        el.style.height = "100%";
      }

      return {
        getWidth: function() { return cel.offsetWidth; },
        getHeight: function() { return cel.offsetHeight; }
      };

    } else {
      el.style.width = px(sizing.width);
      el.style.height = px(sizing.height);

      return {
        getWidth: function() { return el.offsetWidth; },
        getHeight: function() { return el.offsetHeight; }
      };
    }
  }

  // Default implementations for methods
  var defaults = {
    find: function(scope) {
      return querySelectorAll(scope, "." + this.name);
    },
    renderError: function(el, err) {
      var $el = $(el);

      this.clearError(el);

      // Add all these error classes, as Shiny does
      var errClass = "shiny-output-error";
      if (err.type !== null) {
        // use the classes of the error condition as CSS class names
        errClass = errClass + " " + $.map(asArray(err.type), function(type) {
          return errClass + "-" + type;
        }).join(" ");
      }
      errClass = errClass + " htmlwidgets-error";

      // Is el inline or block? If inline or inline-block, just display:none it
      // and add an inline error.
      var display = $el.css("display");
      $el.data("restore-display-mode", display);

      if (display === "inline" || display === "inline-block") {
        $el.hide();
        if (err.message !== "") {
          var errorSpan = $("<span>").addClass(errClass);
          errorSpan.text(err.message);
          $el.after(errorSpan);
        }
      } else if (display === "block") {
        // If block, add an error just after the el, set visibility:none on the
        // el, and position the error to be on top of the el.
        // Mark it with a unique ID and CSS class so we can remove it later.
        $el.css("visibility", "hidden");
        if (err.message !== "") {
          var errorDiv = $("<div>").addClass(errClass).css("position", "absolute")
            .css("top", el.offsetTop)
            .css("left", el.offsetLeft)
            // setting width can push out the page size, forcing otherwise
            // unnecessary scrollbars to appear and making it impossible for
            // the element to shrink; so use max-width instead
            .css("maxWidth", el.offsetWidth)
            .css("height", el.offsetHeight);
          errorDiv.text(err.message);
          $el.after(errorDiv);

          // Really dumb way to keep the size/position of the error in sync with
          // the parent element as the window is resized or whatever.
          var intId = setInterval(function() {
            if (!errorDiv[0].parentElement) {
              clearInterval(intId);
              return;
            }
            errorDiv
              .css("top", el.offsetTop)
              .css("left", el.offsetLeft)
              .css("maxWidth", el.offsetWidth)
              .css("height", el.offsetHeight);
          }, 500);
        }
      }
    },
    clearError: function(el) {
      var $el = $(el);
      var display = $el.data("restore-display-mode");
      $el.data("restore-display-mode", null);

      if (display === "inline" || display === "inline-block") {
        if (display)
          $el.css("display", display);
        $(el.nextSibling).filter(".htmlwidgets-error").remove();
      } else if (display === "block"){
        $el.css("visibility", "inherit");
        $(el.nextSibling).filter(".htmlwidgets-error").remove();
      }
    },
    sizing: {}
  };

  // Called by widget bindings to register a new type of widget. The definition
  // object can contain the following properties:
  // - name (required) - A string indicating the binding name, which will be
  //   used by default as the CSS classname to look for.
  // - initialize (optional) - A function(el) that will be called once per
  //   widget element; if a value is returned, it will be passed as the third
  //   value to renderValue.
  // - renderValue (required) - A function(el, data, initValue) that will be
  //   called with data. Static contexts will cause this to be called once per
  //   element; Shiny apps will cause this to be called multiple times per
  //   element, as the data changes.
  window.HTMLWidgets.widget = function(definition) {
    if (!definition.name) {
      throw new Error("Widget must have a name");
    }
    if (!definition.type) {
      throw new Error("Widget must have a type");
    }
    // Currently we only support output widgets
    if (definition.type !== "output") {
      throw new Error("Unrecognized widget type '" + definition.type + "'");
    }
    // TODO: Verify that .name is a valid CSS classname

    // Support new-style instance-bound definitions. Old-style class-bound
    // definitions have one widget "object" per widget per type/class of
    // widget; the renderValue and resize methods on such widget objects
    // take el and instance arguments, because the widget object can't
    // store them. New-style instance-bound definitions have one widget
    // object per widget instance; the definition that's passed in doesn't
    // provide renderValue or resize methods at all, just the single method
    //   factory(el, width, height)
    // which returns an object that has renderValue(x) and resize(w, h).
    // This enables a far more natural programming style for the widget
    // author, who can store per-instance state using either OO-style
    // instance fields or functional-style closure variables (I guess this
    // is in contrast to what can only be called C-style pseudo-OO which is
    // what we required before).
    if (definition.factory) {
      definition = createLegacyDefinitionAdapter(definition);
    }

    if (!definition.renderValue) {
      throw new Error("Widget must have a renderValue function");
    }

    // For static rendering (non-Shiny), use a simple widget registration
    // scheme. We also use this scheme for Shiny apps/documents that also
    // contain static widgets.
    window.HTMLWidgets.widgets = window.HTMLWidgets.widgets || [];
    // Merge defaults into the definition; don't mutate the original definition.
    var staticBinding = extend({}, defaults, definition);
    overrideMethod(staticBinding, "find", function(superfunc) {
      return function(scope) {
        var results = superfunc(scope);
        // Filter out Shiny outputs, we only want the static kind
        return filterByClass(results, "html-widget-output", false);
      };
    });
    window.HTMLWidgets.widgets.push(staticBinding);

    if (shinyMode) {
      // Shiny is running. Register the definition with an output binding.
      // The definition itself will not be the output binding, instead
      // we will make an output binding object that delegates to the
      // definition. This is because we foolishly used the same method
      // name (renderValue) for htmlwidgets definition and Shiny bindings
      // but they actually have quite different semantics (the Shiny
      // bindings receive data that includes lots of metadata that it
      // strips off before calling htmlwidgets renderValue). We can't
      // just ignore the difference because in some widgets it's helpful
      // to call this.renderValue() from inside of resize(), and if
      // we're not delegating, then that call will go to the Shiny
      // version instead of the htmlwidgets version.

      // Merge defaults with definition, without mutating either.
      var bindingDef = extend({}, defaults, definition);

      // This object will be our actual Shiny binding.
      var shinyBinding = new Shiny.OutputBinding();

      // With a few exceptions, we'll want to simply use the bindingDef's
      // version of methods if they are available, otherwise fall back to
      // Shiny's defaults. NOTE: If Shiny's output bindings gain additional
      // methods in the future, and we want them to be overrideable by
      // HTMLWidget binding definitions, then we'll need to add them to this
      // list.
      delegateMethod(shinyBinding, bindingDef, "getId");
      delegateMethod(shinyBinding, bindingDef, "onValueChange");
      delegateMethod(shinyBinding, bindingDef, "onValueError");
      delegateMethod(shinyBinding, bindingDef, "renderError");
      delegateMethod(shinyBinding, bindingDef, "clearError");
      delegateMethod(shinyBinding, bindingDef, "showProgress");

      // The find, renderValue, and resize are handled differently, because we
      // want to actually decorate the behavior of the bindingDef methods.

      shinyBinding.find = function(scope) {
        var results = bindingDef.find(scope);

        // Only return elements that are Shiny outputs, not static ones
        var dynamicResults = results.filter(".html-widget-output");

        // It's possible that whatever caused Shiny to think there might be
        // new dynamic outputs, also caused there to be new static outputs.
        // Since there might be lots of different htmlwidgets bindings, we
        // schedule execution for later--no need to staticRender multiple
        // times.
        if (results.length !== dynamicResults.length)
          scheduleStaticRender();

        return dynamicResults;
      };

      // Wrap renderValue to handle initialization, which unfortunately isn't
      // supported natively by Shiny at the time of this writing.

      shinyBinding.renderValue = function(el, data) {
        Shiny.renderDependencies(data.deps);
        // Resolve strings marked as javascript literals to objects
        if (!(data.evals instanceof Array)) data.evals = [data.evals];
        for (var i = 0; data.evals && i < data.evals.length; i++) {
          window.HTMLWidgets.evaluateStringMember(data.x, data.evals[i]);
        }
        if (!bindingDef.renderOnNullValue) {
          if (data.x === null) {
            el.style.visibility = "hidden";
            return;
          } else {
            el.style.visibility = "inherit";
          }
        }
        if (!elementData(el, "initialized")) {
          initSizing(el);

          elementData(el, "initialized", true);
          if (bindingDef.initialize) {
            var result = bindingDef.initialize(el, el.offsetWidth,
              el.offsetHeight);
            elementData(el, "init_result", result);
          }
        }
        bindingDef.renderValue(el, data.x, elementData(el, "init_result"));
        evalAndRun(data.jsHooks.render, elementData(el, "init_result"), [el, data.x]);
      };

      // Only override resize if bindingDef implements it
      if (bindingDef.resize) {
        shinyBinding.resize = function(el, width, height) {
          // Shiny can call resize before initialize/renderValue have been
          // called, which doesn't make sense for widgets.
          if (elementData(el, "initialized")) {
            bindingDef.resize(el, width, height, elementData(el, "init_result"));
          }
        };
      }

      Shiny.outputBindings.register(shinyBinding, bindingDef.name);
    }
  };

  var scheduleStaticRenderTimerId = null;
  function scheduleStaticRender() {
    if (!scheduleStaticRenderTimerId) {
      scheduleStaticRenderTimerId = setTimeout(function() {
        scheduleStaticRenderTimerId = null;
        window.HTMLWidgets.staticRender();
      }, 1);
    }
  }

  // Render static widgets after the document finishes loading
  // Statically render all elements that are of this widget's class
  window.HTMLWidgets.staticRender = function() {
    var bindings = window.HTMLWidgets.widgets || [];
    forEach(bindings, function(binding) {
      var matches = binding.find(document.documentElement);
      forEach(matches, function(el) {
        var sizeObj = initSizing(el, binding);

        if (hasClass(el, "html-widget-static-bound"))
          return;
        el.className = el.className + " html-widget-static-bound";

        var initResult;
        if (binding.initialize) {
          initResult = binding.initialize(el,
            sizeObj ? sizeObj.getWidth() : el.offsetWidth,
            sizeObj ? sizeObj.getHeight() : el.offsetHeight
          );
          elementData(el, "init_result", initResult);
        }

        if (binding.resize) {
          var lastSize = {
            w: sizeObj ? sizeObj.getWidth() : el.offsetWidth,
            h: sizeObj ? sizeObj.getHeight() : el.offsetHeight
          };
          var resizeHandler = function(e) {
            var size = {
              w: sizeObj ? sizeObj.getWidth() : el.offsetWidth,
              h: sizeObj ? sizeObj.getHeight() : el.offsetHeight
            };
            if (size.w === 0 && size.h === 0)
              return;
            if (size.w === lastSize.w && size.h === lastSize.h)
              return;
            lastSize = size;
            binding.resize(el, size.w, size.h, initResult);
          };

          on(window, "resize", resizeHandler);

          // This is needed for cases where we're running in a Shiny
          // app, but the widget itself is not a Shiny output, but
          // rather a simple static widget. One example of this is
          // an rmarkdown document that has runtime:shiny and widget
          // that isn't in a render function. Shiny only knows to
          // call resize handlers for Shiny outputs, not for static
          // widgets, so we do it ourselves.
          if (window.jQuery) {
            window.jQuery(document).on(
              "shown.htmlwidgets shown.bs.tab.htmlwidgets shown.bs.collapse.htmlwidgets",
              resizeHandler
            );
            window.jQuery(document).on(
              "hidden.htmlwidgets hidden.bs.tab.htmlwidgets hidden.bs.collapse.htmlwidgets",
              resizeHandler
            );
          }

          // This is needed for the specific case of ioslides, which
          // flips slides between display:none and display:block.
          // Ideally we would not have to have ioslide-specific code
          // here, but rather have ioslides raise a generic event,
          // but the rmarkdown package just went to CRAN so the
          // window to getting that fixed may be long.
          if (window.addEventListener) {
            // It's OK to limit this to window.addEventListener
            // browsers because ioslides itself only supports
            // such browsers.
            on(document, "slideenter", resizeHandler);
            on(document, "slideleave", resizeHandler);
          }
        }

        var scriptData = document.querySelector("script[data-for='" + el.id + "'][type='application/json']");
        if (scriptData) {
          var data = JSON.parse(scriptData.textContent || scriptData.text);
          // Resolve strings marked as javascript literals to objects
          if (!(data.evals instanceof Array)) data.evals = [data.evals];
          for (var k = 0; data.evals && k < data.evals.length; k++) {
            window.HTMLWidgets.evaluateStringMember(data.x, data.evals[k]);
          }
          binding.renderValue(el, data.x, initResult);
          evalAndRun(data.jsHooks.render, initResult, [el, data.x]);
        }
      });
    });

    invokePostRenderHandlers();
  }


  function has_jQuery3() {
    if (!window.jQuery) {
      return false;
    }
    var $version = window.jQuery.fn.jquery;
    var $major_version = parseInt($version.split(".")[0]);
    return $major_version >= 3;
  }

  /*
  / Shiny 1.4 bumped jQuery from 1.x to 3.x which means jQuery's
  / on-ready handler (i.e., $(fn)) is now asyncronous (i.e., it now
  / really means $(setTimeout(fn)).
  / https://jquery.com/upgrade-guide/3.0/#breaking-change-document-ready-handlers-are-now-asynchronous
  /
  / Since Shiny uses $() to schedule initShiny, shiny>=1.4 calls initShiny
  / one tick later than it did before, which means staticRender() is
  / called renderValue() earlier than (advanced) widget authors might be expecting.
  / https://github.com/rstudio/shiny/issues/2630
  /
  / For a concrete example, leaflet has some methods (e.g., updateBounds)
  / which reference Shiny methods registered in initShiny (e.g., setInputValue).
  / Since leaflet is privy to this life-cycle, it knows to use setTimeout() to
  / delay execution of those methods (until Shiny methods are ready)
  / https://github.com/rstudio/leaflet/blob/18ec981/javascript/src/index.js#L266-L268
  /
  / Ideally widget authors wouldn't need to use this setTimeout() hack that
  / leaflet uses to call Shiny methods on a staticRender(). In the long run,
  / the logic initShiny should be broken up so that method registration happens
  / right away, but binding happens later.
  */
  function maybeStaticRenderLater() {
    if (shinyMode && has_jQuery3()) {
      window.jQuery(window.HTMLWidgets.staticRender);
    } else {
      window.HTMLWidgets.staticRender();
    }
  }

  if (document.addEventListener) {
    document.addEventListener("DOMContentLoaded", function() {
      document.removeEventListener("DOMContentLoaded", arguments.callee, false);
      maybeStaticRenderLater();
    }, false);
  } else if (document.attachEvent) {
    document.attachEvent("onreadystatechange", function() {
      if (document.readyState === "complete") {
        document.detachEvent("onreadystatechange", arguments.callee);
        maybeStaticRenderLater();
      }
    });
  }


  window.HTMLWidgets.getAttachmentUrl = function(depname, key) {
    // If no key, default to the first item
    if (typeof(key) === "undefined")
      key = 1;

    var link = document.getElementById(depname + "-" + key + "-attachment");
    if (!link) {
      throw new Error("Attachment " + depname + "/" + key + " not found in document");
    }
    return link.getAttribute("href");
  };

  window.HTMLWidgets.dataframeToD3 = function(df) {
    var names = [];
    var length;
    for (var name in df) {
        if (df.hasOwnProperty(name))
            names.push(name);
        if (typeof(df[name]) !== "object" || typeof(df[name].length) === "undefined") {
            throw new Error("All fields must be arrays");
        } else if (typeof(length) !== "undefined" && length !== df[name].length) {
            throw new Error("All fields must be arrays of the same length");
        }
        length = df[name].length;
    }
    var results = [];
    var item;
    for (var row = 0; row < length; row++) {
        item = {};
        for (var col = 0; col < names.length; col++) {
            item[names[col]] = df[names[col]][row];
        }
        results.push(item);
    }
    return results;
  };

  window.HTMLWidgets.transposeArray2D = function(array) {
      if (array.length === 0) return array;
      var newArray = array[0].map(function(col, i) {
          return array.map(function(row) {
              return row[i]
          })
      });
      return newArray;
  };
  // Split value at splitChar, but allow splitChar to be escaped
  // using escapeChar. Any other characters escaped by escapeChar
  // will be included as usual (including escapeChar itself).
  function splitWithEscape(value, splitChar, escapeChar) {
    var results = [];
    var escapeMode = false;
    var currentResult = "";
    for (var pos = 0; pos < value.length; pos++) {
      if (!escapeMode) {
        if (value[pos] === splitChar) {
          results.push(currentResult);
          currentResult = "";
        } else if (value[pos] === escapeChar) {
          escapeMode = true;
        } else {
          currentResult += value[pos];
        }
      } else {
        currentResult += value[pos];
        escapeMode = false;
      }
    }
    if (currentResult !== "") {
      results.push(currentResult);
    }
    return results;
  }
  // Function authored by Yihui/JJ Allaire
  window.HTMLWidgets.evaluateStringMember = function(o, member) {
    var parts = splitWithEscape(member, '.', '\\');
    for (var i = 0, l = parts.length; i < l; i++) {
      var part = parts[i];
      // part may be a character or 'numeric' member name
      if (o !== null && typeof o === "object" && part in o) {
        if (i == (l - 1)) { // if we are at the end of the line then evalulate
          if (typeof o[part] === "string")
            o[part] = tryEval(o[part]);
        } else { // otherwise continue to next embedded object
          o = o[part];
        }
      }
    }
  };

  // Retrieve the HTMLWidget instance (i.e. the return value of an
  // HTMLWidget binding's initialize() or factory() function)
  // associated with an element, or null if none.
  window.HTMLWidgets.getInstance = function(el) {
    return elementData(el, "init_result");
  };

  // Finds the first element in the scope that matches the selector,
  // and returns the HTMLWidget instance (i.e. the return value of
  // an HTMLWidget binding's initialize() or factory() function)
  // associated with that element, if any. If no element matches the
  // selector, or the first matching element has no HTMLWidget
  // instance associated with it, then null is returned.
  //
  // The scope argument is optional, and defaults to window.document.
  window.HTMLWidgets.find = function(scope, selector) {
    if (arguments.length == 1) {
      selector = scope;
      scope = document;
    }

    var el = scope.querySelector(selector);
    if (el === null) {
      return null;
    } else {
      return window.HTMLWidgets.getInstance(el);
    }
  };

  // Finds all elements in the scope that match the selector, and
  // returns the HTMLWidget instances (i.e. the return values of
  // an HTMLWidget binding's initialize() or factory() function)
  // associated with the elements, in an array. If elements that
  // match the selector don't have an associated HTMLWidget
  // instance, the returned array will contain nulls.
  //
  // The scope argument is optional, and defaults to window.document.
  window.HTMLWidgets.findAll = function(scope, selector) {
    if (arguments.length == 1) {
      selector = scope;
      scope = document;
    }

    var nodes = scope.querySelectorAll(selector);
    var results = [];
    for (var i = 0; i < nodes.length; i++) {
      results.push(window.HTMLWidgets.getInstance(nodes[i]));
    }
    return results;
  };

  var postRenderHandlers = [];
  function invokePostRenderHandlers() {
    while (postRenderHandlers.length) {
      var handler = postRenderHandlers.shift();
      if (handler) {
        handler();
      }
    }
  }

  // Register the given callback function to be invoked after the
  // next time static widgets are rendered.
  window.HTMLWidgets.addPostRenderHandler = function(callback) {
    postRenderHandlers.push(callback);
  };

  // Takes a new-style instance-bound definition, and returns an
  // old-style class-bound definition. This saves us from having
  // to rewrite all the logic in this file to accomodate both
  // types of definitions.
  function createLegacyDefinitionAdapter(defn) {
    var result = {
      name: defn.name,
      type: defn.type,
      initialize: function(el, width, height) {
        return defn.factory(el, width, height);
      },
      renderValue: function(el, x, instance) {
        return instance.renderValue(x);
      },
      resize: function(el, width, height, instance) {
        return instance.resize(width, height);
      }
    };

    if (defn.find)
      result.find = defn.find;
    if (defn.renderError)
      result.renderError = defn.renderError;
    if (defn.clearError)
      result.clearError = defn.clearError;

    return result;
  }
})();

\"></script>\n",
       "<link href=\"data:text/css;charset-utf-8;base64,LnN0cl92aWV3IHVsLCAuc3RyX3ZpZXcgbGkgewogIGxpc3Qtc3R5bGU6IG5vbmU7CiAgcGFkZGluZzogMDsKICBtYXJnaW46IDAuNWVtIDA7CiAgZm9udC1mYW1pbHk6IG1vbm9zcGFjZTsKfQoKLnN0cl92aWV3IC5tYXRjaCB7CiAgYm9yZGVyOiAxcHggc29saWQgI2NjYzsKICBiYWNrZ3JvdW5kLWNvbG9yOiAjZWVlOwp9Cg==\" rel=\"stylesheet\" />\n",
       "<script title=\"str_view-binding\" src=\"data:application/javascript;base64,SFRNTFdpZGdldHMud2lkZ2V0KHsKCiAgbmFtZTogJ3N0cl92aWV3JywKCiAgdHlwZTogJ291dHB1dCcsCgogIGluaXRpYWxpemU6IGZ1bmN0aW9uKGVsLCB3aWR0aCwgaGVpZ2h0KSB7CiAgfSwKCiAgcmVuZGVyVmFsdWU6IGZ1bmN0aW9uKGVsLCB4LCBpbnN0YW5jZSkgewogICAgZWwuaW5uZXJIVE1MID0geC5odG1sOwogIH0sCgogIHJlc2l6ZTogZnVuY3Rpb24oZWwsIHdpZHRoLCBoZWlnaHQsIGluc3RhbmNlKSB7CiAgfQoKfSk7Cg==\"></script>\n",
       "\t</head>\n",
       "\t<body>\n",
       "\t\t<div id=\"htmlwidget-99d94a47e9866b55640c\" style=\"width:960px;height:300px;\" class=\"str_view html-widget\"></div>\n",
       "<script type=\"application/json\" data-for=\"htmlwidget-99d94a47e9866b55640c\">{\"x\":{\"html\":\"<ul>\\n  <li>Word2vec isn't <span class='match'>deep<\\/span> learning. Its explicitly and deliberately as shallow as possible - one of Mikolov's central realisations was that a very simple model trained on vast amounts of data can be superior to a complicated model trained on less data.\\n\\nThere's another algorithm called Paragraph2Vec which builds on Word2Vec and *does* incorporate 'deep learning' (in so far as it uses a neural network). Its purpose is to amalgamate the Word2Vec vectors for the words in a chunk of text - kinda like you do by averaging them. P2V tries to find a paragraph vector which is a good predictor of the word vectors, with the idea being that such a vector effectively represents the content of the paragraph.<\\/li>\\n  <li>But even if it is, so what? Such experiment could pay off if we would be able to figure out how to pick best model from 100 or even more runs.\\n\\nELM simply would learn better model than whatever <span class='match'>deep<\\/span> learning method learns. But my understanding that it is not what they claim, ELM proponents say that while performance is not on par with deep learning, advantage is that you barely need any computations to learn, so model learns faster in terms of add-multiply computations.\\n<\\/li>\\n  <li>I think the extremely fast training speed claimed by ELM is something like cheating.\\nFor example, ELM papers usually report training mnist in few minutes while <span class='match'>deep<\\/span> learning in several hours, or training other (little more complex than mnist) toy datasets in hours while deep learning in days. I am sure they elaborately chosed some slowest deep learning algrithims in out-of-date papers with very naive implementations.\\nAs a fair comparison, I can train an 8 layer imagenet classification model in 15 hours using one GTX780, with top 1 error rate same as AlexNet, and I am sure again, during the ELM research lifetime, they can not get this fast training speed<\\/li>\\n  <li>RBMs are still state of the art at modeling binary data, afaik (although this may have changed in the last year or so as I haven't been keeping up).\\n\\nBut for pre-training you can use autoencoders, which is much easier (no intractability, no need for approximations). And as a generative model of non-binary data, RBMs perform very poorly (even Gaussian mixture models often beat Gaussian RBMs).\\n\\nIt seems that the <span class='match'>deep<\\/span> learning community is slowly getting over the 'convnet explosion' of 2012 a bit, and there's a lot more interest again in generative models (along with recurrent models). Variational autoencoders are just one example, a lot of new ideas for generative models are currently being published.<\\/li>\\n  <li>Hey guys- I came across this yesterday and I was at first startled by its accuracy.  Then I began to notice-\\n\\n* It's really slow- 10-15 seconds is typical.\\n* The results are rather inconsistent.\\n* I've come across misspellings, like \\\"ell\\\" instead of \\\"eel\\\"\\n\\nA press release from them says:\\n\\n&gt;\\\"Using <span class='match'>deep<\\/span> learning, we set up an advanced image recognition technology to give results from our servers,\\\" says Mazur. \\\"With this, we can go deeply into identifying, say, the exact make and model of a car or breed of dog — not just a classification. What sets us apart is that we always provide an answer with a varying degree of detail. It's not just an exact answer or no answer at all.\\\"\\n\\nhttp://www.prnewswire.com/news-releases/camfind---the-first-visual-search-engine-goes-social-300067412.html\\n\\nWhat do you guys think?\\n\\n**edit** By \\\"human\\\" I'm thinking Mechanical Turk.<\\/li>\\n  <li>It did get me thinking along those lines with my own project actually.  It's not a bad solution in a lot of cases.  \\n\\nI'm just dying of curiosity about this particular service though.  I'm pretty sure <span class='match'>deep<\\/span> learning isn't *this* good yet, but I can't find a way to prove it's not using it.<\\/li>\\n  <li>Right, the answer could have been found via google reverse image search or something similar, but definitely not <span class='match'>deep<\\/span> learning image recognition. You know what I mean?<\\/li>\\n  <li>Yeah, I think this system has a lot of different parts. Definitely OCR and dominant color detection, probably reverse image search and maybe <span class='match'>deep<\\/span> learning (ImageNet classifier would probably detect that your image was an animal, which combined with some wikipedia searching would confirm the info from the URL). \\n\\nMechanical Turk is probably being used as well, perhaps at random to give the appearance of \\\"magic\\\" results on some searches.<\\/li>\\n  <li>Most/bulk of calculations for <span class='match'>deep<\\/span> learning neural nets could be represented as matrix multiplications. And GPUs are good at multiplying matrices. \\n\\nNot sure about other ML algorithms. <\\/li>\\n  <li>That first answer is from 2011, before the <span class='match'>deep<\\/span> learning renaissance. I don't know that they necessarily use deep learning for search queries now, but I don't think that question is good evidence at this point that they don't.<\\/li>\\n  <li>Is <span class='match'>deep<\\/span> learning used for search by any company? In my opinion, DL isn't particularly useful for search since there's no hierarchical representations in search data, and in case of good features ensembles work best. This is confirmed by my knowledge of machine learning behind Yandex – the major search engine in Russia. They don't use DL, but they do hell a lot of ensembling with all sorts of weak learners. But, again, it's not Google, so maybe the whole Google Search team got replaced with Google Brain years ago, while Google Brain team nowadays plays Atari games all days long, who knows?<\\/li>\\n  <li>I guess the initial excitement is over, but we will now see a lot more 'Deep Learning for X' in the coming years, where the original contribution will not be to design interesting networks, but use existing ones in unique ways. For example, CVPR this year has ~ 75 papers related to <span class='match'>deep<\\/span> learning, possibly the highest ever! <\\/li>\\n  <li>There might be fewer papers, but that isn't because some new approach has come out and replaced <span class='match'>deep<\\/span> learning. ConvNets are pretty well researched at this point, since those were the focus of a lot of effort the past few years, I think we are going to see something similar happen to recurrent nets for the next few years.<\\/li>\\n  <li>Deep Learning is a codename for \\\"Neural Networks\\\", which was introduced because NNs were hot in 80-90s, but then all hopes crashed, and people (mostly) abandoned them. Other methods (SVM, ensembles) took their place. It's said that papers that contain \\\"Neural\\\" in their titles were more likely to get rejected at conferences at that time. Oh, and, yes, those new neural nets are <span class='match'>deep<\\/span>er than their ancestors.\\n\\nThe deep learning started out in mid 2000s with unsupervised pretraining. Hinton's lab used RBM to pretrain (unsupervised) a deep neural net and got interesting results. As far as I understand, these guys then went to researching further development of similar methods. Deep Belief Networks, for example.\\n\\nThe next step was to recall LeCun's convolutional networks (they were invented and used in 90s for something like zipcode recognition): in 2012 Alex Krizhevsky blow up Computer Vision community by setting a new record on a major image recognition benchmark.\\n\\nThere are other problems that demonstrated how promising Deep Learning is, but I'm not going to speak of it, I'd rather concentrate of current state of the field.\\n\\nEssentially, over past 10 years of Deep Learning renaissance, the following things were understood / researched:\\n1. Computers got significantly faster (especially if you have a decent GPU)\\n2. We collected a lot of data\\n3. Since NN's objective is nonlinear, the result heavily depends on the initialization point. Nowadays people now several good ways to initialize weight matrices.\\n4. Some activation functions (ReLU, maxout) work better than others.\\n5. Some stochasticity (Dropout) forces your neural networks to learn mode robust representations.\\n6. New methods (AdaDelta, Adam, and lots and lots of others) of optimization speed up learning while keeping computational complexity low.\\n\\nAs far as I know, all modern neural nets (RBMs, Feedforward Nets, Convolutional nets, RNNs + LSTM) were invented even before the year 2000, so the success is mostly due to the facts I outlined above.\\n\\nReadings:\\nThere's an overview of the field from Ilya Sutskever: http://yyue.blogspot.ru/2015/01/a-brief-overview-of-deep-learning.html\\nAnd the Deep Learning book from Bengio et al. http://www.iro.umontreal.ca/~bengioy/dlbook/<\\/li>\\n  <li>I can't wait until someone does an anthropological study of the fad stampedes in the AI/ML fields.  Neural nets to svms to bayes nonparametrics to <span class='match'>deep<\\/span> learning.  They all follow the pattern you've suggested and it'd be greatly interesting to analyze the frequency data of the different stages as a methodological optimization problem: papers stop because maxima have been reached.<\\/li>\\n  <li>It's not an \\\"ELI5\\\", because I wouldn't even be attempting to explain <span class='match'>deep<\\/span> learning (or probably even general machine learning) to a 5 year old.\\n\\nMost applied machine learning involves a heavy human investment in feature engineering, i.e. deriving information about your training cases in a useful format for an off-the-shelf algorithm to look at and learn from.\\n\\nDeep learning aims to learn appropriate features from the raw or nearly-raw data, one way or another. The most popular way today is to train a neural network with multiple layers of nonlinear feature extractors via stochastic gradient descent, essentially \\\"change each parameter in the direction which makes [some differentiable substitute for] the number and magnitude of the mistakes I make go down\\\". These directions are efficiently computable via the backpropagation algorithm.<\\/li>\\n  <li>May be not consciousness. But there is something (whatever it called) that decides if a given action is a good idea or not. For example is it Ok to spit or to dance right now? And critic seems to fulfills this role.\\n\\nAs for Chinese Room, Searle conveniently left out the fact that software model(books with hieroglyphs in his \\\"mental experiment\\\") could learn and could create new things. If room have learned math over the years and at the end produced textbook on <span class='match'>deep<\\/span> learning for example(in mandarin), while person learned nothing, it would leave reader with way different impression.\\n\\nAnd ML models do learn, and there are generative ones. Person in Chinese room is just a computer(literally one who computes, one who fulfills the role of hardware executing instructions). <\\/li>\\n  <li>You can also check out Yoshua Bengios <span class='match'>deep<\\/span> learning book (or rather the draft). It also has a chapter on RNNs. [Link](http://www.iro.umontreal.ca/~bengioy/dlbook/)<\\/li>\\n  <li>So a \\\"<span class='match'>deep<\\/span> boltzmann machine with fine scalability\\\" is essentially a neural network where all the layers are connected to all the other layers. This is like a restricted boltzmann machine expect that there are connections between some of the hidden units, but not all. The fact that the boltzmann machine is not fully connected allows for some Gibbs sampling to be run in parallel.\\n\\nPerhaps the results are not too surprising. The whole idea behind deep learning is to build hierarchical representations of the data. It is hard to train the top-layers because they are so far removed from where the data flows in. However, just because they are hard to train does not mean they are irrelevant. Top-layers may be the key to building more versatile models.<\\/li>\\n  <li>I read the article and I still don't know why <span class='match'>deep<\\/span> learning is a mandate for anything. Sorry.<\\/li>\\n  <li>See [Quora thread](http://www.quora.com/Will-<span class='match'>deep<\\/span>-learning-make-other-Machine-Learning-algorithms-obsolete) on the question \\\"Will deep learning make other Machine Learning algorithms obsolete\\\". There are some interesting and qualified answers on both sides.<\\/li>\\n  <li>In my opinion: for \\\"classic\\\", tabular data (as opposed to <span class='match'>deep<\\/span> learning stuff like images and autio), tree ensembles are at least as good as neural networks and much, much easier to use.<\\/li>\\n  <li>The reason neural networks are so effective is quite simple - the error is well attributed.\\n\\nBackpropagation was the 2nd important breakthrough , now error could be effectively be attributed to neurons <span class='match'>deep<\\/span> within the network.\\n\\nBackprop is just calculus, the gradient of downhill in error space.\\n\\nTrial and error, slight improvement each time gets closer to the desired answer - no mystery there.\\n\\nBackprop can get stuck in local minima and be unstable in very deep or recurrent nets - evolutionary genetic methods seem to correct this with a more global search of the parameter space.\\n\\nhttp://colah.github.io/posts/2014-03-NN-Manifolds-Topology/\\n\\nThe idea that knowledge is a highly structured lower dimensional manifold in the unstructured high dimensional space of say pixels or wikipedia corpus seems legit.\\n\\nNeural networks alone are not effective - all the recent breakthroughs occur because of multiple levels of structure, deep nets.\\n\\nMinsky's 1968 criticism that a single layer perceptron ( neural net) cannot model the XOR function.\\n\\nThen Geoff Hinton developed the 'hidden' layer - the 1st importatnt breakthrough.\\n\\nThis allows function compostition and XOR is easily modelled. More layers add more non-linearity and ever more complex functions can potentially be modelled.\\n\\nExactly why heirarchical representations are so effective at encoding complex structure compactly is not so clear but there are many examples in other branches of information theory. Huffman Trees for instance.\\n\\nLooking at the internal representations the deep neural nets produce for a face detector - the 1st layer is all edge detector and HOG like filters , the second layer composes these into curves and small parts, in the next layer these are composed into eye and nose detectors and then a layer that detects faces.\\n\\nIMHO the analysis of why evolution so effectively produces solutions to enviroments is an analagous problem.\\n\\nThis video ask good questions\\n[The unreasnoble effictiveness of deep learning](https://www.youtube.com/watch?v=sc-KbuZqGkI)<\\/li>\\n  <li>Sounds interesting!\\n\\nI think Theano would have been totally fine if we were building a more conventional  network. However, building exotic architectures like this without breaking Theano's symbolic differentiation graph is very bad. There are a couple of parts to this.\\n\\n1. **Anything you would normally implement as a loop or map you use [scan](http://www.<span class='match'>deep<\\/span>learning.net/software/theano/tutorial/loop.html).** Most networks would not involve inner scans, but in implementing the capsule model it proves to be unavoidable. (I figured out that it is *possible* to implement with only matrix operations, but you end up building matrices as large as the unfolded loop. The cure was worse than the disease.) Scan is slightly frustrating to use, but more importantly is brutally slow in both compilation/optimization and execution. Because loops generate symbolic graphs as deep as the entire unfolded loop, differentiation is extremely expensive. Scan is also implemented in python, which results in interop overhead (especially if you're using a GPU). Profile evidence:\\n\\n    Class\\n    ---\\n    &lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; &lt;#call&gt; &lt;#apply&gt; &lt;Class name&gt;\\n      100.0%   100.0%     162.809s       4.07e+01s     Py       4        4   &lt;class 'theano.scan_module.scan_op.Scan'&gt;\\n\\n  (This line includes the computation done inside the scan — the scan op itself didn't consume three minutes of this run. But this code without the scan is a few orders of magnitude faster.)\\n\\n  This brings us to the second point:\\n\\n2. **What's actually happening under the hood (i.e., what you need to know to optimize your code) is hugely indirected from the code you write.** Programming in Theano is like performing surgery while wearing oven mitts. Which are greased.\\n\\n  [Conditionals](http://deeplearning.net/software/theano/tutorial/conditions.html) are probably the most comical example of this we ran into. There are two conditionals in the Theano symbolic system, `switch` and `ifelse`. `ifelse` does what you *want* a conditional to do; namely, it only evaluates the branch it actually takes. `switch`, on the other hand, *evaluates both branches*.\\n\\n  To put this in context, take the following code:\\n\\n    if false:\\n        fire_the_missiles()\\n    else:\\n        print(\\\"All clear! No missiles today!\\\")\\n\\n  `ifelse` would correctly print `All clear! No missiles today!` and avoid firing the missiles.\\n  `switch` would do both.\\n\\n  `switch` is only conditional in that it still returns the result of the correct branch.\\n\\n  From the docs:\\n  &gt; In this example, the IfElse op spends less time (about half as much) than Switch since it computes only one variable out of the two.\\n\\n  So at first glance, this seems like a home run. Just use `ifelse`. But wait! These two profiles are from code which is otherwise identical (they were run for different numbers of cycles — look at the time per call):\\n\\n    &lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; &lt;#call&gt; &lt;#apply&gt; &lt;Op name&gt;\\n       68.6%    68.6%      18.000s       9.40e-06s     Py    1914937       54   if{inplace}\\n       1.2%    84.7%       0.001s       9.77e-07s     C     1040       52   Elemwise{Composite{[Switch(GE(i0, i1), i2, Switch(i3, i2, i0))]}}\\n\\n  `ifelse`, despite only running one branch, takes an order of magnitude longer. That's because (and this is *not* in the docs) `ifelse` is written in Python, while `switch` is written in C. I ended up rewriting this code to use `switch` without firing any missiles.\\n\\n3. **Compiling your Theano every time you make a change eats days.** Anytime you rerun, Theano recomputes the symbolic graph from scratch and then compiles it to C/GPU/whatever. In our case this took upwards of twenty minutes. You can avoid (some of) this by running it as Python code and turning off optimization, but then actually running the code will take 1-2 orders of magnitude longer. For simpler graphs, this might not be a problem.\\n\\n\\nSorry about the rant — this was just a very frustrating experience, and I'd like to save anyone else from repeating it.\\n\\n\\nBy contrast, writing this in Torch was a great experience. It did require calculating gradients by (well, by Matlab), but they only took a few days to get right anyway. At every step we knew what we were doing, what things were working and what weren't, and what the machine was doing.\\n\\nLua itself runs pretty damn fast ([language speed comparison](http://julialang.org/benchmarks/)), so you can afford to just write straight Lua for most things. Since there's no symbolic graph to tiptoe around, you can write whatever raw code you want to without breaking anything or causing exponential slowdowns. And if you want to, it's super easy (actually, completely trivial — it just works) to interop with a CUDA kernel. \\n\\nPlus, Torch is improving very fast. DeepMind, Facebook, Google, Toronto, Montreal, NYU, us at MIT... [all using Torch](http://torch.ch/whoweare.html), and all contributing [libraries](https://research.facebook.com/blog/879898285375829/fair-open-sources-deep-learning-modules-for-torch/), [tools](https://github.com/facebook/iTorch), and [projects](https://sites.google.com/a/deepmind.com/dqn/). It's not just the future of the deep learning community, it's the *present* of the deep learning community.\\n\\nSwitching to Torch was a huge relief. Save yourself the heartache and go straight there.\\n\\nNinja edit: caveat emptor, but take our code if it helps: http://www.reddit.com/r/MachineLearning/comments/35tqvg/understanding_optimizing_neural_networks_that/cr8zv9g<\\/li>\\n  <li>This was less than a year ago, and the data domain was essentially any that a large web company would collect - click through rates, purchase histories, user profiles, historical behavior of these features, etc.\\n\\n&gt;  the value these nets create eclipses the insane money the Very Large Companies are paying for top neural net engineers\\n\\nApart from a few specialized domains, <span class='match'>deep<\\/span> networks rarely provide huge gains, especially on tabular data as someone below mentioned. Thus from cost/benefit standpoint I think a lot of companies choose to use a simple model that gets you 90% there and stop before hiring a team of deep network engineers that will get you 92% there.  My team had one part-time neural network engineer compared to roughly 10-20 other statisticians and general ML researchers.\\n\\nI actually mainly focus on deep learning in my research, but it's nowhere near the point where you can throw a dataset at an off-the-shelf solver and have it produce something meaningful.<\\/li>\\n  <li>Have you seen lectures from Nando on <span class='match'>deep<\\/span> learning ? He is talking about the model in one of them. here is the playlist http://www.youtube.com/watch?v=P78QYjWh5sM&amp;index=3&amp;list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu<\\/li>\\n  <li>The approach is actually pretty similar to working with categorical data. Essentially, you create sparse vectors of numbers from the vocabulary (all unique words that are contained in the combined documents) for each document where the values in the vector can be simply the word count frequency, or inverse-document frequency and so on. That's basically the classic approach, the so-called bag-of-words model. I have written [an article](http://sebastianraschka.com/Articles/2014_naive_bayes_1.html) on Naive Bayes classifiers and bag-of-words if you are interested.\\n\\nThen, there is also the \\\"<span class='match'>deep<\\/span> learning\\\" approach called Word2Vec (by Google). You can find an overview and useful links [here](https://code.google.com/p/word2vec/).\\n\\n<\\/li>\\n  <li>&gt; 10x speedup\\n\\nCareful.\\n\\n&gt; It will have 3D memory, resulting in up to 5X improvement in <span class='match'>deep<\\/span> learning applications.  And it will feature NVLink – NVIDIA’s high-speed interconnect, which links together two or more GPUs — that will lead to a total 10X improvement in deep learning.\\n\\nSo 2X of the 10X there comes from using two GPUs, and the remaining '5X improvement' is not the same as 'up to 5X improvement'.<\\/li>\\n  <li>QUOTE: *\\\"Mixed-precision computing enables Pascal architecture-based GPUs to compute at 16-bit floating point accuracy at* **twice the rate** *of 32-bit floating point accuracy.*\\n\\n*Increased floating point performance particularly benefits classification and convolution – two key activities in <span class='match'>deep<\\/span> learning – while achieving needed accuracy\\\"*\\n\\n<\\/li>\\n  <li>What a sensationalist headline. Does 16-bit FP work for *all of <span class='match'>deep<\\/span> learning* or just something nvidia bothered to test on ? <\\/li>\\n  <li>It's interesting that unsupervsed pretraining can hurt the performance. In the earlier days of <span class='match'>deep<\\/span> learning Geoff Hinton had a kind of information-theoretical argument for UL (if I remember correctly): there is a lot more information in the images than in the labels. This should be true even when all of the images are labelled...\\n\\nBy the way, I wish the authors defined what the unpooling operator `F` does exactly (I can guess, but I'd rather not)<\\/li>\\n  <li>tl;dw : \\\"The latest result for Pacman is now superhuman.\\\"\\n\\n[**Sildes** for David Silver's ICLR 2015 Talk \\\\[pdf\\\\]](http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf) --- slide [22](http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf#22) discusses Google Deepmind's latest GORILA, a massively parallel reinforcement learner.\\n\\n**\\\"** In this talk I will discuss how reinforcement learning (RL) can be combined with <span class='match'>deep<\\/span> learning (DL). There are several ways to combine DL and RL together, including value-based, policy-based, and model-based approaches with planning. Several of these approaches have well-known divergence issues, and I will present simple methods for addressing these instabilities. These methods have achieved notable success in the Atari 2600 domain. I will present recent a selection of recent results that improve on the published state-of-the-art in Atari and other challenging domains. Finally, I will discuss how RL can be used to improve DL, even when the native problem is supervised or unsupervised learning.**\\\"** - **David Silver**, ICLR 2015\\n\\n[**Video of Part 2**](https://www.youtube.com/watch?v=zXa6UFLQCtg)\\n\\n--\\n\\n[ICLR 2015, for papers &amp; slides, scroll far down](http://www.iclr.cc/doku.php?id=iclr2015:main)\\n\\n--\\n\\n[David Silver's Reinforcement Learning Course UC London](https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;list=PL5X3mDkKaJrL42i_jhE4N-p6E2Ol62Ofa)<\\/li>\\n  <li>In addition to SuperFX's [comment](https://www.reddit.com/r/MachineLearning/comments/36jj3h/neural_network_summer_curriculum/creq46l),\\n\\nRead/watch these papers/slides/videos:\\nhttp://labrosa.ee.columbia.edu/cuneuralnet/\\nhttp://nlp.stanford.edu/courses/NAACL2013/\\nhttps://youtu.be/DleXA5ADG78\\n\\nDo this tutorial:\\nhttp://<span class='match'>deep<\\/span>learning.net/tutorial/contents.html\\n\\nRead this online book:\\nhttp://neuralnetworksanddeeplearning.com/\\n\\nRead this arxiv deep learning survey paper by Schmidhuber:\\nhttp://arxiv.org/abs/1404.7828\\n\\nRead any and all trending arxiv papers about neural networks. If you want I can send you a giant list of interesting ones I've bookmarked over the last two years.\\n\\nRead this book (although it's about spiking neural models, which are more complex than artificial/deep neural networks):\\nhttp://www.amazon.com/How-Build-Brain-Architecture-Architectures/dp/0199794545<\\/li>\\n  <li>Deep Learning is a subfield of Machine Learning.  There are a lot of things that fall under the umbrella of ML that aren't relevant to <span class='match'>deep<\\/span> learning.<\\/li>\\n  <li>Deep learning is a part of machine learning rather than a distinct field. It will be much easier to start at the bottom with something like logistic regression or random trees rather than jumped straight into <span class='match'>deep<\\/span> learning.<\\/li>\\n  <li>Since you have python experience, I would recommend getting familiar with Theano. Theano underpins a lot of <span class='match'>deep<\\/span> learning packages, so if you are comfortable with it you have a much better understanding of what those libraries are doing. Theano is really a math compiler that is used for deep learning, so coding in theano tends to be expressing the math as the specific theano functions. I found this to be really helpful to understanding the math, since you can write the code and experiment with changing things pretty easily. \\n\\nThey have a good set of [tutorials](http://deeplearning.net/tutorial/contents.html) with example code to learn from. <\\/li>\\n  <li>Thats true, I did scikit-learn before theano, but the OP sounds interested in the <span class='match'>deep<\\/span> learning aspects so might as well start there. Scikit-learn didn't really give any benefit to learning theano, but Numpy absolutely would. <\\/li>\\n  <li>I don't want to interrupt OP's request, but I wanted to ask a question too. I also have a huge interest within AI, <span class='match'>deep<\\/span> learning and NLP, but have the same issues, the math. Does anyone know what kind of subjects/math would be good to follow up? I know it's a lot of statistical subjects. I've had a course that's a step under calculus which includes basic linear algebra and differential equations, should I consider to take calculus also? Udacity has some good courses (free?) for Stasticial subjects btw, I've started a bit on the intro to machine learning course:) <\\/li>\\n  <li>It is up to OP to chose but I will explain my point a little better:\\n\\nTheano introduces a new paradigm and OP will have to learn it at the same time that numpy, ML \\\"good practices\\\" and the math. Scikit-learn is way easier and he can treat it like a black-box\\n\\nAlso, is OP comfortable with Linux and has a good GPU? If the answer to one of these is no the feedback time to get any results can be a pain for OP's own learning process when doing <span class='match'>deep<\\/span> learning. I wouldn't recommend at all trying to do Theano with GPU in Windows, my experience ended up with a BSoD when trying to install CUDA<\\/li>\\n  <li>Sounds like Andrej Karpathy of Stanford U. is also moving over to Torch:\\n\\n\\\"The code is written in Torch 7, which has recently become my favorite <span class='match'>deep<\\/span> learning framework. I've only started working with Torch/LUA over the last few months and it hasn't been easy (I spent a good amount of time digging through the raw Torch code on Github and asking questions on their gitter to get things done), but once you get a hang of things it offers a lot of flexibility and speed. I've also worked with Caffe and Theano in the past and I believe Torch, while not perfect, gets its levels of abstraction and philosophy right better than others.\\\" <\\/li>\\n  <li>Geoff Hinton has a good introduction to Python on his webpage: \\n\\nhttp://www.cs.toronto.edu/~hinton/\\n\\nHe gives a lot of examples of <span class='match'>deep<\\/span> learning in Python.  <\\/li>\\n  <li>I don't mean to be sassy.  The former post saying Bishop is weak on <span class='match'>deep<\\/span> learning. I thought it was ironic because Toronto is famous for its Deep Learning (e.g. Hinton) and we use Bishop exclusively.<\\/li>\\n  <li>Caffe is a pretty good <span class='match'>deep<\\/span> learning framework, mshadow/cxxnet as well. However, they are nowhere near as good for prototyping.<\\/li>\\n  <li>Here's a [list of resources about <span class='match'>deep<\\/span> learning and neural networks](http://datacoder.aidanf.net/issues/4#start) that I put together last week .<\\/li>\\n  <li>\\\"Reproduced with permission from [C. Olah](http://colah.github.io/).\\\"\\n\\nC Olah has some serious clear analysis of neural nets , convolutions and <span class='match'>deep<\\/span> learning - his few posts are absolute gems of clarity.\\n<\\/li>\\n  <li>I am serious, its mostly because of my interest in <span class='match'>deep<\\/span> learning that I want to try it, thus I posted here. <\\/li>\\n  <li>I really like kernel methods, although I don't use them as much as maybe I should.  \\n\\n* Support vector machines haven't gone away, and they're still just as good as they were before <span class='match'>deep<\\/span> learning took off.\\n\\n* I've also been really impressed with Gaussian process priors and gaussian process regression, and they've gotten more scalable and robust.<\\/li>\\n  <li>There is still some random forest/ensemble of decision tree work going on that is quite relevant to medical genomics.\\n\\nA lot of it focuses on improved feature selection and importance scores which is very important in wide data sets with lots of irrelevant features (ie genetics). Off the top of my head check out \\\"Artificial Contrasts With Ensembles\\\" ( [my implementation](https://github.com/ryanbressler/CloudForest) ) which has been around for a bit i've found to work quit well with genetic data and maybe \\\"Module Guided Random Forests\\\" which I haven't had time to implement/try but may be adaptable to use say pathway information.\\n\\nHellinger distance decision trees are also really cool and have seen some recent interest.\\n\\nOther areas of active research include scaling all sorts of methods, manifold learning, compressed sensing and matrix factorization methods. [Check out nuit blache if you haven't.](http://nuit-blanche.blogspot.dk/) It highlights ton's of cool research only some of which is (currently know to be) related to <span class='match'>deep<\\/span> learning. And the author posts here sometimes. <\\/li>\\n  <li>A rough sample of the [2014 NIPS proceedings](http://papers.nips.cc/book/advances-in-neural-information-processing-systems-27-2014) suggests more than 75% of active ML research has very little to do with <span class='match'>deep<\\/span> learning or NNs. \\nI find DNNs intriguing but nearly all of my work involves traditional feature selection in supervised and semi-supervised scenarios.<\\/li>\\n  <li>I am doing research in reinforcement learning methods, especially policy search methods with finite horizon. My field of application is robotics, mainly because I enjoy it to see how my algorithms move things in the real world - simply because I do not get excited about results like \\\"It translated 75.342% of the words correctly from Finnish to English\\\". It is actually interesting to see how algorithms manipulate the real world directly.\\nFurthermore, I am interested in stochastic optimization methods and dimensionality reduction methods and how to translate these ideas into RL.\\n\\nWell, there is currently a huge hype about NN and Deep Learning (again) - I don't think I really want to participate in this in the near future. But of course, I try to be up to date about the current results with Deep learning in robotics, but sometimes I am a bit upset about sentences like \\\"If <span class='match'>deep<\\/span> learning does not work, just use more data\\\" and so on... maybe some people got a bit too excited. But other results are quiet interesting, on the other hand this field seems to be quite saturated with researchers (because it so cool,new and hip ;) ).<\\/li>\\n  <li>i use mrfs/crfs for biometrics, <span class='match'>deep<\\/span> learning is starting to make it's way into biometrics but one problem is that many biometric datasets just aren't that big. It's not like we can skim flikr or google images for images of chairs. Ground truth for faces/fingerprints/iris/etc usually involves a long process (mostly done by Kevin Boywer's team) of getting a couple hundred people in different environments.  Facebook/Google have started to turn that process upside down where people are submitting their labeled photos to them, and even then it's really just limited to faces.  Unfortunately not having much data rules out really leveraging many of those 'big data' methods. <\\/li>\\n  <li>To be honest, I would recommend starting simpler than neural nets. They're fun to play with (I wouldn't be doing graduate work in this area if I didn't think so) and incredibly powerful when deployed against tasks that actually require their flexibility, but any <span class='match'>deep<\\/span> learning (or machine learning in general) expert will tell you that to solve practical problems, you should start simple and not try to do too much at once. Were I trying to solve a practical problem quickly, my first recourse would never be to anything neural net related.\\n\\nIf you want to have a cool demo running relatively quickly, I settle on the problem you want to solve, and build the web frontend for it, isolating the machine learning code in one or a few methods/functions, such that you can easily swap it out. Start with some off-the-shelf thing in e.g. scikit-learn, a linear SVM or a random forest/boosted tree ensemble. These things are fairly good at coping whatever random thing you throw at them provided the task isn't too complex and the data isn't too large. Not only will this give you more immediate positive reinforcement and keep you interested, it will give you a baseline against which to compare other methods you try. If you can't beat a linear classifier with a neural network then your task may not be complex enough to benefit from the added model capacity; if you do significantly *worse* with a neural net then you either have bugs or are simply overfitting, and need to learn about regularization techniques.<\\/li>\\n  <li>His Stanford lecture is excellent, and much more \\\"meaty\\\" than the Coursera class. It's a full-on lecture, that delves much <span class='match'>deep<\\/span>er into the theory than the coursera ml-class. \\n\\nAs for your questions: it's an excellent starting point in ML, and covers a lot of basics. I think it can help you in understanding Bishop, although it doesn't cover all the chapters of Bishop, and is probably still a bit less theoretical than the book. So reading Bishop will give you a deeper understanding, but the Ng lectures are a good starting point. Also note that there are class notes on the web for this class, and they are very good!\\n\\nI have no idea about the homework.\\n\\n\\nAlso: remember that the class was recorded in 2009 (IIRC), so it doesn't cover brand-new ML research (e.g. no deep learning). However I actually think is a big plus, since you can learn foundations on Neural Nets anywhere (Hinton's coursera class is still a good starting point), but this class covers topics that I'd consider elementary for a solid base in ML.<\\/li>\\n  <li>I took the course at Stanford in 2013 and it wasn't that different from the 2010 version on SEE (which may be the ones available on youtube). The fundamentals don't change that quickly. You can quickly start working on cutting edge stuff after you have the basics (e.g. my course project used <span class='match'>deep<\\/span> learning, which wasn't covered at all in the lectures).\\n\\nOverall, I thought the course was well worth doing. For me Prof. Ng made things sound almost too easy in the lectures. I only really understood the material after laboring over the assignments so definitely don't think just viewing the videos is sufficient. That's my two cents anyways.<\\/li>\\n<\\/ul>\"},\"evals\":[],\"jsHooks\":[]}</script>\n",
       "\t</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "HTML widgets cannot be represented in plain text (need html)"
      ]
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "deep_learning_posts <- data$body[str_which(data$body, \"deep learning\")]\n",
    "\n",
    "# View strings in HTML format with the first occurence of a pattern highlighted\n",
    "str_view(deep_learning_posts, pattern=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!doctype html>\n",
       "<html>\n",
       "\t<head>\n",
       "\t\t<meta charset=\"utf-8\">\n",
       "\t\t<script title=\"htmlwidgets\" src=\"data:application/javascript;base64,(function() {
  // If window.HTMLWidgets is already defined, then use it; otherwise create a
  // new object. This allows preceding code to set options that affect the
  // initialization process (though none currently exist).
  window.HTMLWidgets = window.HTMLWidgets || {};

  // See if we're running in a viewer pane. If not, we're in a web browser.
  var viewerMode = window.HTMLWidgets.viewerMode =
      /\bviewer_pane=1\b/.test(window.location);

  // See if we're running in Shiny mode. If not, it's a static document.
  // Note that static widgets can appear in both Shiny and static modes, but
  // obviously, Shiny widgets can only appear in Shiny apps/documents.
  var shinyMode = window.HTMLWidgets.shinyMode =
      typeof(window.Shiny) !== "undefined" && !!window.Shiny.outputBindings;

  // We can't count on jQuery being available, so we implement our own
  // version if necessary.
  function querySelectorAll(scope, selector) {
    if (typeof(jQuery) !== "undefined" && scope instanceof jQuery) {
      return scope.find(selector);
    }
    if (scope.querySelectorAll) {
      return scope.querySelectorAll(selector);
    }
  }

  function asArray(value) {
    if (value === null)
      return [];
    if ($.isArray(value))
      return value;
    return [value];
  }

  // Implement jQuery's extend
  function extend(target /*, ... */) {
    if (arguments.length == 1) {
      return target;
    }
    for (var i = 1; i < arguments.length; i++) {
      var source = arguments[i];
      for (var prop in source) {
        if (source.hasOwnProperty(prop)) {
          target[prop] = source[prop];
        }
      }
    }
    return target;
  }

  // IE8 doesn't support Array.forEach.
  function forEach(values, callback, thisArg) {
    if (values.forEach) {
      values.forEach(callback, thisArg);
    } else {
      for (var i = 0; i < values.length; i++) {
        callback.call(thisArg, values[i], i, values);
      }
    }
  }

  // Replaces the specified method with the return value of funcSource.
  //
  // Note that funcSource should not BE the new method, it should be a function
  // that RETURNS the new method. funcSource receives a single argument that is
  // the overridden method, it can be called from the new method. The overridden
  // method can be called like a regular function, it has the target permanently
  // bound to it so "this" will work correctly.
  function overrideMethod(target, methodName, funcSource) {
    var superFunc = target[methodName] || function() {};
    var superFuncBound = function() {
      return superFunc.apply(target, arguments);
    };
    target[methodName] = funcSource(superFuncBound);
  }

  // Add a method to delegator that, when invoked, calls
  // delegatee.methodName. If there is no such method on
  // the delegatee, but there was one on delegator before
  // delegateMethod was called, then the original version
  // is invoked instead.
  // For example:
  //
  // var a = {
  //   method1: function() { console.log('a1'); }
  //   method2: function() { console.log('a2'); }
  // };
  // var b = {
  //   method1: function() { console.log('b1'); }
  // };
  // delegateMethod(a, b, "method1");
  // delegateMethod(a, b, "method2");
  // a.method1();
  // a.method2();
  //
  // The output would be "b1", "a2".
  function delegateMethod(delegator, delegatee, methodName) {
    var inherited = delegator[methodName];
    delegator[methodName] = function() {
      var target = delegatee;
      var method = delegatee[methodName];

      // The method doesn't exist on the delegatee. Instead,
      // call the method on the delegator, if it exists.
      if (!method) {
        target = delegator;
        method = inherited;
      }

      if (method) {
        return method.apply(target, arguments);
      }
    };
  }

  // Implement a vague facsimilie of jQuery's data method
  function elementData(el, name, value) {
    if (arguments.length == 2) {
      return el["htmlwidget_data_" + name];
    } else if (arguments.length == 3) {
      el["htmlwidget_data_" + name] = value;
      return el;
    } else {
      throw new Error("Wrong number of arguments for elementData: " +
        arguments.length);
    }
  }

  // http://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
  function escapeRegExp(str) {
    return str.replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
  }

  function hasClass(el, className) {
    var re = new RegExp("\\b" + escapeRegExp(className) + "\\b");
    return re.test(el.className);
  }

  // elements - array (or array-like object) of HTML elements
  // className - class name to test for
  // include - if true, only return elements with given className;
  //   if false, only return elements *without* given className
  function filterByClass(elements, className, include) {
    var results = [];
    for (var i = 0; i < elements.length; i++) {
      if (hasClass(elements[i], className) == include)
        results.push(elements[i]);
    }
    return results;
  }

  function on(obj, eventName, func) {
    if (obj.addEventListener) {
      obj.addEventListener(eventName, func, false);
    } else if (obj.attachEvent) {
      obj.attachEvent(eventName, func);
    }
  }

  function off(obj, eventName, func) {
    if (obj.removeEventListener)
      obj.removeEventListener(eventName, func, false);
    else if (obj.detachEvent) {
      obj.detachEvent(eventName, func);
    }
  }

  // Translate array of values to top/right/bottom/left, as usual with
  // the "padding" CSS property
  // https://developer.mozilla.org/en-US/docs/Web/CSS/padding
  function unpackPadding(value) {
    if (typeof(value) === "number")
      value = [value];
    if (value.length === 1) {
      return {top: value[0], right: value[0], bottom: value[0], left: value[0]};
    }
    if (value.length === 2) {
      return {top: value[0], right: value[1], bottom: value[0], left: value[1]};
    }
    if (value.length === 3) {
      return {top: value[0], right: value[1], bottom: value[2], left: value[1]};
    }
    if (value.length === 4) {
      return {top: value[0], right: value[1], bottom: value[2], left: value[3]};
    }
  }

  // Convert an unpacked padding object to a CSS value
  function paddingToCss(paddingObj) {
    return paddingObj.top + "px " + paddingObj.right + "px " + paddingObj.bottom + "px " + paddingObj.left + "px";
  }

  // Makes a number suitable for CSS
  function px(x) {
    if (typeof(x) === "number")
      return x + "px";
    else
      return x;
  }

  // Retrieves runtime widget sizing information for an element.
  // The return value is either null, or an object with fill, padding,
  // defaultWidth, defaultHeight fields.
  function sizingPolicy(el) {
    var sizingEl = document.querySelector("script[data-for='" + el.id + "'][type='application/htmlwidget-sizing']");
    if (!sizingEl)
      return null;
    var sp = JSON.parse(sizingEl.textContent || sizingEl.text || "{}");
    if (viewerMode) {
      return sp.viewer;
    } else {
      return sp.browser;
    }
  }

  // @param tasks Array of strings (or falsy value, in which case no-op).
  //   Each element must be a valid JavaScript expression that yields a
  //   function. Or, can be an array of objects with "code" and "data"
  //   properties; in this case, the "code" property should be a string
  //   of JS that's an expr that yields a function, and "data" should be
  //   an object that will be added as an additional argument when that
  //   function is called.
  // @param target The object that will be "this" for each function
  //   execution.
  // @param args Array of arguments to be passed to the functions. (The
  //   same arguments will be passed to all functions.)
  function evalAndRun(tasks, target, args) {
    if (tasks) {
      forEach(tasks, function(task) {
        var theseArgs = args;
        if (typeof(task) === "object") {
          theseArgs = theseArgs.concat([task.data]);
          task = task.code;
        }
        var taskFunc = tryEval(task);
        if (typeof(taskFunc) !== "function") {
          throw new Error("Task must be a function! Source:\n" + task);
        }
        taskFunc.apply(target, theseArgs);
      });
    }
  }

  // Attempt eval() both with and without enclosing in parentheses.
  // Note that enclosing coerces a function declaration into
  // an expression that eval() can parse
  // (otherwise, a SyntaxError is thrown)
  function tryEval(code) {
    var result = null;
    try {
      result = eval(code);
    } catch(error) {
      if (!error instanceof SyntaxError) {
        throw error;
      }
      try {
        result = eval("(" + code + ")");
      } catch(e) {
        if (e instanceof SyntaxError) {
          throw error;
        } else {
          throw e;
        }
      }
    }
    return result;
  }

  function initSizing(el) {
    var sizing = sizingPolicy(el);
    if (!sizing)
      return;

    var cel = document.getElementById("htmlwidget_container");
    if (!cel)
      return;

    if (typeof(sizing.padding) !== "undefined") {
      document.body.style.margin = "0";
      document.body.style.padding = paddingToCss(unpackPadding(sizing.padding));
    }

    if (sizing.fill) {
      document.body.style.overflow = "hidden";
      document.body.style.width = "100%";
      document.body.style.height = "100%";
      document.documentElement.style.width = "100%";
      document.documentElement.style.height = "100%";
      if (cel) {
        cel.style.position = "absolute";
        var pad = unpackPadding(sizing.padding);
        cel.style.top = pad.top + "px";
        cel.style.right = pad.right + "px";
        cel.style.bottom = pad.bottom + "px";
        cel.style.left = pad.left + "px";
        el.style.width = "100%";
        el.style.height = "100%";
      }

      return {
        getWidth: function() { return cel.offsetWidth; },
        getHeight: function() { return cel.offsetHeight; }
      };

    } else {
      el.style.width = px(sizing.width);
      el.style.height = px(sizing.height);

      return {
        getWidth: function() { return el.offsetWidth; },
        getHeight: function() { return el.offsetHeight; }
      };
    }
  }

  // Default implementations for methods
  var defaults = {
    find: function(scope) {
      return querySelectorAll(scope, "." + this.name);
    },
    renderError: function(el, err) {
      var $el = $(el);

      this.clearError(el);

      // Add all these error classes, as Shiny does
      var errClass = "shiny-output-error";
      if (err.type !== null) {
        // use the classes of the error condition as CSS class names
        errClass = errClass + " " + $.map(asArray(err.type), function(type) {
          return errClass + "-" + type;
        }).join(" ");
      }
      errClass = errClass + " htmlwidgets-error";

      // Is el inline or block? If inline or inline-block, just display:none it
      // and add an inline error.
      var display = $el.css("display");
      $el.data("restore-display-mode", display);

      if (display === "inline" || display === "inline-block") {
        $el.hide();
        if (err.message !== "") {
          var errorSpan = $("<span>").addClass(errClass);
          errorSpan.text(err.message);
          $el.after(errorSpan);
        }
      } else if (display === "block") {
        // If block, add an error just after the el, set visibility:none on the
        // el, and position the error to be on top of the el.
        // Mark it with a unique ID and CSS class so we can remove it later.
        $el.css("visibility", "hidden");
        if (err.message !== "") {
          var errorDiv = $("<div>").addClass(errClass).css("position", "absolute")
            .css("top", el.offsetTop)
            .css("left", el.offsetLeft)
            // setting width can push out the page size, forcing otherwise
            // unnecessary scrollbars to appear and making it impossible for
            // the element to shrink; so use max-width instead
            .css("maxWidth", el.offsetWidth)
            .css("height", el.offsetHeight);
          errorDiv.text(err.message);
          $el.after(errorDiv);

          // Really dumb way to keep the size/position of the error in sync with
          // the parent element as the window is resized or whatever.
          var intId = setInterval(function() {
            if (!errorDiv[0].parentElement) {
              clearInterval(intId);
              return;
            }
            errorDiv
              .css("top", el.offsetTop)
              .css("left", el.offsetLeft)
              .css("maxWidth", el.offsetWidth)
              .css("height", el.offsetHeight);
          }, 500);
        }
      }
    },
    clearError: function(el) {
      var $el = $(el);
      var display = $el.data("restore-display-mode");
      $el.data("restore-display-mode", null);

      if (display === "inline" || display === "inline-block") {
        if (display)
          $el.css("display", display);
        $(el.nextSibling).filter(".htmlwidgets-error").remove();
      } else if (display === "block"){
        $el.css("visibility", "inherit");
        $(el.nextSibling).filter(".htmlwidgets-error").remove();
      }
    },
    sizing: {}
  };

  // Called by widget bindings to register a new type of widget. The definition
  // object can contain the following properties:
  // - name (required) - A string indicating the binding name, which will be
  //   used by default as the CSS classname to look for.
  // - initialize (optional) - A function(el) that will be called once per
  //   widget element; if a value is returned, it will be passed as the third
  //   value to renderValue.
  // - renderValue (required) - A function(el, data, initValue) that will be
  //   called with data. Static contexts will cause this to be called once per
  //   element; Shiny apps will cause this to be called multiple times per
  //   element, as the data changes.
  window.HTMLWidgets.widget = function(definition) {
    if (!definition.name) {
      throw new Error("Widget must have a name");
    }
    if (!definition.type) {
      throw new Error("Widget must have a type");
    }
    // Currently we only support output widgets
    if (definition.type !== "output") {
      throw new Error("Unrecognized widget type '" + definition.type + "'");
    }
    // TODO: Verify that .name is a valid CSS classname

    // Support new-style instance-bound definitions. Old-style class-bound
    // definitions have one widget "object" per widget per type/class of
    // widget; the renderValue and resize methods on such widget objects
    // take el and instance arguments, because the widget object can't
    // store them. New-style instance-bound definitions have one widget
    // object per widget instance; the definition that's passed in doesn't
    // provide renderValue or resize methods at all, just the single method
    //   factory(el, width, height)
    // which returns an object that has renderValue(x) and resize(w, h).
    // This enables a far more natural programming style for the widget
    // author, who can store per-instance state using either OO-style
    // instance fields or functional-style closure variables (I guess this
    // is in contrast to what can only be called C-style pseudo-OO which is
    // what we required before).
    if (definition.factory) {
      definition = createLegacyDefinitionAdapter(definition);
    }

    if (!definition.renderValue) {
      throw new Error("Widget must have a renderValue function");
    }

    // For static rendering (non-Shiny), use a simple widget registration
    // scheme. We also use this scheme for Shiny apps/documents that also
    // contain static widgets.
    window.HTMLWidgets.widgets = window.HTMLWidgets.widgets || [];
    // Merge defaults into the definition; don't mutate the original definition.
    var staticBinding = extend({}, defaults, definition);
    overrideMethod(staticBinding, "find", function(superfunc) {
      return function(scope) {
        var results = superfunc(scope);
        // Filter out Shiny outputs, we only want the static kind
        return filterByClass(results, "html-widget-output", false);
      };
    });
    window.HTMLWidgets.widgets.push(staticBinding);

    if (shinyMode) {
      // Shiny is running. Register the definition with an output binding.
      // The definition itself will not be the output binding, instead
      // we will make an output binding object that delegates to the
      // definition. This is because we foolishly used the same method
      // name (renderValue) for htmlwidgets definition and Shiny bindings
      // but they actually have quite different semantics (the Shiny
      // bindings receive data that includes lots of metadata that it
      // strips off before calling htmlwidgets renderValue). We can't
      // just ignore the difference because in some widgets it's helpful
      // to call this.renderValue() from inside of resize(), and if
      // we're not delegating, then that call will go to the Shiny
      // version instead of the htmlwidgets version.

      // Merge defaults with definition, without mutating either.
      var bindingDef = extend({}, defaults, definition);

      // This object will be our actual Shiny binding.
      var shinyBinding = new Shiny.OutputBinding();

      // With a few exceptions, we'll want to simply use the bindingDef's
      // version of methods if they are available, otherwise fall back to
      // Shiny's defaults. NOTE: If Shiny's output bindings gain additional
      // methods in the future, and we want them to be overrideable by
      // HTMLWidget binding definitions, then we'll need to add them to this
      // list.
      delegateMethod(shinyBinding, bindingDef, "getId");
      delegateMethod(shinyBinding, bindingDef, "onValueChange");
      delegateMethod(shinyBinding, bindingDef, "onValueError");
      delegateMethod(shinyBinding, bindingDef, "renderError");
      delegateMethod(shinyBinding, bindingDef, "clearError");
      delegateMethod(shinyBinding, bindingDef, "showProgress");

      // The find, renderValue, and resize are handled differently, because we
      // want to actually decorate the behavior of the bindingDef methods.

      shinyBinding.find = function(scope) {
        var results = bindingDef.find(scope);

        // Only return elements that are Shiny outputs, not static ones
        var dynamicResults = results.filter(".html-widget-output");

        // It's possible that whatever caused Shiny to think there might be
        // new dynamic outputs, also caused there to be new static outputs.
        // Since there might be lots of different htmlwidgets bindings, we
        // schedule execution for later--no need to staticRender multiple
        // times.
        if (results.length !== dynamicResults.length)
          scheduleStaticRender();

        return dynamicResults;
      };

      // Wrap renderValue to handle initialization, which unfortunately isn't
      // supported natively by Shiny at the time of this writing.

      shinyBinding.renderValue = function(el, data) {
        Shiny.renderDependencies(data.deps);
        // Resolve strings marked as javascript literals to objects
        if (!(data.evals instanceof Array)) data.evals = [data.evals];
        for (var i = 0; data.evals && i < data.evals.length; i++) {
          window.HTMLWidgets.evaluateStringMember(data.x, data.evals[i]);
        }
        if (!bindingDef.renderOnNullValue) {
          if (data.x === null) {
            el.style.visibility = "hidden";
            return;
          } else {
            el.style.visibility = "inherit";
          }
        }
        if (!elementData(el, "initialized")) {
          initSizing(el);

          elementData(el, "initialized", true);
          if (bindingDef.initialize) {
            var result = bindingDef.initialize(el, el.offsetWidth,
              el.offsetHeight);
            elementData(el, "init_result", result);
          }
        }
        bindingDef.renderValue(el, data.x, elementData(el, "init_result"));
        evalAndRun(data.jsHooks.render, elementData(el, "init_result"), [el, data.x]);
      };

      // Only override resize if bindingDef implements it
      if (bindingDef.resize) {
        shinyBinding.resize = function(el, width, height) {
          // Shiny can call resize before initialize/renderValue have been
          // called, which doesn't make sense for widgets.
          if (elementData(el, "initialized")) {
            bindingDef.resize(el, width, height, elementData(el, "init_result"));
          }
        };
      }

      Shiny.outputBindings.register(shinyBinding, bindingDef.name);
    }
  };

  var scheduleStaticRenderTimerId = null;
  function scheduleStaticRender() {
    if (!scheduleStaticRenderTimerId) {
      scheduleStaticRenderTimerId = setTimeout(function() {
        scheduleStaticRenderTimerId = null;
        window.HTMLWidgets.staticRender();
      }, 1);
    }
  }

  // Render static widgets after the document finishes loading
  // Statically render all elements that are of this widget's class
  window.HTMLWidgets.staticRender = function() {
    var bindings = window.HTMLWidgets.widgets || [];
    forEach(bindings, function(binding) {
      var matches = binding.find(document.documentElement);
      forEach(matches, function(el) {
        var sizeObj = initSizing(el, binding);

        if (hasClass(el, "html-widget-static-bound"))
          return;
        el.className = el.className + " html-widget-static-bound";

        var initResult;
        if (binding.initialize) {
          initResult = binding.initialize(el,
            sizeObj ? sizeObj.getWidth() : el.offsetWidth,
            sizeObj ? sizeObj.getHeight() : el.offsetHeight
          );
          elementData(el, "init_result", initResult);
        }

        if (binding.resize) {
          var lastSize = {
            w: sizeObj ? sizeObj.getWidth() : el.offsetWidth,
            h: sizeObj ? sizeObj.getHeight() : el.offsetHeight
          };
          var resizeHandler = function(e) {
            var size = {
              w: sizeObj ? sizeObj.getWidth() : el.offsetWidth,
              h: sizeObj ? sizeObj.getHeight() : el.offsetHeight
            };
            if (size.w === 0 && size.h === 0)
              return;
            if (size.w === lastSize.w && size.h === lastSize.h)
              return;
            lastSize = size;
            binding.resize(el, size.w, size.h, initResult);
          };

          on(window, "resize", resizeHandler);

          // This is needed for cases where we're running in a Shiny
          // app, but the widget itself is not a Shiny output, but
          // rather a simple static widget. One example of this is
          // an rmarkdown document that has runtime:shiny and widget
          // that isn't in a render function. Shiny only knows to
          // call resize handlers for Shiny outputs, not for static
          // widgets, so we do it ourselves.
          if (window.jQuery) {
            window.jQuery(document).on(
              "shown.htmlwidgets shown.bs.tab.htmlwidgets shown.bs.collapse.htmlwidgets",
              resizeHandler
            );
            window.jQuery(document).on(
              "hidden.htmlwidgets hidden.bs.tab.htmlwidgets hidden.bs.collapse.htmlwidgets",
              resizeHandler
            );
          }

          // This is needed for the specific case of ioslides, which
          // flips slides between display:none and display:block.
          // Ideally we would not have to have ioslide-specific code
          // here, but rather have ioslides raise a generic event,
          // but the rmarkdown package just went to CRAN so the
          // window to getting that fixed may be long.
          if (window.addEventListener) {
            // It's OK to limit this to window.addEventListener
            // browsers because ioslides itself only supports
            // such browsers.
            on(document, "slideenter", resizeHandler);
            on(document, "slideleave", resizeHandler);
          }
        }

        var scriptData = document.querySelector("script[data-for='" + el.id + "'][type='application/json']");
        if (scriptData) {
          var data = JSON.parse(scriptData.textContent || scriptData.text);
          // Resolve strings marked as javascript literals to objects
          if (!(data.evals instanceof Array)) data.evals = [data.evals];
          for (var k = 0; data.evals && k < data.evals.length; k++) {
            window.HTMLWidgets.evaluateStringMember(data.x, data.evals[k]);
          }
          binding.renderValue(el, data.x, initResult);
          evalAndRun(data.jsHooks.render, initResult, [el, data.x]);
        }
      });
    });

    invokePostRenderHandlers();
  }


  function has_jQuery3() {
    if (!window.jQuery) {
      return false;
    }
    var $version = window.jQuery.fn.jquery;
    var $major_version = parseInt($version.split(".")[0]);
    return $major_version >= 3;
  }

  /*
  / Shiny 1.4 bumped jQuery from 1.x to 3.x which means jQuery's
  / on-ready handler (i.e., $(fn)) is now asyncronous (i.e., it now
  / really means $(setTimeout(fn)).
  / https://jquery.com/upgrade-guide/3.0/#breaking-change-document-ready-handlers-are-now-asynchronous
  /
  / Since Shiny uses $() to schedule initShiny, shiny>=1.4 calls initShiny
  / one tick later than it did before, which means staticRender() is
  / called renderValue() earlier than (advanced) widget authors might be expecting.
  / https://github.com/rstudio/shiny/issues/2630
  /
  / For a concrete example, leaflet has some methods (e.g., updateBounds)
  / which reference Shiny methods registered in initShiny (e.g., setInputValue).
  / Since leaflet is privy to this life-cycle, it knows to use setTimeout() to
  / delay execution of those methods (until Shiny methods are ready)
  / https://github.com/rstudio/leaflet/blob/18ec981/javascript/src/index.js#L266-L268
  /
  / Ideally widget authors wouldn't need to use this setTimeout() hack that
  / leaflet uses to call Shiny methods on a staticRender(). In the long run,
  / the logic initShiny should be broken up so that method registration happens
  / right away, but binding happens later.
  */
  function maybeStaticRenderLater() {
    if (shinyMode && has_jQuery3()) {
      window.jQuery(window.HTMLWidgets.staticRender);
    } else {
      window.HTMLWidgets.staticRender();
    }
  }

  if (document.addEventListener) {
    document.addEventListener("DOMContentLoaded", function() {
      document.removeEventListener("DOMContentLoaded", arguments.callee, false);
      maybeStaticRenderLater();
    }, false);
  } else if (document.attachEvent) {
    document.attachEvent("onreadystatechange", function() {
      if (document.readyState === "complete") {
        document.detachEvent("onreadystatechange", arguments.callee);
        maybeStaticRenderLater();
      }
    });
  }


  window.HTMLWidgets.getAttachmentUrl = function(depname, key) {
    // If no key, default to the first item
    if (typeof(key) === "undefined")
      key = 1;

    var link = document.getElementById(depname + "-" + key + "-attachment");
    if (!link) {
      throw new Error("Attachment " + depname + "/" + key + " not found in document");
    }
    return link.getAttribute("href");
  };

  window.HTMLWidgets.dataframeToD3 = function(df) {
    var names = [];
    var length;
    for (var name in df) {
        if (df.hasOwnProperty(name))
            names.push(name);
        if (typeof(df[name]) !== "object" || typeof(df[name].length) === "undefined") {
            throw new Error("All fields must be arrays");
        } else if (typeof(length) !== "undefined" && length !== df[name].length) {
            throw new Error("All fields must be arrays of the same length");
        }
        length = df[name].length;
    }
    var results = [];
    var item;
    for (var row = 0; row < length; row++) {
        item = {};
        for (var col = 0; col < names.length; col++) {
            item[names[col]] = df[names[col]][row];
        }
        results.push(item);
    }
    return results;
  };

  window.HTMLWidgets.transposeArray2D = function(array) {
      if (array.length === 0) return array;
      var newArray = array[0].map(function(col, i) {
          return array.map(function(row) {
              return row[i]
          })
      });
      return newArray;
  };
  // Split value at splitChar, but allow splitChar to be escaped
  // using escapeChar. Any other characters escaped by escapeChar
  // will be included as usual (including escapeChar itself).
  function splitWithEscape(value, splitChar, escapeChar) {
    var results = [];
    var escapeMode = false;
    var currentResult = "";
    for (var pos = 0; pos < value.length; pos++) {
      if (!escapeMode) {
        if (value[pos] === splitChar) {
          results.push(currentResult);
          currentResult = "";
        } else if (value[pos] === escapeChar) {
          escapeMode = true;
        } else {
          currentResult += value[pos];
        }
      } else {
        currentResult += value[pos];
        escapeMode = false;
      }
    }
    if (currentResult !== "") {
      results.push(currentResult);
    }
    return results;
  }
  // Function authored by Yihui/JJ Allaire
  window.HTMLWidgets.evaluateStringMember = function(o, member) {
    var parts = splitWithEscape(member, '.', '\\');
    for (var i = 0, l = parts.length; i < l; i++) {
      var part = parts[i];
      // part may be a character or 'numeric' member name
      if (o !== null && typeof o === "object" && part in o) {
        if (i == (l - 1)) { // if we are at the end of the line then evalulate
          if (typeof o[part] === "string")
            o[part] = tryEval(o[part]);
        } else { // otherwise continue to next embedded object
          o = o[part];
        }
      }
    }
  };

  // Retrieve the HTMLWidget instance (i.e. the return value of an
  // HTMLWidget binding's initialize() or factory() function)
  // associated with an element, or null if none.
  window.HTMLWidgets.getInstance = function(el) {
    return elementData(el, "init_result");
  };

  // Finds the first element in the scope that matches the selector,
  // and returns the HTMLWidget instance (i.e. the return value of
  // an HTMLWidget binding's initialize() or factory() function)
  // associated with that element, if any. If no element matches the
  // selector, or the first matching element has no HTMLWidget
  // instance associated with it, then null is returned.
  //
  // The scope argument is optional, and defaults to window.document.
  window.HTMLWidgets.find = function(scope, selector) {
    if (arguments.length == 1) {
      selector = scope;
      scope = document;
    }

    var el = scope.querySelector(selector);
    if (el === null) {
      return null;
    } else {
      return window.HTMLWidgets.getInstance(el);
    }
  };

  // Finds all elements in the scope that match the selector, and
  // returns the HTMLWidget instances (i.e. the return values of
  // an HTMLWidget binding's initialize() or factory() function)
  // associated with the elements, in an array. If elements that
  // match the selector don't have an associated HTMLWidget
  // instance, the returned array will contain nulls.
  //
  // The scope argument is optional, and defaults to window.document.
  window.HTMLWidgets.findAll = function(scope, selector) {
    if (arguments.length == 1) {
      selector = scope;
      scope = document;
    }

    var nodes = scope.querySelectorAll(selector);
    var results = [];
    for (var i = 0; i < nodes.length; i++) {
      results.push(window.HTMLWidgets.getInstance(nodes[i]));
    }
    return results;
  };

  var postRenderHandlers = [];
  function invokePostRenderHandlers() {
    while (postRenderHandlers.length) {
      var handler = postRenderHandlers.shift();
      if (handler) {
        handler();
      }
    }
  }

  // Register the given callback function to be invoked after the
  // next time static widgets are rendered.
  window.HTMLWidgets.addPostRenderHandler = function(callback) {
    postRenderHandlers.push(callback);
  };

  // Takes a new-style instance-bound definition, and returns an
  // old-style class-bound definition. This saves us from having
  // to rewrite all the logic in this file to accomodate both
  // types of definitions.
  function createLegacyDefinitionAdapter(defn) {
    var result = {
      name: defn.name,
      type: defn.type,
      initialize: function(el, width, height) {
        return defn.factory(el, width, height);
      },
      renderValue: function(el, x, instance) {
        return instance.renderValue(x);
      },
      resize: function(el, width, height, instance) {
        return instance.resize(width, height);
      }
    };

    if (defn.find)
      result.find = defn.find;
    if (defn.renderError)
      result.renderError = defn.renderError;
    if (defn.clearError)
      result.clearError = defn.clearError;

    return result;
  }
})();

\"></script>\n",
       "<link href=\"data:text/css;charset-utf-8;base64,LnN0cl92aWV3IHVsLCAuc3RyX3ZpZXcgbGkgewogIGxpc3Qtc3R5bGU6IG5vbmU7CiAgcGFkZGluZzogMDsKICBtYXJnaW46IDAuNWVtIDA7CiAgZm9udC1mYW1pbHk6IG1vbm9zcGFjZTsKfQoKLnN0cl92aWV3IC5tYXRjaCB7CiAgYm9yZGVyOiAxcHggc29saWQgI2NjYzsKICBiYWNrZ3JvdW5kLWNvbG9yOiAjZWVlOwp9Cg==\" rel=\"stylesheet\" />\n",
       "<script title=\"str_view-binding\" src=\"data:application/javascript;base64,SFRNTFdpZGdldHMud2lkZ2V0KHsKCiAgbmFtZTogJ3N0cl92aWV3JywKCiAgdHlwZTogJ291dHB1dCcsCgogIGluaXRpYWxpemU6IGZ1bmN0aW9uKGVsLCB3aWR0aCwgaGVpZ2h0KSB7CiAgfSwKCiAgcmVuZGVyVmFsdWU6IGZ1bmN0aW9uKGVsLCB4LCBpbnN0YW5jZSkgewogICAgZWwuaW5uZXJIVE1MID0geC5odG1sOwogIH0sCgogIHJlc2l6ZTogZnVuY3Rpb24oZWwsIHdpZHRoLCBoZWlnaHQsIGluc3RhbmNlKSB7CiAgfQoKfSk7Cg==\"></script>\n",
       "\t</head>\n",
       "\t<body>\n",
       "\t\t<div id=\"htmlwidget-4f3605da91ef07fd9704\" style=\"width:960px;height:300px;\" class=\"str_view html-widget\"></div>\n",
       "<script type=\"application/json\" data-for=\"htmlwidget-4f3605da91ef07fd9704\">{\"x\":{\"html\":\"<ul>\\n  <li>Word2vec isn't <span class='match'>deep<\\/span> learning. Its explicitly and deliberately as shallow as possible - one of Mikolov's central realisations was that a very simple model trained on vast amounts of data can be superior to a complicated model trained on less data.\\n\\nThere's another algorithm called Paragraph2Vec which builds on Word2Vec and *does* incorporate '<span class='match'>deep<\\/span> learning' (in so far as it uses a neural network). Its purpose is to amalgamate the Word2Vec vectors for the words in a chunk of text - kinda like you do by averaging them. P2V tries to find a paragraph vector which is a good predictor of the word vectors, with the idea being that such a vector effectively represents the content of the paragraph.<\\/li>\\n  <li>But even if it is, so what? Such experiment could pay off if we would be able to figure out how to pick best model from 100 or even more runs.\\n\\nELM simply would learn better model than whatever <span class='match'>deep<\\/span> learning method learns. But my understanding that it is not what they claim, ELM proponents say that while performance is not on par with <span class='match'>deep<\\/span> learning, advantage is that you barely need any computations to learn, so model learns faster in terms of add-multiply computations.\\n<\\/li>\\n  <li>I think the extremely fast training speed claimed by ELM is something like cheating.\\nFor example, ELM papers usually report training mnist in few minutes while <span class='match'>deep<\\/span> learning in several hours, or training other (little more complex than mnist) toy datasets in hours while <span class='match'>deep<\\/span> learning in days. I am sure they elaborately chosed some slowest <span class='match'>deep<\\/span> learning algrithims in out-of-date papers with very naive implementations.\\nAs a fair comparison, I can train an 8 layer imagenet classification model in 15 hours using one GTX780, with top 1 error rate same as AlexNet, and I am sure again, during the ELM research lifetime, they can not get this fast training speed<\\/li>\\n  <li>RBMs are still state of the art at modeling binary data, afaik (although this may have changed in the last year or so as I haven't been keeping up).\\n\\nBut for pre-training you can use autoencoders, which is much easier (no intractability, no need for approximations). And as a generative model of non-binary data, RBMs perform very poorly (even Gaussian mixture models often beat Gaussian RBMs).\\n\\nIt seems that the <span class='match'>deep<\\/span> learning community is slowly getting over the 'convnet explosion' of 2012 a bit, and there's a lot more interest again in generative models (along with recurrent models). Variational autoencoders are just one example, a lot of new ideas for generative models are currently being published.<\\/li>\\n  <li>Hey guys- I came across this yesterday and I was at first startled by its accuracy.  Then I began to notice-\\n\\n* It's really slow- 10-15 seconds is typical.\\n* The results are rather inconsistent.\\n* I've come across misspellings, like \\\"ell\\\" instead of \\\"eel\\\"\\n\\nA press release from them says:\\n\\n&gt;\\\"Using <span class='match'>deep<\\/span> learning, we set up an advanced image recognition technology to give results from our servers,\\\" says Mazur. \\\"With this, we can go <span class='match'>deep<\\/span>ly into identifying, say, the exact make and model of a car or breed of dog — not just a classification. What sets us apart is that we always provide an answer with a varying degree of detail. It's not just an exact answer or no answer at all.\\\"\\n\\nhttp://www.prnewswire.com/news-releases/camfind---the-first-visual-search-engine-goes-social-300067412.html\\n\\nWhat do you guys think?\\n\\n**edit** By \\\"human\\\" I'm thinking Mechanical Turk.<\\/li>\\n  <li>It did get me thinking along those lines with my own project actually.  It's not a bad solution in a lot of cases.  \\n\\nI'm just dying of curiosity about this particular service though.  I'm pretty sure <span class='match'>deep<\\/span> learning isn't *this* good yet, but I can't find a way to prove it's not using it.<\\/li>\\n  <li>Right, the answer could have been found via google reverse image search or something similar, but definitely not <span class='match'>deep<\\/span> learning image recognition. You know what I mean?<\\/li>\\n  <li>Yeah, I think this system has a lot of different parts. Definitely OCR and dominant color detection, probably reverse image search and maybe <span class='match'>deep<\\/span> learning (ImageNet classifier would probably detect that your image was an animal, which combined with some wikipedia searching would confirm the info from the URL). \\n\\nMechanical Turk is probably being used as well, perhaps at random to give the appearance of \\\"magic\\\" results on some searches.<\\/li>\\n  <li>Most/bulk of calculations for <span class='match'>deep<\\/span> learning neural nets could be represented as matrix multiplications. And GPUs are good at multiplying matrices. \\n\\nNot sure about other ML algorithms. <\\/li>\\n  <li>That first answer is from 2011, before the <span class='match'>deep<\\/span> learning renaissance. I don't know that they necessarily use <span class='match'>deep<\\/span> learning for search queries now, but I don't think that question is good evidence at this point that they don't.<\\/li>\\n  <li>Is <span class='match'>deep<\\/span> learning used for search by any company? In my opinion, DL isn't particularly useful for search since there's no hierarchical representations in search data, and in case of good features ensembles work best. This is confirmed by my knowledge of machine learning behind Yandex – the major search engine in Russia. They don't use DL, but they do hell a lot of ensembling with all sorts of weak learners. But, again, it's not Google, so maybe the whole Google Search team got replaced with Google Brain years ago, while Google Brain team nowadays plays Atari games all days long, who knows?<\\/li>\\n  <li>I guess the initial excitement is over, but we will now see a lot more 'Deep Learning for X' in the coming years, where the original contribution will not be to design interesting networks, but use existing ones in unique ways. For example, CVPR this year has ~ 75 papers related to <span class='match'>deep<\\/span> learning, possibly the highest ever! <\\/li>\\n  <li>There might be fewer papers, but that isn't because some new approach has come out and replaced <span class='match'>deep<\\/span> learning. ConvNets are pretty well researched at this point, since those were the focus of a lot of effort the past few years, I think we are going to see something similar happen to recurrent nets for the next few years.<\\/li>\\n  <li>Deep Learning is a codename for \\\"Neural Networks\\\", which was introduced because NNs were hot in 80-90s, but then all hopes crashed, and people (mostly) abandoned them. Other methods (SVM, ensembles) took their place. It's said that papers that contain \\\"Neural\\\" in their titles were more likely to get rejected at conferences at that time. Oh, and, yes, those new neural nets are <span class='match'>deep<\\/span>er than their ancestors.\\n\\nThe <span class='match'>deep<\\/span> learning started out in mid 2000s with unsupervised pretraining. Hinton's lab used RBM to pretrain (unsupervised) a <span class='match'>deep<\\/span> neural net and got interesting results. As far as I understand, these guys then went to researching further development of similar methods. Deep Belief Networks, for example.\\n\\nThe next step was to recall LeCun's convolutional networks (they were invented and used in 90s for something like zipcode recognition): in 2012 Alex Krizhevsky blow up Computer Vision community by setting a new record on a major image recognition benchmark.\\n\\nThere are other problems that demonstrated how promising Deep Learning is, but I'm not going to speak of it, I'd rather concentrate of current state of the field.\\n\\nEssentially, over past 10 years of Deep Learning renaissance, the following things were understood / researched:\\n1. Computers got significantly faster (especially if you have a decent GPU)\\n2. We collected a lot of data\\n3. Since NN's objective is nonlinear, the result heavily depends on the initialization point. Nowadays people now several good ways to initialize weight matrices.\\n4. Some activation functions (ReLU, maxout) work better than others.\\n5. Some stochasticity (Dropout) forces your neural networks to learn mode robust representations.\\n6. New methods (AdaDelta, Adam, and lots and lots of others) of optimization speed up learning while keeping computational complexity low.\\n\\nAs far as I know, all modern neural nets (RBMs, Feedforward Nets, Convolutional nets, RNNs + LSTM) were invented even before the year 2000, so the success is mostly due to the facts I outlined above.\\n\\nReadings:\\nThere's an overview of the field from Ilya Sutskever: http://yyue.blogspot.ru/2015/01/a-brief-overview-of-<span class='match'>deep<\\/span>-learning.html\\nAnd the Deep Learning book from Bengio et al. http://www.iro.umontreal.ca/~bengioy/dlbook/<\\/li>\\n  <li>I can't wait until someone does an anthropological study of the fad stampedes in the AI/ML fields.  Neural nets to svms to bayes nonparametrics to <span class='match'>deep<\\/span> learning.  They all follow the pattern you've suggested and it'd be greatly interesting to analyze the frequency data of the different stages as a methodological optimization problem: papers stop because maxima have been reached.<\\/li>\\n  <li>It's not an \\\"ELI5\\\", because I wouldn't even be attempting to explain <span class='match'>deep<\\/span> learning (or probably even general machine learning) to a 5 year old.\\n\\nMost applied machine learning involves a heavy human investment in feature engineering, i.e. deriving information about your training cases in a useful format for an off-the-shelf algorithm to look at and learn from.\\n\\nDeep learning aims to learn appropriate features from the raw or nearly-raw data, one way or another. The most popular way today is to train a neural network with multiple layers of nonlinear feature extractors via stochastic gradient descent, essentially \\\"change each parameter in the direction which makes [some differentiable substitute for] the number and magnitude of the mistakes I make go down\\\". These directions are efficiently computable via the backpropagation algorithm.<\\/li>\\n  <li>May be not consciousness. But there is something (whatever it called) that decides if a given action is a good idea or not. For example is it Ok to spit or to dance right now? And critic seems to fulfills this role.\\n\\nAs for Chinese Room, Searle conveniently left out the fact that software model(books with hieroglyphs in his \\\"mental experiment\\\") could learn and could create new things. If room have learned math over the years and at the end produced textbook on <span class='match'>deep<\\/span> learning for example(in mandarin), while person learned nothing, it would leave reader with way different impression.\\n\\nAnd ML models do learn, and there are generative ones. Person in Chinese room is just a computer(literally one who computes, one who fulfills the role of hardware executing instructions). <\\/li>\\n  <li>You can also check out Yoshua Bengios <span class='match'>deep<\\/span> learning book (or rather the draft). It also has a chapter on RNNs. [Link](http://www.iro.umontreal.ca/~bengioy/dlbook/)<\\/li>\\n  <li>So a \\\"<span class='match'>deep<\\/span> boltzmann machine with fine scalability\\\" is essentially a neural network where all the layers are connected to all the other layers. This is like a restricted boltzmann machine expect that there are connections between some of the hidden units, but not all. The fact that the boltzmann machine is not fully connected allows for some Gibbs sampling to be run in parallel.\\n\\nPerhaps the results are not too surprising. The whole idea behind <span class='match'>deep<\\/span> learning is to build hierarchical representations of the data. It is hard to train the top-layers because they are so far removed from where the data flows in. However, just because they are hard to train does not mean they are irrelevant. Top-layers may be the key to building more versatile models.<\\/li>\\n  <li>I read the article and I still don't know why <span class='match'>deep<\\/span> learning is a mandate for anything. Sorry.<\\/li>\\n  <li>See [Quora thread](http://www.quora.com/Will-<span class='match'>deep<\\/span>-learning-make-other-Machine-Learning-algorithms-obsolete) on the question \\\"Will <span class='match'>deep<\\/span> learning make other Machine Learning algorithms obsolete\\\". There are some interesting and qualified answers on both sides.<\\/li>\\n  <li>In my opinion: for \\\"classic\\\", tabular data (as opposed to <span class='match'>deep<\\/span> learning stuff like images and autio), tree ensembles are at least as good as neural networks and much, much easier to use.<\\/li>\\n  <li>The reason neural networks are so effective is quite simple - the error is well attributed.\\n\\nBackpropagation was the 2nd important breakthrough , now error could be effectively be attributed to neurons <span class='match'>deep<\\/span> within the network.\\n\\nBackprop is just calculus, the gradient of downhill in error space.\\n\\nTrial and error, slight improvement each time gets closer to the desired answer - no mystery there.\\n\\nBackprop can get stuck in local minima and be unstable in very <span class='match'>deep<\\/span> or recurrent nets - evolutionary genetic methods seem to correct this with a more global search of the parameter space.\\n\\nhttp://colah.github.io/posts/2014-03-NN-Manifolds-Topology/\\n\\nThe idea that knowledge is a highly structured lower dimensional manifold in the unstructured high dimensional space of say pixels or wikipedia corpus seems legit.\\n\\nNeural networks alone are not effective - all the recent breakthroughs occur because of multiple levels of structure, <span class='match'>deep<\\/span> nets.\\n\\nMinsky's 1968 criticism that a single layer perceptron ( neural net) cannot model the XOR function.\\n\\nThen Geoff Hinton developed the 'hidden' layer - the 1st importatnt breakthrough.\\n\\nThis allows function compostition and XOR is easily modelled. More layers add more non-linearity and ever more complex functions can potentially be modelled.\\n\\nExactly why heirarchical representations are so effective at encoding complex structure compactly is not so clear but there are many examples in other branches of information theory. Huffman Trees for instance.\\n\\nLooking at the internal representations the <span class='match'>deep<\\/span> neural nets produce for a face detector - the 1st layer is all edge detector and HOG like filters , the second layer composes these into curves and small parts, in the next layer these are composed into eye and nose detectors and then a layer that detects faces.\\n\\nIMHO the analysis of why evolution so effectively produces solutions to enviroments is an analagous problem.\\n\\nThis video ask good questions\\n[The unreasnoble effictiveness of <span class='match'>deep<\\/span> learning](https://www.youtube.com/watch?v=sc-KbuZqGkI)<\\/li>\\n  <li>Sounds interesting!\\n\\nI think Theano would have been totally fine if we were building a more conventional  network. However, building exotic architectures like this without breaking Theano's symbolic differentiation graph is very bad. There are a couple of parts to this.\\n\\n1. **Anything you would normally implement as a loop or map you use [scan](http://www.<span class='match'>deep<\\/span>learning.net/software/theano/tutorial/loop.html).** Most networks would not involve inner scans, but in implementing the capsule model it proves to be unavoidable. (I figured out that it is *possible* to implement with only matrix operations, but you end up building matrices as large as the unfolded loop. The cure was worse than the disease.) Scan is slightly frustrating to use, but more importantly is brutally slow in both compilation/optimization and execution. Because loops generate symbolic graphs as <span class='match'>deep<\\/span> as the entire unfolded loop, differentiation is extremely expensive. Scan is also implemented in python, which results in interop overhead (especially if you're using a GPU). Profile evidence:\\n\\n    Class\\n    ---\\n    &lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; &lt;#call&gt; &lt;#apply&gt; &lt;Class name&gt;\\n      100.0%   100.0%     162.809s       4.07e+01s     Py       4        4   &lt;class 'theano.scan_module.scan_op.Scan'&gt;\\n\\n  (This line includes the computation done inside the scan — the scan op itself didn't consume three minutes of this run. But this code without the scan is a few orders of magnitude faster.)\\n\\n  This brings us to the second point:\\n\\n2. **What's actually happening under the hood (i.e., what you need to know to optimize your code) is hugely indirected from the code you write.** Programming in Theano is like performing surgery while wearing oven mitts. Which are greased.\\n\\n  [Conditionals](http://<span class='match'>deep<\\/span>learning.net/software/theano/tutorial/conditions.html) are probably the most comical example of this we ran into. There are two conditionals in the Theano symbolic system, `switch` and `ifelse`. `ifelse` does what you *want* a conditional to do; namely, it only evaluates the branch it actually takes. `switch`, on the other hand, *evaluates both branches*.\\n\\n  To put this in context, take the following code:\\n\\n    if false:\\n        fire_the_missiles()\\n    else:\\n        print(\\\"All clear! No missiles today!\\\")\\n\\n  `ifelse` would correctly print `All clear! No missiles today!` and avoid firing the missiles.\\n  `switch` would do both.\\n\\n  `switch` is only conditional in that it still returns the result of the correct branch.\\n\\n  From the docs:\\n  &gt; In this example, the IfElse op spends less time (about half as much) than Switch since it computes only one variable out of the two.\\n\\n  So at first glance, this seems like a home run. Just use `ifelse`. But wait! These two profiles are from code which is otherwise identical (they were run for different numbers of cycles — look at the time per call):\\n\\n    &lt;% time&gt; &lt;sum %&gt; &lt;apply time&gt; &lt;time per call&gt; &lt;type&gt; &lt;#call&gt; &lt;#apply&gt; &lt;Op name&gt;\\n       68.6%    68.6%      18.000s       9.40e-06s     Py    1914937       54   if{inplace}\\n       1.2%    84.7%       0.001s       9.77e-07s     C     1040       52   Elemwise{Composite{[Switch(GE(i0, i1), i2, Switch(i3, i2, i0))]}}\\n\\n  `ifelse`, despite only running one branch, takes an order of magnitude longer. That's because (and this is *not* in the docs) `ifelse` is written in Python, while `switch` is written in C. I ended up rewriting this code to use `switch` without firing any missiles.\\n\\n3. **Compiling your Theano every time you make a change eats days.** Anytime you rerun, Theano recomputes the symbolic graph from scratch and then compiles it to C/GPU/whatever. In our case this took upwards of twenty minutes. You can avoid (some of) this by running it as Python code and turning off optimization, but then actually running the code will take 1-2 orders of magnitude longer. For simpler graphs, this might not be a problem.\\n\\n\\nSorry about the rant — this was just a very frustrating experience, and I'd like to save anyone else from repeating it.\\n\\n\\nBy contrast, writing this in Torch was a great experience. It did require calculating gradients by (well, by Matlab), but they only took a few days to get right anyway. At every step we knew what we were doing, what things were working and what weren't, and what the machine was doing.\\n\\nLua itself runs pretty damn fast ([language speed comparison](http://julialang.org/benchmarks/)), so you can afford to just write straight Lua for most things. Since there's no symbolic graph to tiptoe around, you can write whatever raw code you want to without breaking anything or causing exponential slowdowns. And if you want to, it's super easy (actually, completely trivial — it just works) to interop with a CUDA kernel. \\n\\nPlus, Torch is improving very fast. DeepMind, Facebook, Google, Toronto, Montreal, NYU, us at MIT... [all using Torch](http://torch.ch/whoweare.html), and all contributing [libraries](https://research.facebook.com/blog/879898285375829/fair-open-sources-<span class='match'>deep<\\/span>-learning-modules-for-torch/), [tools](https://github.com/facebook/iTorch), and [projects](https://sites.google.com/a/<span class='match'>deep<\\/span>mind.com/dqn/). It's not just the future of the <span class='match'>deep<\\/span> learning community, it's the *present* of the <span class='match'>deep<\\/span> learning community.\\n\\nSwitching to Torch was a huge relief. Save yourself the heartache and go straight there.\\n\\nNinja edit: caveat emptor, but take our code if it helps: http://www.reddit.com/r/MachineLearning/comments/35tqvg/understanding_optimizing_neural_networks_that/cr8zv9g<\\/li>\\n  <li>This was less than a year ago, and the data domain was essentially any that a large web company would collect - click through rates, purchase histories, user profiles, historical behavior of these features, etc.\\n\\n&gt;  the value these nets create eclipses the insane money the Very Large Companies are paying for top neural net engineers\\n\\nApart from a few specialized domains, <span class='match'>deep<\\/span> networks rarely provide huge gains, especially on tabular data as someone below mentioned. Thus from cost/benefit standpoint I think a lot of companies choose to use a simple model that gets you 90% there and stop before hiring a team of <span class='match'>deep<\\/span> network engineers that will get you 92% there.  My team had one part-time neural network engineer compared to roughly 10-20 other statisticians and general ML researchers.\\n\\nI actually mainly focus on <span class='match'>deep<\\/span> learning in my research, but it's nowhere near the point where you can throw a dataset at an off-the-shelf solver and have it produce something meaningful.<\\/li>\\n  <li>Have you seen lectures from Nando on <span class='match'>deep<\\/span> learning ? He is talking about the model in one of them. here is the playlist http://www.youtube.com/watch?v=P78QYjWh5sM&amp;index=3&amp;list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu<\\/li>\\n  <li>The approach is actually pretty similar to working with categorical data. Essentially, you create sparse vectors of numbers from the vocabulary (all unique words that are contained in the combined documents) for each document where the values in the vector can be simply the word count frequency, or inverse-document frequency and so on. That's basically the classic approach, the so-called bag-of-words model. I have written [an article](http://sebastianraschka.com/Articles/2014_naive_bayes_1.html) on Naive Bayes classifiers and bag-of-words if you are interested.\\n\\nThen, there is also the \\\"<span class='match'>deep<\\/span> learning\\\" approach called Word2Vec (by Google). You can find an overview and useful links [here](https://code.google.com/p/word2vec/).\\n\\n<\\/li>\\n  <li>&gt; 10x speedup\\n\\nCareful.\\n\\n&gt; It will have 3D memory, resulting in up to 5X improvement in <span class='match'>deep<\\/span> learning applications.  And it will feature NVLink – NVIDIA’s high-speed interconnect, which links together two or more GPUs — that will lead to a total 10X improvement in <span class='match'>deep<\\/span> learning.\\n\\nSo 2X of the 10X there comes from using two GPUs, and the remaining '5X improvement' is not the same as 'up to 5X improvement'.<\\/li>\\n  <li>QUOTE: *\\\"Mixed-precision computing enables Pascal architecture-based GPUs to compute at 16-bit floating point accuracy at* **twice the rate** *of 32-bit floating point accuracy.*\\n\\n*Increased floating point performance particularly benefits classification and convolution – two key activities in <span class='match'>deep<\\/span> learning – while achieving needed accuracy\\\"*\\n\\n<\\/li>\\n  <li>What a sensationalist headline. Does 16-bit FP work for *all of <span class='match'>deep<\\/span> learning* or just something nvidia bothered to test on ? <\\/li>\\n  <li>It's interesting that unsupervsed pretraining can hurt the performance. In the earlier days of <span class='match'>deep<\\/span> learning Geoff Hinton had a kind of information-theoretical argument for UL (if I remember correctly): there is a lot more information in the images than in the labels. This should be true even when all of the images are labelled...\\n\\nBy the way, I wish the authors defined what the unpooling operator `F` does exactly (I can guess, but I'd rather not)<\\/li>\\n  <li>tl;dw : \\\"The latest result for Pacman is now superhuman.\\\"\\n\\n[**Sildes** for David Silver's ICLR 2015 Talk \\\\[pdf\\\\]](http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf) --- slide [22](http://www.iclr.cc/lib/exe/fetch.php?media=iclr2015:silver-iclr2015.pdf#22) discusses Google Deepmind's latest GORILA, a massively parallel reinforcement learner.\\n\\n**\\\"** In this talk I will discuss how reinforcement learning (RL) can be combined with <span class='match'>deep<\\/span> learning (DL). There are several ways to combine DL and RL together, including value-based, policy-based, and model-based approaches with planning. Several of these approaches have well-known divergence issues, and I will present simple methods for addressing these instabilities. These methods have achieved notable success in the Atari 2600 domain. I will present recent a selection of recent results that improve on the published state-of-the-art in Atari and other challenging domains. Finally, I will discuss how RL can be used to improve DL, even when the native problem is supervised or unsupervised learning.**\\\"** - **David Silver**, ICLR 2015\\n\\n[**Video of Part 2**](https://www.youtube.com/watch?v=zXa6UFLQCtg)\\n\\n--\\n\\n[ICLR 2015, for papers &amp; slides, scroll far down](http://www.iclr.cc/doku.php?id=iclr2015:main)\\n\\n--\\n\\n[David Silver's Reinforcement Learning Course UC London](https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;list=PL5X3mDkKaJrL42i_jhE4N-p6E2Ol62Ofa)<\\/li>\\n  <li>In addition to SuperFX's [comment](https://www.reddit.com/r/MachineLearning/comments/36jj3h/neural_network_summer_curriculum/creq46l),\\n\\nRead/watch these papers/slides/videos:\\nhttp://labrosa.ee.columbia.edu/cuneuralnet/\\nhttp://nlp.stanford.edu/courses/NAACL2013/\\nhttps://youtu.be/DleXA5ADG78\\n\\nDo this tutorial:\\nhttp://<span class='match'>deep<\\/span>learning.net/tutorial/contents.html\\n\\nRead this online book:\\nhttp://neuralnetworksand<span class='match'>deep<\\/span>learning.com/\\n\\nRead this arxiv <span class='match'>deep<\\/span> learning survey paper by Schmidhuber:\\nhttp://arxiv.org/abs/1404.7828\\n\\nRead any and all trending arxiv papers about neural networks. If you want I can send you a giant list of interesting ones I've bookmarked over the last two years.\\n\\nRead this book (although it's about spiking neural models, which are more complex than artificial/<span class='match'>deep<\\/span> neural networks):\\nhttp://www.amazon.com/How-Build-Brain-Architecture-Architectures/dp/0199794545<\\/li>\\n  <li>Deep Learning is a subfield of Machine Learning.  There are a lot of things that fall under the umbrella of ML that aren't relevant to <span class='match'>deep<\\/span> learning.<\\/li>\\n  <li>Deep learning is a part of machine learning rather than a distinct field. It will be much easier to start at the bottom with something like logistic regression or random trees rather than jumped straight into <span class='match'>deep<\\/span> learning.<\\/li>\\n  <li>Since you have python experience, I would recommend getting familiar with Theano. Theano underpins a lot of <span class='match'>deep<\\/span> learning packages, so if you are comfortable with it you have a much better understanding of what those libraries are doing. Theano is really a math compiler that is used for <span class='match'>deep<\\/span> learning, so coding in theano tends to be expressing the math as the specific theano functions. I found this to be really helpful to understanding the math, since you can write the code and experiment with changing things pretty easily. \\n\\nThey have a good set of [tutorials](http://<span class='match'>deep<\\/span>learning.net/tutorial/contents.html) with example code to learn from. <\\/li>\\n  <li>Thats true, I did scikit-learn before theano, but the OP sounds interested in the <span class='match'>deep<\\/span> learning aspects so might as well start there. Scikit-learn didn't really give any benefit to learning theano, but Numpy absolutely would. <\\/li>\\n  <li>I don't want to interrupt OP's request, but I wanted to ask a question too. I also have a huge interest within AI, <span class='match'>deep<\\/span> learning and NLP, but have the same issues, the math. Does anyone know what kind of subjects/math would be good to follow up? I know it's a lot of statistical subjects. I've had a course that's a step under calculus which includes basic linear algebra and differential equations, should I consider to take calculus also? Udacity has some good courses (free?) for Stasticial subjects btw, I've started a bit on the intro to machine learning course:) <\\/li>\\n  <li>It is up to OP to chose but I will explain my point a little better:\\n\\nTheano introduces a new paradigm and OP will have to learn it at the same time that numpy, ML \\\"good practices\\\" and the math. Scikit-learn is way easier and he can treat it like a black-box\\n\\nAlso, is OP comfortable with Linux and has a good GPU? If the answer to one of these is no the feedback time to get any results can be a pain for OP's own learning process when doing <span class='match'>deep<\\/span> learning. I wouldn't recommend at all trying to do Theano with GPU in Windows, my experience ended up with a BSoD when trying to install CUDA<\\/li>\\n  <li>Sounds like Andrej Karpathy of Stanford U. is also moving over to Torch:\\n\\n\\\"The code is written in Torch 7, which has recently become my favorite <span class='match'>deep<\\/span> learning framework. I've only started working with Torch/LUA over the last few months and it hasn't been easy (I spent a good amount of time digging through the raw Torch code on Github and asking questions on their gitter to get things done), but once you get a hang of things it offers a lot of flexibility and speed. I've also worked with Caffe and Theano in the past and I believe Torch, while not perfect, gets its levels of abstraction and philosophy right better than others.\\\" <\\/li>\\n  <li>Geoff Hinton has a good introduction to Python on his webpage: \\n\\nhttp://www.cs.toronto.edu/~hinton/\\n\\nHe gives a lot of examples of <span class='match'>deep<\\/span> learning in Python.  <\\/li>\\n  <li>I don't mean to be sassy.  The former post saying Bishop is weak on <span class='match'>deep<\\/span> learning. I thought it was ironic because Toronto is famous for its Deep Learning (e.g. Hinton) and we use Bishop exclusively.<\\/li>\\n  <li>Caffe is a pretty good <span class='match'>deep<\\/span> learning framework, mshadow/cxxnet as well. However, they are nowhere near as good for prototyping.<\\/li>\\n  <li>Here's a [list of resources about <span class='match'>deep<\\/span> learning and neural networks](http://datacoder.aidanf.net/issues/4#start) that I put together last week .<\\/li>\\n  <li>\\\"Reproduced with permission from [C. Olah](http://colah.github.io/).\\\"\\n\\nC Olah has some serious clear analysis of neural nets , convolutions and <span class='match'>deep<\\/span> learning - his few posts are absolute gems of clarity.\\n<\\/li>\\n  <li>I am serious, its mostly because of my interest in <span class='match'>deep<\\/span> learning that I want to try it, thus I posted here. <\\/li>\\n  <li>I really like kernel methods, although I don't use them as much as maybe I should.  \\n\\n* Support vector machines haven't gone away, and they're still just as good as they were before <span class='match'>deep<\\/span> learning took off.\\n\\n* I've also been really impressed with Gaussian process priors and gaussian process regression, and they've gotten more scalable and robust.<\\/li>\\n  <li>There is still some random forest/ensemble of decision tree work going on that is quite relevant to medical genomics.\\n\\nA lot of it focuses on improved feature selection and importance scores which is very important in wide data sets with lots of irrelevant features (ie genetics). Off the top of my head check out \\\"Artificial Contrasts With Ensembles\\\" ( [my implementation](https://github.com/ryanbressler/CloudForest) ) which has been around for a bit i've found to work quit well with genetic data and maybe \\\"Module Guided Random Forests\\\" which I haven't had time to implement/try but may be adaptable to use say pathway information.\\n\\nHellinger distance decision trees are also really cool and have seen some recent interest.\\n\\nOther areas of active research include scaling all sorts of methods, manifold learning, compressed sensing and matrix factorization methods. [Check out nuit blache if you haven't.](http://nuit-blanche.blogspot.dk/) It highlights ton's of cool research only some of which is (currently know to be) related to <span class='match'>deep<\\/span> learning. And the author posts here sometimes. <\\/li>\\n  <li>A rough sample of the [2014 NIPS proceedings](http://papers.nips.cc/book/advances-in-neural-information-processing-systems-27-2014) suggests more than 75% of active ML research has very little to do with <span class='match'>deep<\\/span> learning or NNs. \\nI find DNNs intriguing but nearly all of my work involves traditional feature selection in supervised and semi-supervised scenarios.<\\/li>\\n  <li>I am doing research in reinforcement learning methods, especially policy search methods with finite horizon. My field of application is robotics, mainly because I enjoy it to see how my algorithms move things in the real world - simply because I do not get excited about results like \\\"It translated 75.342% of the words correctly from Finnish to English\\\". It is actually interesting to see how algorithms manipulate the real world directly.\\nFurthermore, I am interested in stochastic optimization methods and dimensionality reduction methods and how to translate these ideas into RL.\\n\\nWell, there is currently a huge hype about NN and Deep Learning (again) - I don't think I really want to participate in this in the near future. But of course, I try to be up to date about the current results with Deep learning in robotics, but sometimes I am a bit upset about sentences like \\\"If <span class='match'>deep<\\/span> learning does not work, just use more data\\\" and so on... maybe some people got a bit too excited. But other results are quiet interesting, on the other hand this field seems to be quite saturated with researchers (because it so cool,new and hip ;) ).<\\/li>\\n  <li>i use mrfs/crfs for biometrics, <span class='match'>deep<\\/span> learning is starting to make it's way into biometrics but one problem is that many biometric datasets just aren't that big. It's not like we can skim flikr or google images for images of chairs. Ground truth for faces/fingerprints/iris/etc usually involves a long process (mostly done by Kevin Boywer's team) of getting a couple hundred people in different environments.  Facebook/Google have started to turn that process upside down where people are submitting their labeled photos to them, and even then it's really just limited to faces.  Unfortunately not having much data rules out really leveraging many of those 'big data' methods. <\\/li>\\n  <li>To be honest, I would recommend starting simpler than neural nets. They're fun to play with (I wouldn't be doing graduate work in this area if I didn't think so) and incredibly powerful when deployed against tasks that actually require their flexibility, but any <span class='match'>deep<\\/span> learning (or machine learning in general) expert will tell you that to solve practical problems, you should start simple and not try to do too much at once. Were I trying to solve a practical problem quickly, my first recourse would never be to anything neural net related.\\n\\nIf you want to have a cool demo running relatively quickly, I settle on the problem you want to solve, and build the web frontend for it, isolating the machine learning code in one or a few methods/functions, such that you can easily swap it out. Start with some off-the-shelf thing in e.g. scikit-learn, a linear SVM or a random forest/boosted tree ensemble. These things are fairly good at coping whatever random thing you throw at them provided the task isn't too complex and the data isn't too large. Not only will this give you more immediate positive reinforcement and keep you interested, it will give you a baseline against which to compare other methods you try. If you can't beat a linear classifier with a neural network then your task may not be complex enough to benefit from the added model capacity; if you do significantly *worse* with a neural net then you either have bugs or are simply overfitting, and need to learn about regularization techniques.<\\/li>\\n  <li>His Stanford lecture is excellent, and much more \\\"meaty\\\" than the Coursera class. It's a full-on lecture, that delves much <span class='match'>deep<\\/span>er into the theory than the coursera ml-class. \\n\\nAs for your questions: it's an excellent starting point in ML, and covers a lot of basics. I think it can help you in understanding Bishop, although it doesn't cover all the chapters of Bishop, and is probably still a bit less theoretical than the book. So reading Bishop will give you a <span class='match'>deep<\\/span>er understanding, but the Ng lectures are a good starting point. Also note that there are class notes on the web for this class, and they are very good!\\n\\nI have no idea about the homework.\\n\\n\\nAlso: remember that the class was recorded in 2009 (IIRC), so it doesn't cover brand-new ML research (e.g. no <span class='match'>deep<\\/span> learning). However I actually think is a big plus, since you can learn foundations on Neural Nets anywhere (Hinton's coursera class is still a good starting point), but this class covers topics that I'd consider elementary for a solid base in ML.<\\/li>\\n  <li>I took the course at Stanford in 2013 and it wasn't that different from the 2010 version on SEE (which may be the ones available on youtube). The fundamentals don't change that quickly. You can quickly start working on cutting edge stuff after you have the basics (e.g. my course project used <span class='match'>deep<\\/span> learning, which wasn't covered at all in the lectures).\\n\\nOverall, I thought the course was well worth doing. For me Prof. Ng made things sound almost too easy in the lectures. I only really understood the material after laboring over the assignments so definitely don't think just viewing the videos is sufficient. That's my two cents anyways.<\\/li>\\n<\\/ul>\"},\"evals\":[],\"jsHooks\":[]}</script>\n",
       "\t</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "HTML widgets cannot be represented in plain text (need html)"
      ]
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View strings in HTML format with the first all occurences highlighted\n",
    "str_view_all(deep_learning_posts, pattern=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "'Word2vec isn\\'t deep learning. Its explicitly\\nand deliberately as shallow as possible - one of\\nMikolov\\'s central realisations was that a very\\nsimple model trained on vast amounts of data\\ncan be superior to a complicated model trained\\non less data. There\\'s another algorithm called\\nParagraph2Vec which builds on Word2Vec and *does*\\nincorporate \\'deep learning\\' (in so far as it uses\\na neural network). Its purpose is to amalgamate\\nthe Word2Vec vectors for the words in a chunk of\\ntext - kinda like you do by averaging them. P2V\\ntries to find a paragraph vector which is a good\\npredictor of the word vectors, with the idea being\\nthat such a vector effectively represents the\\ncontent of the paragraph.'"
      ],
      "text/latex": [
       "'Word2vec isn\\textbackslash{}'t deep learning. Its explicitly\\textbackslash{}nand deliberately as shallow as possible - one of\\textbackslash{}nMikolov\\textbackslash{}'s central realisations was that a very\\textbackslash{}nsimple model trained on vast amounts of data\\textbackslash{}ncan be superior to a complicated model trained\\textbackslash{}non less data. There\\textbackslash{}'s another algorithm called\\textbackslash{}nParagraph2Vec which builds on Word2Vec and *does*\\textbackslash{}nincorporate \\textbackslash{}'deep learning\\textbackslash{}' (in so far as it uses\\textbackslash{}na neural network). Its purpose is to amalgamate\\textbackslash{}nthe Word2Vec vectors for the words in a chunk of\\textbackslash{}ntext - kinda like you do by averaging them. P2V\\textbackslash{}ntries to find a paragraph vector which is a good\\textbackslash{}npredictor of the word vectors, with the idea being\\textbackslash{}nthat such a vector effectively represents the\\textbackslash{}ncontent of the paragraph.'"
      ],
      "text/markdown": [
       "'Word2vec isn\\'t deep learning. Its explicitly\\nand deliberately as shallow as possible - one of\\nMikolov\\'s central realisations was that a very\\nsimple model trained on vast amounts of data\\ncan be superior to a complicated model trained\\non less data. There\\'s another algorithm called\\nParagraph2Vec which builds on Word2Vec and *does*\\nincorporate \\'deep learning\\' (in so far as it uses\\na neural network). Its purpose is to amalgamate\\nthe Word2Vec vectors for the words in a chunk of\\ntext - kinda like you do by averaging them. P2V\\ntries to find a paragraph vector which is a good\\npredictor of the word vectors, with the idea being\\nthat such a vector effectively represents the\\ncontent of the paragraph.'"
      ],
      "text/plain": [
       "[1] \"Word2vec isn't deep learning. Its explicitly\\nand deliberately as shallow as possible - one of\\nMikolov's central realisations was that a very\\nsimple model trained on vast amounts of data\\ncan be superior to a complicated model trained\\non less data. There's another algorithm called\\nParagraph2Vec which builds on Word2Vec and *does*\\nincorporate 'deep learning' (in so far as it uses\\na neural network). Its purpose is to amalgamate\\nthe Word2Vec vectors for the words in a chunk of\\ntext - kinda like you do by averaging them. P2V\\ntries to find a paragraph vector which is a good\\npredictor of the word vectors, with the idea being\\nthat such a vector effectively represents the\\ncontent of the paragraph.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Format strings into paragraphs of a given width with str_wrap()\n",
    "wrapped <- str_wrap(data$body[str_which(data$body, \"deep learning\")][1], \n",
    "                    width = 50)\n",
    "wrapped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec isn't deep learning. Its explicitly\n",
      "and deliberately as shallow as possible - one of\n",
      "Mikolov's central realisations was that a very\n",
      "simple model trained on vast amounts of data\n",
      "can be superior to a complicated model trained\n",
      "on less data. There's another algorithm called\n",
      "Paragraph2Vec which builds on Word2Vec and *does*\n",
      "incorporate 'deep learning' (in so far as it uses\n",
      "a neural network). Its purpose is to amalgamate\n",
      "the Word2Vec vectors for the words in a chunk of\n",
      "text - kinda like you do by averaging them. P2V\n",
      "tries to find a paragraph vector which is a good\n",
      "predictor of the word vectors, with the idea being\n",
      "that such a vector effectively represents the\n",
      "content of the paragraph."
     ]
    }
   ],
   "source": [
    "# Print wrapped string with output obeying newlines\n",
    "wrapped %>% cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!doctype html>\n",
       "<html>\n",
       "\t<head>\n",
       "\t\t<meta charset=\"utf-8\">\n",
       "\t\t<script title=\"htmlwidgets\" src=\"data:application/javascript;base64,(function() {
  // If window.HTMLWidgets is already defined, then use it; otherwise create a
  // new object. This allows preceding code to set options that affect the
  // initialization process (though none currently exist).
  window.HTMLWidgets = window.HTMLWidgets || {};

  // See if we're running in a viewer pane. If not, we're in a web browser.
  var viewerMode = window.HTMLWidgets.viewerMode =
      /\bviewer_pane=1\b/.test(window.location);

  // See if we're running in Shiny mode. If not, it's a static document.
  // Note that static widgets can appear in both Shiny and static modes, but
  // obviously, Shiny widgets can only appear in Shiny apps/documents.
  var shinyMode = window.HTMLWidgets.shinyMode =
      typeof(window.Shiny) !== "undefined" && !!window.Shiny.outputBindings;

  // We can't count on jQuery being available, so we implement our own
  // version if necessary.
  function querySelectorAll(scope, selector) {
    if (typeof(jQuery) !== "undefined" && scope instanceof jQuery) {
      return scope.find(selector);
    }
    if (scope.querySelectorAll) {
      return scope.querySelectorAll(selector);
    }
  }

  function asArray(value) {
    if (value === null)
      return [];
    if ($.isArray(value))
      return value;
    return [value];
  }

  // Implement jQuery's extend
  function extend(target /*, ... */) {
    if (arguments.length == 1) {
      return target;
    }
    for (var i = 1; i < arguments.length; i++) {
      var source = arguments[i];
      for (var prop in source) {
        if (source.hasOwnProperty(prop)) {
          target[prop] = source[prop];
        }
      }
    }
    return target;
  }

  // IE8 doesn't support Array.forEach.
  function forEach(values, callback, thisArg) {
    if (values.forEach) {
      values.forEach(callback, thisArg);
    } else {
      for (var i = 0; i < values.length; i++) {
        callback.call(thisArg, values[i], i, values);
      }
    }
  }

  // Replaces the specified method with the return value of funcSource.
  //
  // Note that funcSource should not BE the new method, it should be a function
  // that RETURNS the new method. funcSource receives a single argument that is
  // the overridden method, it can be called from the new method. The overridden
  // method can be called like a regular function, it has the target permanently
  // bound to it so "this" will work correctly.
  function overrideMethod(target, methodName, funcSource) {
    var superFunc = target[methodName] || function() {};
    var superFuncBound = function() {
      return superFunc.apply(target, arguments);
    };
    target[methodName] = funcSource(superFuncBound);
  }

  // Add a method to delegator that, when invoked, calls
  // delegatee.methodName. If there is no such method on
  // the delegatee, but there was one on delegator before
  // delegateMethod was called, then the original version
  // is invoked instead.
  // For example:
  //
  // var a = {
  //   method1: function() { console.log('a1'); }
  //   method2: function() { console.log('a2'); }
  // };
  // var b = {
  //   method1: function() { console.log('b1'); }
  // };
  // delegateMethod(a, b, "method1");
  // delegateMethod(a, b, "method2");
  // a.method1();
  // a.method2();
  //
  // The output would be "b1", "a2".
  function delegateMethod(delegator, delegatee, methodName) {
    var inherited = delegator[methodName];
    delegator[methodName] = function() {
      var target = delegatee;
      var method = delegatee[methodName];

      // The method doesn't exist on the delegatee. Instead,
      // call the method on the delegator, if it exists.
      if (!method) {
        target = delegator;
        method = inherited;
      }

      if (method) {
        return method.apply(target, arguments);
      }
    };
  }

  // Implement a vague facsimilie of jQuery's data method
  function elementData(el, name, value) {
    if (arguments.length == 2) {
      return el["htmlwidget_data_" + name];
    } else if (arguments.length == 3) {
      el["htmlwidget_data_" + name] = value;
      return el;
    } else {
      throw new Error("Wrong number of arguments for elementData: " +
        arguments.length);
    }
  }

  // http://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
  function escapeRegExp(str) {
    return str.replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
  }

  function hasClass(el, className) {
    var re = new RegExp("\\b" + escapeRegExp(className) + "\\b");
    return re.test(el.className);
  }

  // elements - array (or array-like object) of HTML elements
  // className - class name to test for
  // include - if true, only return elements with given className;
  //   if false, only return elements *without* given className
  function filterByClass(elements, className, include) {
    var results = [];
    for (var i = 0; i < elements.length; i++) {
      if (hasClass(elements[i], className) == include)
        results.push(elements[i]);
    }
    return results;
  }

  function on(obj, eventName, func) {
    if (obj.addEventListener) {
      obj.addEventListener(eventName, func, false);
    } else if (obj.attachEvent) {
      obj.attachEvent(eventName, func);
    }
  }

  function off(obj, eventName, func) {
    if (obj.removeEventListener)
      obj.removeEventListener(eventName, func, false);
    else if (obj.detachEvent) {
      obj.detachEvent(eventName, func);
    }
  }

  // Translate array of values to top/right/bottom/left, as usual with
  // the "padding" CSS property
  // https://developer.mozilla.org/en-US/docs/Web/CSS/padding
  function unpackPadding(value) {
    if (typeof(value) === "number")
      value = [value];
    if (value.length === 1) {
      return {top: value[0], right: value[0], bottom: value[0], left: value[0]};
    }
    if (value.length === 2) {
      return {top: value[0], right: value[1], bottom: value[0], left: value[1]};
    }
    if (value.length === 3) {
      return {top: value[0], right: value[1], bottom: value[2], left: value[1]};
    }
    if (value.length === 4) {
      return {top: value[0], right: value[1], bottom: value[2], left: value[3]};
    }
  }

  // Convert an unpacked padding object to a CSS value
  function paddingToCss(paddingObj) {
    return paddingObj.top + "px " + paddingObj.right + "px " + paddingObj.bottom + "px " + paddingObj.left + "px";
  }

  // Makes a number suitable for CSS
  function px(x) {
    if (typeof(x) === "number")
      return x + "px";
    else
      return x;
  }

  // Retrieves runtime widget sizing information for an element.
  // The return value is either null, or an object with fill, padding,
  // defaultWidth, defaultHeight fields.
  function sizingPolicy(el) {
    var sizingEl = document.querySelector("script[data-for='" + el.id + "'][type='application/htmlwidget-sizing']");
    if (!sizingEl)
      return null;
    var sp = JSON.parse(sizingEl.textContent || sizingEl.text || "{}");
    if (viewerMode) {
      return sp.viewer;
    } else {
      return sp.browser;
    }
  }

  // @param tasks Array of strings (or falsy value, in which case no-op).
  //   Each element must be a valid JavaScript expression that yields a
  //   function. Or, can be an array of objects with "code" and "data"
  //   properties; in this case, the "code" property should be a string
  //   of JS that's an expr that yields a function, and "data" should be
  //   an object that will be added as an additional argument when that
  //   function is called.
  // @param target The object that will be "this" for each function
  //   execution.
  // @param args Array of arguments to be passed to the functions. (The
  //   same arguments will be passed to all functions.)
  function evalAndRun(tasks, target, args) {
    if (tasks) {
      forEach(tasks, function(task) {
        var theseArgs = args;
        if (typeof(task) === "object") {
          theseArgs = theseArgs.concat([task.data]);
          task = task.code;
        }
        var taskFunc = tryEval(task);
        if (typeof(taskFunc) !== "function") {
          throw new Error("Task must be a function! Source:\n" + task);
        }
        taskFunc.apply(target, theseArgs);
      });
    }
  }

  // Attempt eval() both with and without enclosing in parentheses.
  // Note that enclosing coerces a function declaration into
  // an expression that eval() can parse
  // (otherwise, a SyntaxError is thrown)
  function tryEval(code) {
    var result = null;
    try {
      result = eval(code);
    } catch(error) {
      if (!error instanceof SyntaxError) {
        throw error;
      }
      try {
        result = eval("(" + code + ")");
      } catch(e) {
        if (e instanceof SyntaxError) {
          throw error;
        } else {
          throw e;
        }
      }
    }
    return result;
  }

  function initSizing(el) {
    var sizing = sizingPolicy(el);
    if (!sizing)
      return;

    var cel = document.getElementById("htmlwidget_container");
    if (!cel)
      return;

    if (typeof(sizing.padding) !== "undefined") {
      document.body.style.margin = "0";
      document.body.style.padding = paddingToCss(unpackPadding(sizing.padding));
    }

    if (sizing.fill) {
      document.body.style.overflow = "hidden";
      document.body.style.width = "100%";
      document.body.style.height = "100%";
      document.documentElement.style.width = "100%";
      document.documentElement.style.height = "100%";
      if (cel) {
        cel.style.position = "absolute";
        var pad = unpackPadding(sizing.padding);
        cel.style.top = pad.top + "px";
        cel.style.right = pad.right + "px";
        cel.style.bottom = pad.bottom + "px";
        cel.style.left = pad.left + "px";
        el.style.width = "100%";
        el.style.height = "100%";
      }

      return {
        getWidth: function() { return cel.offsetWidth; },
        getHeight: function() { return cel.offsetHeight; }
      };

    } else {
      el.style.width = px(sizing.width);
      el.style.height = px(sizing.height);

      return {
        getWidth: function() { return el.offsetWidth; },
        getHeight: function() { return el.offsetHeight; }
      };
    }
  }

  // Default implementations for methods
  var defaults = {
    find: function(scope) {
      return querySelectorAll(scope, "." + this.name);
    },
    renderError: function(el, err) {
      var $el = $(el);

      this.clearError(el);

      // Add all these error classes, as Shiny does
      var errClass = "shiny-output-error";
      if (err.type !== null) {
        // use the classes of the error condition as CSS class names
        errClass = errClass + " " + $.map(asArray(err.type), function(type) {
          return errClass + "-" + type;
        }).join(" ");
      }
      errClass = errClass + " htmlwidgets-error";

      // Is el inline or block? If inline or inline-block, just display:none it
      // and add an inline error.
      var display = $el.css("display");
      $el.data("restore-display-mode", display);

      if (display === "inline" || display === "inline-block") {
        $el.hide();
        if (err.message !== "") {
          var errorSpan = $("<span>").addClass(errClass);
          errorSpan.text(err.message);
          $el.after(errorSpan);
        }
      } else if (display === "block") {
        // If block, add an error just after the el, set visibility:none on the
        // el, and position the error to be on top of the el.
        // Mark it with a unique ID and CSS class so we can remove it later.
        $el.css("visibility", "hidden");
        if (err.message !== "") {
          var errorDiv = $("<div>").addClass(errClass).css("position", "absolute")
            .css("top", el.offsetTop)
            .css("left", el.offsetLeft)
            // setting width can push out the page size, forcing otherwise
            // unnecessary scrollbars to appear and making it impossible for
            // the element to shrink; so use max-width instead
            .css("maxWidth", el.offsetWidth)
            .css("height", el.offsetHeight);
          errorDiv.text(err.message);
          $el.after(errorDiv);

          // Really dumb way to keep the size/position of the error in sync with
          // the parent element as the window is resized or whatever.
          var intId = setInterval(function() {
            if (!errorDiv[0].parentElement) {
              clearInterval(intId);
              return;
            }
            errorDiv
              .css("top", el.offsetTop)
              .css("left", el.offsetLeft)
              .css("maxWidth", el.offsetWidth)
              .css("height", el.offsetHeight);
          }, 500);
        }
      }
    },
    clearError: function(el) {
      var $el = $(el);
      var display = $el.data("restore-display-mode");
      $el.data("restore-display-mode", null);

      if (display === "inline" || display === "inline-block") {
        if (display)
          $el.css("display", display);
        $(el.nextSibling).filter(".htmlwidgets-error").remove();
      } else if (display === "block"){
        $el.css("visibility", "inherit");
        $(el.nextSibling).filter(".htmlwidgets-error").remove();
      }
    },
    sizing: {}
  };

  // Called by widget bindings to register a new type of widget. The definition
  // object can contain the following properties:
  // - name (required) - A string indicating the binding name, which will be
  //   used by default as the CSS classname to look for.
  // - initialize (optional) - A function(el) that will be called once per
  //   widget element; if a value is returned, it will be passed as the third
  //   value to renderValue.
  // - renderValue (required) - A function(el, data, initValue) that will be
  //   called with data. Static contexts will cause this to be called once per
  //   element; Shiny apps will cause this to be called multiple times per
  //   element, as the data changes.
  window.HTMLWidgets.widget = function(definition) {
    if (!definition.name) {
      throw new Error("Widget must have a name");
    }
    if (!definition.type) {
      throw new Error("Widget must have a type");
    }
    // Currently we only support output widgets
    if (definition.type !== "output") {
      throw new Error("Unrecognized widget type '" + definition.type + "'");
    }
    // TODO: Verify that .name is a valid CSS classname

    // Support new-style instance-bound definitions. Old-style class-bound
    // definitions have one widget "object" per widget per type/class of
    // widget; the renderValue and resize methods on such widget objects
    // take el and instance arguments, because the widget object can't
    // store them. New-style instance-bound definitions have one widget
    // object per widget instance; the definition that's passed in doesn't
    // provide renderValue or resize methods at all, just the single method
    //   factory(el, width, height)
    // which returns an object that has renderValue(x) and resize(w, h).
    // This enables a far more natural programming style for the widget
    // author, who can store per-instance state using either OO-style
    // instance fields or functional-style closure variables (I guess this
    // is in contrast to what can only be called C-style pseudo-OO which is
    // what we required before).
    if (definition.factory) {
      definition = createLegacyDefinitionAdapter(definition);
    }

    if (!definition.renderValue) {
      throw new Error("Widget must have a renderValue function");
    }

    // For static rendering (non-Shiny), use a simple widget registration
    // scheme. We also use this scheme for Shiny apps/documents that also
    // contain static widgets.
    window.HTMLWidgets.widgets = window.HTMLWidgets.widgets || [];
    // Merge defaults into the definition; don't mutate the original definition.
    var staticBinding = extend({}, defaults, definition);
    overrideMethod(staticBinding, "find", function(superfunc) {
      return function(scope) {
        var results = superfunc(scope);
        // Filter out Shiny outputs, we only want the static kind
        return filterByClass(results, "html-widget-output", false);
      };
    });
    window.HTMLWidgets.widgets.push(staticBinding);

    if (shinyMode) {
      // Shiny is running. Register the definition with an output binding.
      // The definition itself will not be the output binding, instead
      // we will make an output binding object that delegates to the
      // definition. This is because we foolishly used the same method
      // name (renderValue) for htmlwidgets definition and Shiny bindings
      // but they actually have quite different semantics (the Shiny
      // bindings receive data that includes lots of metadata that it
      // strips off before calling htmlwidgets renderValue). We can't
      // just ignore the difference because in some widgets it's helpful
      // to call this.renderValue() from inside of resize(), and if
      // we're not delegating, then that call will go to the Shiny
      // version instead of the htmlwidgets version.

      // Merge defaults with definition, without mutating either.
      var bindingDef = extend({}, defaults, definition);

      // This object will be our actual Shiny binding.
      var shinyBinding = new Shiny.OutputBinding();

      // With a few exceptions, we'll want to simply use the bindingDef's
      // version of methods if they are available, otherwise fall back to
      // Shiny's defaults. NOTE: If Shiny's output bindings gain additional
      // methods in the future, and we want them to be overrideable by
      // HTMLWidget binding definitions, then we'll need to add them to this
      // list.
      delegateMethod(shinyBinding, bindingDef, "getId");
      delegateMethod(shinyBinding, bindingDef, "onValueChange");
      delegateMethod(shinyBinding, bindingDef, "onValueError");
      delegateMethod(shinyBinding, bindingDef, "renderError");
      delegateMethod(shinyBinding, bindingDef, "clearError");
      delegateMethod(shinyBinding, bindingDef, "showProgress");

      // The find, renderValue, and resize are handled differently, because we
      // want to actually decorate the behavior of the bindingDef methods.

      shinyBinding.find = function(scope) {
        var results = bindingDef.find(scope);

        // Only return elements that are Shiny outputs, not static ones
        var dynamicResults = results.filter(".html-widget-output");

        // It's possible that whatever caused Shiny to think there might be
        // new dynamic outputs, also caused there to be new static outputs.
        // Since there might be lots of different htmlwidgets bindings, we
        // schedule execution for later--no need to staticRender multiple
        // times.
        if (results.length !== dynamicResults.length)
          scheduleStaticRender();

        return dynamicResults;
      };

      // Wrap renderValue to handle initialization, which unfortunately isn't
      // supported natively by Shiny at the time of this writing.

      shinyBinding.renderValue = function(el, data) {
        Shiny.renderDependencies(data.deps);
        // Resolve strings marked as javascript literals to objects
        if (!(data.evals instanceof Array)) data.evals = [data.evals];
        for (var i = 0; data.evals && i < data.evals.length; i++) {
          window.HTMLWidgets.evaluateStringMember(data.x, data.evals[i]);
        }
        if (!bindingDef.renderOnNullValue) {
          if (data.x === null) {
            el.style.visibility = "hidden";
            return;
          } else {
            el.style.visibility = "inherit";
          }
        }
        if (!elementData(el, "initialized")) {
          initSizing(el);

          elementData(el, "initialized", true);
          if (bindingDef.initialize) {
            var result = bindingDef.initialize(el, el.offsetWidth,
              el.offsetHeight);
            elementData(el, "init_result", result);
          }
        }
        bindingDef.renderValue(el, data.x, elementData(el, "init_result"));
        evalAndRun(data.jsHooks.render, elementData(el, "init_result"), [el, data.x]);
      };

      // Only override resize if bindingDef implements it
      if (bindingDef.resize) {
        shinyBinding.resize = function(el, width, height) {
          // Shiny can call resize before initialize/renderValue have been
          // called, which doesn't make sense for widgets.
          if (elementData(el, "initialized")) {
            bindingDef.resize(el, width, height, elementData(el, "init_result"));
          }
        };
      }

      Shiny.outputBindings.register(shinyBinding, bindingDef.name);
    }
  };

  var scheduleStaticRenderTimerId = null;
  function scheduleStaticRender() {
    if (!scheduleStaticRenderTimerId) {
      scheduleStaticRenderTimerId = setTimeout(function() {
        scheduleStaticRenderTimerId = null;
        window.HTMLWidgets.staticRender();
      }, 1);
    }
  }

  // Render static widgets after the document finishes loading
  // Statically render all elements that are of this widget's class
  window.HTMLWidgets.staticRender = function() {
    var bindings = window.HTMLWidgets.widgets || [];
    forEach(bindings, function(binding) {
      var matches = binding.find(document.documentElement);
      forEach(matches, function(el) {
        var sizeObj = initSizing(el, binding);

        if (hasClass(el, "html-widget-static-bound"))
          return;
        el.className = el.className + " html-widget-static-bound";

        var initResult;
        if (binding.initialize) {
          initResult = binding.initialize(el,
            sizeObj ? sizeObj.getWidth() : el.offsetWidth,
            sizeObj ? sizeObj.getHeight() : el.offsetHeight
          );
          elementData(el, "init_result", initResult);
        }

        if (binding.resize) {
          var lastSize = {
            w: sizeObj ? sizeObj.getWidth() : el.offsetWidth,
            h: sizeObj ? sizeObj.getHeight() : el.offsetHeight
          };
          var resizeHandler = function(e) {
            var size = {
              w: sizeObj ? sizeObj.getWidth() : el.offsetWidth,
              h: sizeObj ? sizeObj.getHeight() : el.offsetHeight
            };
            if (size.w === 0 && size.h === 0)
              return;
            if (size.w === lastSize.w && size.h === lastSize.h)
              return;
            lastSize = size;
            binding.resize(el, size.w, size.h, initResult);
          };

          on(window, "resize", resizeHandler);

          // This is needed for cases where we're running in a Shiny
          // app, but the widget itself is not a Shiny output, but
          // rather a simple static widget. One example of this is
          // an rmarkdown document that has runtime:shiny and widget
          // that isn't in a render function. Shiny only knows to
          // call resize handlers for Shiny outputs, not for static
          // widgets, so we do it ourselves.
          if (window.jQuery) {
            window.jQuery(document).on(
              "shown.htmlwidgets shown.bs.tab.htmlwidgets shown.bs.collapse.htmlwidgets",
              resizeHandler
            );
            window.jQuery(document).on(
              "hidden.htmlwidgets hidden.bs.tab.htmlwidgets hidden.bs.collapse.htmlwidgets",
              resizeHandler
            );
          }

          // This is needed for the specific case of ioslides, which
          // flips slides between display:none and display:block.
          // Ideally we would not have to have ioslide-specific code
          // here, but rather have ioslides raise a generic event,
          // but the rmarkdown package just went to CRAN so the
          // window to getting that fixed may be long.
          if (window.addEventListener) {
            // It's OK to limit this to window.addEventListener
            // browsers because ioslides itself only supports
            // such browsers.
            on(document, "slideenter", resizeHandler);
            on(document, "slideleave", resizeHandler);
          }
        }

        var scriptData = document.querySelector("script[data-for='" + el.id + "'][type='application/json']");
        if (scriptData) {
          var data = JSON.parse(scriptData.textContent || scriptData.text);
          // Resolve strings marked as javascript literals to objects
          if (!(data.evals instanceof Array)) data.evals = [data.evals];
          for (var k = 0; data.evals && k < data.evals.length; k++) {
            window.HTMLWidgets.evaluateStringMember(data.x, data.evals[k]);
          }
          binding.renderValue(el, data.x, initResult);
          evalAndRun(data.jsHooks.render, initResult, [el, data.x]);
        }
      });
    });

    invokePostRenderHandlers();
  }


  function has_jQuery3() {
    if (!window.jQuery) {
      return false;
    }
    var $version = window.jQuery.fn.jquery;
    var $major_version = parseInt($version.split(".")[0]);
    return $major_version >= 3;
  }

  /*
  / Shiny 1.4 bumped jQuery from 1.x to 3.x which means jQuery's
  / on-ready handler (i.e., $(fn)) is now asyncronous (i.e., it now
  / really means $(setTimeout(fn)).
  / https://jquery.com/upgrade-guide/3.0/#breaking-change-document-ready-handlers-are-now-asynchronous
  /
  / Since Shiny uses $() to schedule initShiny, shiny>=1.4 calls initShiny
  / one tick later than it did before, which means staticRender() is
  / called renderValue() earlier than (advanced) widget authors might be expecting.
  / https://github.com/rstudio/shiny/issues/2630
  /
  / For a concrete example, leaflet has some methods (e.g., updateBounds)
  / which reference Shiny methods registered in initShiny (e.g., setInputValue).
  / Since leaflet is privy to this life-cycle, it knows to use setTimeout() to
  / delay execution of those methods (until Shiny methods are ready)
  / https://github.com/rstudio/leaflet/blob/18ec981/javascript/src/index.js#L266-L268
  /
  / Ideally widget authors wouldn't need to use this setTimeout() hack that
  / leaflet uses to call Shiny methods on a staticRender(). In the long run,
  / the logic initShiny should be broken up so that method registration happens
  / right away, but binding happens later.
  */
  function maybeStaticRenderLater() {
    if (shinyMode && has_jQuery3()) {
      window.jQuery(window.HTMLWidgets.staticRender);
    } else {
      window.HTMLWidgets.staticRender();
    }
  }

  if (document.addEventListener) {
    document.addEventListener("DOMContentLoaded", function() {
      document.removeEventListener("DOMContentLoaded", arguments.callee, false);
      maybeStaticRenderLater();
    }, false);
  } else if (document.attachEvent) {
    document.attachEvent("onreadystatechange", function() {
      if (document.readyState === "complete") {
        document.detachEvent("onreadystatechange", arguments.callee);
        maybeStaticRenderLater();
      }
    });
  }


  window.HTMLWidgets.getAttachmentUrl = function(depname, key) {
    // If no key, default to the first item
    if (typeof(key) === "undefined")
      key = 1;

    var link = document.getElementById(depname + "-" + key + "-attachment");
    if (!link) {
      throw new Error("Attachment " + depname + "/" + key + " not found in document");
    }
    return link.getAttribute("href");
  };

  window.HTMLWidgets.dataframeToD3 = function(df) {
    var names = [];
    var length;
    for (var name in df) {
        if (df.hasOwnProperty(name))
            names.push(name);
        if (typeof(df[name]) !== "object" || typeof(df[name].length) === "undefined") {
            throw new Error("All fields must be arrays");
        } else if (typeof(length) !== "undefined" && length !== df[name].length) {
            throw new Error("All fields must be arrays of the same length");
        }
        length = df[name].length;
    }
    var results = [];
    var item;
    for (var row = 0; row < length; row++) {
        item = {};
        for (var col = 0; col < names.length; col++) {
            item[names[col]] = df[names[col]][row];
        }
        results.push(item);
    }
    return results;
  };

  window.HTMLWidgets.transposeArray2D = function(array) {
      if (array.length === 0) return array;
      var newArray = array[0].map(function(col, i) {
          return array.map(function(row) {
              return row[i]
          })
      });
      return newArray;
  };
  // Split value at splitChar, but allow splitChar to be escaped
  // using escapeChar. Any other characters escaped by escapeChar
  // will be included as usual (including escapeChar itself).
  function splitWithEscape(value, splitChar, escapeChar) {
    var results = [];
    var escapeMode = false;
    var currentResult = "";
    for (var pos = 0; pos < value.length; pos++) {
      if (!escapeMode) {
        if (value[pos] === splitChar) {
          results.push(currentResult);
          currentResult = "";
        } else if (value[pos] === escapeChar) {
          escapeMode = true;
        } else {
          currentResult += value[pos];
        }
      } else {
        currentResult += value[pos];
        escapeMode = false;
      }
    }
    if (currentResult !== "") {
      results.push(currentResult);
    }
    return results;
  }
  // Function authored by Yihui/JJ Allaire
  window.HTMLWidgets.evaluateStringMember = function(o, member) {
    var parts = splitWithEscape(member, '.', '\\');
    for (var i = 0, l = parts.length; i < l; i++) {
      var part = parts[i];
      // part may be a character or 'numeric' member name
      if (o !== null && typeof o === "object" && part in o) {
        if (i == (l - 1)) { // if we are at the end of the line then evalulate
          if (typeof o[part] === "string")
            o[part] = tryEval(o[part]);
        } else { // otherwise continue to next embedded object
          o = o[part];
        }
      }
    }
  };

  // Retrieve the HTMLWidget instance (i.e. the return value of an
  // HTMLWidget binding's initialize() or factory() function)
  // associated with an element, or null if none.
  window.HTMLWidgets.getInstance = function(el) {
    return elementData(el, "init_result");
  };

  // Finds the first element in the scope that matches the selector,
  // and returns the HTMLWidget instance (i.e. the return value of
  // an HTMLWidget binding's initialize() or factory() function)
  // associated with that element, if any. If no element matches the
  // selector, or the first matching element has no HTMLWidget
  // instance associated with it, then null is returned.
  //
  // The scope argument is optional, and defaults to window.document.
  window.HTMLWidgets.find = function(scope, selector) {
    if (arguments.length == 1) {
      selector = scope;
      scope = document;
    }

    var el = scope.querySelector(selector);
    if (el === null) {
      return null;
    } else {
      return window.HTMLWidgets.getInstance(el);
    }
  };

  // Finds all elements in the scope that match the selector, and
  // returns the HTMLWidget instances (i.e. the return values of
  // an HTMLWidget binding's initialize() or factory() function)
  // associated with the elements, in an array. If elements that
  // match the selector don't have an associated HTMLWidget
  // instance, the returned array will contain nulls.
  //
  // The scope argument is optional, and defaults to window.document.
  window.HTMLWidgets.findAll = function(scope, selector) {
    if (arguments.length == 1) {
      selector = scope;
      scope = document;
    }

    var nodes = scope.querySelectorAll(selector);
    var results = [];
    for (var i = 0; i < nodes.length; i++) {
      results.push(window.HTMLWidgets.getInstance(nodes[i]));
    }
    return results;
  };

  var postRenderHandlers = [];
  function invokePostRenderHandlers() {
    while (postRenderHandlers.length) {
      var handler = postRenderHandlers.shift();
      if (handler) {
        handler();
      }
    }
  }

  // Register the given callback function to be invoked after the
  // next time static widgets are rendered.
  window.HTMLWidgets.addPostRenderHandler = function(callback) {
    postRenderHandlers.push(callback);
  };

  // Takes a new-style instance-bound definition, and returns an
  // old-style class-bound definition. This saves us from having
  // to rewrite all the logic in this file to accomodate both
  // types of definitions.
  function createLegacyDefinitionAdapter(defn) {
    var result = {
      name: defn.name,
      type: defn.type,
      initialize: function(el, width, height) {
        return defn.factory(el, width, height);
      },
      renderValue: function(el, x, instance) {
        return instance.renderValue(x);
      },
      resize: function(el, width, height, instance) {
        return instance.resize(width, height);
      }
    };

    if (defn.find)
      result.find = defn.find;
    if (defn.renderError)
      result.renderError = defn.renderError;
    if (defn.clearError)
      result.clearError = defn.clearError;

    return result;
  }
})();

\"></script>\n",
       "<link href=\"data:text/css;charset-utf-8;base64,LnN0cl92aWV3IHVsLCAuc3RyX3ZpZXcgbGkgewogIGxpc3Qtc3R5bGU6IG5vbmU7CiAgcGFkZGluZzogMDsKICBtYXJnaW46IDAuNWVtIDA7CiAgZm9udC1mYW1pbHk6IG1vbm9zcGFjZTsKfQoKLnN0cl92aWV3IC5tYXRjaCB7CiAgYm9yZGVyOiAxcHggc29saWQgI2NjYzsKICBiYWNrZ3JvdW5kLWNvbG9yOiAjZWVlOwp9Cg==\" rel=\"stylesheet\" />\n",
       "<script title=\"str_view-binding\" src=\"data:application/javascript;base64,SFRNTFdpZGdldHMud2lkZ2V0KHsKCiAgbmFtZTogJ3N0cl92aWV3JywKCiAgdHlwZTogJ291dHB1dCcsCgogIGluaXRpYWxpemU6IGZ1bmN0aW9uKGVsLCB3aWR0aCwgaGVpZ2h0KSB7CiAgfSwKCiAgcmVuZGVyVmFsdWU6IGZ1bmN0aW9uKGVsLCB4LCBpbnN0YW5jZSkgewogICAgZWwuaW5uZXJIVE1MID0geC5odG1sOwogIH0sCgogIHJlc2l6ZTogZnVuY3Rpb24oZWwsIHdpZHRoLCBoZWlnaHQsIGluc3RhbmNlKSB7CiAgfQoKfSk7Cg==\"></script>\n",
       "\t</head>\n",
       "\t<body>\n",
       "\t\t<div id=\"htmlwidget-6beb09accc773f45238d\" style=\"width:960px;height:10px;\" class=\"str_view html-widget\"></div>\n",
       "<script type=\"application/json\" data-for=\"htmlwidget-6beb09accc773f45238d\">{\"x\":{\"html\":\"<ul>\\n  <li>Word2vec isn't <span class='match'>deep<\\/span> learning. Its explicitly<br>and deliberately as shallow as possible - one of<br>Mikolov's central realisations was that a very<br>simple model trained on vast amounts of data<br>can be superior to a complicated model trained<br>on less data. There's another algorithm called<br>Paragraph2Vec which builds on Word2Vec and *does*<br>incorporate '<span class='match'>deep<\\/span> learning' (in so far as it uses<br>a neural network). Its purpose is to amalgamate<br>the Word2Vec vectors for the words in a chunk of<br>text - kinda like you do by averaging them. P2V<br>tries to find a paragraph vector which is a good<br>predictor of the word vectors, with the idea being<br>that such a vector effectively represents the<br>content of the paragraph.<\\/li>\\n<\\/ul>\"},\"evals\":[],\"jsHooks\":[]}</script>\n",
       "\t</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "HTML widgets cannot be represented in plain text (need html)"
      ]
     },
     "metadata": {
      "text/html": {
       "isolated": true
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display wrapped paragraph as HTML, inserting paragraph breaks\n",
    "str_wrap(data$body[str_which(data$body, \"deep learning\")][1], width = 50) %>%\n",
    "str_replace_all(\"\\n\", \"<br>\") %>%\n",
    "str_view_all(pattern = \"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
